<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>
<!-- 2016-12-24 Sat 00:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Inferencia Estadística</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Mario Román" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Inferencia Estadística
<br>
<span class="subtitle">Theory</span>
</h1>
<div id="table-of-contents">
<h2>&Iacute;ndice</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5ae4708">Apuntes en clase</a>
<ul>
<li><a href="#org22e363c">Introducción</a>
<ul>
<li><a href="#orgd555113">Estadísticos</a></li>
</ul>
</li>
<li><a href="#org5575ac8">Distribuciones continuas</a>
<ul>
<li><a href="#org1d9bccc">Distribución uniforme</a></li>
<li><a href="#org385d8ce">Distribución gamma</a></li>
<li><a href="#org71a18a5">Distribución de Dirichlet</a></li>
</ul>
</li>
<li><a href="#org55360ea">Máxima verosimilitud</a>
<ul>
<li>
<ul>
<li><a href="#orgc407b73">Ejemplo de una distribución binomial</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4de7cf9">1. Introducción a la inferencia estadística. Estadísticos muestrales.</a>
<ul>
<li><a href="#org74cdb15">Planteamiento de un problema de inferencia.</a>
<ul>
<li><a href="#org1b1bf3d">Modelo</a></li>
<li><a href="#org64358e3">Muestra aleatoria simple</a></li>
<li><a href="#org15e2f48">Estadístico muestral</a></li>
</ul>
</li>
<li><a href="#orgb5f58ac">Función de distribución empírica</a>
<ul>
<li><a href="#org75576df">Función de distribución empírica</a></li>
<li><a href="#org358ac5e">Propiedades de la función de distribución empírica</a></li>
<li><a href="#org3a6bbcf">Teorema de Glivenko-Cantelli</a></li>
</ul>
</li>
<li><a href="#org035fb91"><span class="todo TODO">TODO</span> Funciones características</a></li>
</ul>
</li>
<li><a href="#org637bd39">2. Muestreo de poblaciones normales</a>
<ul>
<li><a href="#org4560e4a">Distribución chi cuadrado de Pearson</a>
<ul>
<li><a href="#orgd050086">Distribución chi cuadrado</a></li>
<li><a href="#orgb25eda2">Función de densidad</a></li>
<li><a href="#org0ccdbdd">Función generatriz de momentos</a></li>
<li><a href="#org09fa7d0">Esperanza y varianza</a></li>
<li><a href="#org0a9952d">Propiedad de reproductividad</a></li>
<li><a href="#org2c18c27">Relación con la distribución normal</a></li>
</ul>
</li>
<li><a href="#orgfc24c72">Distribución t de Student</a>
<ul>
<li><a href="#orgc38c070">T de Student</a></li>
<li><a href="#org0027d71"><span class="todo TODO">TODO</span> Función de densidad</a></li>
<li><a href="#org4bd0acc">Momentos</a></li>
</ul>
</li>
<li><a href="#orgaae86d7">Distribución F de Snedecor</a>
<ul>
<li><a href="#org9130612">Definición</a></li>
</ul>
</li>
<li><a href="#org0b29dd6">Muestreo de la normal</a>
<ul>
<li><a href="#org2ba0381">Lema de Fisher</a></li>
<li><a href="#org0b7394b"><span class="todo TODO">TODO</span> Distribuciones del muestreo de una normal unidimensional</a></li>
</ul>
</li>
<li><a href="#org14eee2e">Muestreo de dos normales</a>
<ul>
<li><a href="#org660cb80">Extensión del lema de Fisher</a></li>
<li><a href="#orgd53965d"><span class="todo TODO">TODO</span> Distribuciones del muestreo de dos normales unidimensionales</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb3fd7b8">3. Suficiencia y completitud</a>
<ul>
<li><a href="#orgc263d43">Estadísticos suficientes y completos</a>
<ul>
<li><a href="#org0de69eb">Estadístico suficiente</a></li>
<li><a href="#org63527ec">Teorema de factorización de Fisher-Neyman</a></li>
<li><a href="#org147fe64">Propiedades de los estadísticos suficientes</a></li>
<li><a href="#orgd63a4dd">Estadístico completo</a></li>
</ul>
</li>
<li><a href="#orgdf61e0f"><span class="todo TODO">TODO</span> Suficiencia y completitud en familias exponenciales</a></li>
</ul>
</li>
<li><a href="#orgdeac23e">4. Estimación puntual</a>
<ul>
<li><a href="#org5475b56">Planteamiento del problema de estimación</a>
<ul>
<li><a href="#org058b8fa">Estimador puntual</a></li>
<li><a href="#org1a37a9e">Función de pérdida y de riesgo</a></li>
<li><a href="#org8eb21b3">Estimador óptimo</a></li>
</ul>
</li>
<li><a href="#org64dc450">Estimación de menor error cuadrático</a>
<ul>
<li><a href="#orga1f4c89">Función de pérdida cuadrática</a></li>
</ul>
</li>
<li><a href="#org52197b5">Estimación insesgada de mínima varianza</a>
<ul>
<li><a href="#org6d4bb55">Estimador insesgado</a></li>
<li><a href="#org2cced98">Estimador insesgado uniformemente de mínima varianza</a></li>
<li><a href="#orgbede4d6">Teorema de Raó-Blackwell</a></li>
<li><a href="#org6f83bb0">Teorema de Lehmann-Scheffé</a></li>
</ul>
</li>
<li><a href="#org4708cd9">Estimación eficiente</a>
<ul>
<li><a href="#orgbcd1ad0">Condiciones de regularidad de Fréchet-Cramer-Rao</a></li>
<li><a href="#orgdc9ccfa"><span class="todo TODO">TODO</span> Cota de Fréchet-Cramer-Raó</a></li>
<li><a href="#org20b3702"><span class="todo TODO">TODO</span> Estimador eficiente</a></li>
<li><a href="#org5cb73a4"><span class="todo TODO">TODO</span> Caracterización de estimadores eficientes</a></li>
</ul>
</li>
<li><a href="#orgd160662"><span class="todo TODO">TODO</span> Estimación de máxima verosimilitud</a></li>
<li><a href="#org885cb3a"><span class="todo TODO">TODO</span> Otros métodos de estimación</a></li>
</ul>
</li>
<li><a href="#org7defb41"><span class="todo TODO">TODO</span> 5. Estimación por intervalos de confianza</a></li>
<li><a href="#org3cb2ed0"><span class="todo TODO">TODO</span> 6. Contraste de hipótesis</a>
<ul>
<li><a href="#org7b0583a">Planteamiento del problema</a>
<ul>
<li><a href="#org7bbbfc6">Problema de contraste de hipótesis</a></li>
<li><a href="#orgf0de564">Test de hipótesis</a></li>
<li><a href="#orgaef340b">Tipos de errores de un test de hipótesis</a></li>
</ul>
</li>
<li><a href="#orga7a440e"><span class="todo TODO">TODO</span> Lema de Neyman-Pearson</a></li>
<li><a href="#org265c5ae"><span class="todo TODO">TODO</span> Test de la razón de verosimilitudes</a></li>
<li><a href="#orgeb4d2b5"><span class="todo TODO">TODO</span> Dualidad entre tests de hipótesis y regiones de confianza</a></li>
</ul>
</li>
<li><a href="#org1679cec">7. Teoría general de modelos lineales</a></li>
</ul>
</div>
</div>
<div id="outline-container-org5ae4708" class="outline-2">
<h2 id="org5ae4708">Apuntes en clase</h2>
<div class="outline-text-2" id="text-org5ae4708">
</div><div id="outline-container-org22e363c" class="outline-3">
<h3 id="org22e363c">Introducción</h3>
<div class="outline-text-3" id="text-org22e363c">
</div><div id="outline-container-orgd555113" class="outline-4">
<h4 id="orgd555113">Estadísticos</h4>
<div class="outline-text-4" id="text-orgd555113">
<div class="definition">
<p>
<b>Estadístico</b>. Función medible sobre variables aleatorias \(f(X_1,X_2,\dots,X_n)\).
Se dice que es un <b>estimador consistente</b> de un parámetro cuando converge 
en probabilidad a él.
</p>

</div>
</div>
</div>
</div>

<div id="outline-container-org5575ac8" class="outline-3">
<h3 id="org5575ac8">Distribuciones continuas</h3>
<div class="outline-text-3" id="text-org5575ac8">
</div><div id="outline-container-org1d9bccc" class="outline-4">
<h4 id="org1d9bccc">Distribución uniforme</h4>
<div class="outline-text-4" id="text-org1d9bccc">
<div class="definition">
<p>
<b>Distribución uniforme</b>. Sobre un intervalo \([a,b]\), definida por:
</p>

<p>
\[f(x|a,b) = \frac{1}{b-a}\]
</p>

</div>

<p>
\[EX = \int^b_a \frac{x}{b-a}dx = \frac{b+a}{2}\]
\[Var X = \int_a^b \frac{(x-\frac{b+a}{2})^2}{b-a} dx = \frac{(b-a)^2}{12}\]
</p>
</div>
</div>

<div id="outline-container-org385d8ce" class="outline-4">
<h4 id="org385d8ce">Distribución gamma</h4>
<div class="outline-text-4" id="text-org385d8ce">
<div class="definition">
<p>
<b>Función gamma</b>. La siguiente integral converge para \(\alpha > 0\):
</p>

<p>
\[\Gamma(\alpha) = \int_0^\infty t^{\alpha-1}e^{-t}dt\]
</p>

</div>

<p>
Cumple que: \(\Gamma(\alpha+1) = \alpha\Gamma(\alpha)\), de esta forma, generaliza al factorial
con \(\Gamma(n) = (n-1)!\).
</p>

<div class="definition">
<p>
<b>Distribución gamma</b>. Sobre \([0,\infty)\), dada \(\alpha\), se tiene la función de distribución:
</p>

<p>
\[f(x|\alpha,\beta) = 
 \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1}e^{-x/\beta}\]
</p>

</div>
</div>
</div>

<div id="outline-container-org71a18a5" class="outline-4">
<h4 id="org71a18a5">Distribución de Dirichlet</h4>
<div class="outline-text-4" id="text-org71a18a5">
<div class="definition">
<p>
<b>Distribución de Dirichlet</b>. 
</p>

<p>
\[f(x_1\dots x_n| \alpha_1, \dots \alpha_k,\alpha_{k+1}) = 
 \frac{\Gamma(\alpha_1+\dots+\alpha_{k+1})}
 {\Gamma(\alpha_1)\dots \Gamma(\alpha_{k+1})}
 x_1^{\alpha_1-1} \dots x_{k}^{\alpha_{k-1}-1}
 \]
</p>

</div>
</div>
</div>
</div>

<div id="outline-container-org55360ea" class="outline-3">
<h3 id="org55360ea">Máxima verosimilitud</h3>
<div class="outline-text-3" id="text-org55360ea">
<div class="definition">
<p>
<b>Máxima verosimilitud</b>. Sean \(X_1,\dots,X_n\) v.as. independientes extraídas de una
función de probabilidad perteneciente a una familia 
\(\{f(\bullet | \theta), \theta \in \Theta\}\) llamada <b>modelo</b>, pero con \(\theta\) desconocida
</p>

<p>
Llamamos <b>estimador de máxima verosimilitud</b> de \(\theta\) al \(\hat\theta\) que maximiza 
</p>

<p>
\[{\cal L}(\hat\theta | x_1,\dots,x_n) = \prod_{i=1}^n f(x_i|\theta)\]
</p>

<p>
llamada <b>función de verosimilitud</b>.
</p>

</div>

<p>
La idea del método es tomar la función de densidad conjunta asumiendo independencia:
</p>

<p>
\[f(x_1,\dots,x_n | \theta) = f(x_1|\theta) f(x_2|\theta) \dots f(x_n|\theta)\]
</p>

<p>
Y, suponiendo que los valores fueran fijos, estimar \(\theta\) con la función de
la función de verosimilitud o de su logaritmo:
</p>

<p>
\[\hat l (\hat\theta | x_1,\dots,x_n) = \sum_{i=1}^n \ln f(x_i|\theta)\]
</p>
</div>


<div id="outline-container-orgc407b73" class="outline-5">
<h5 id="orgc407b73">Ejemplo de una distribución binomial</h5>
</div>
</div>
</div>

<div id="outline-container-org4de7cf9" class="outline-2">
<h2 id="org4de7cf9">1. Introducción a la inferencia estadística. Estadísticos muestrales.</h2>
<div class="outline-text-2" id="text-org4de7cf9">
</div><div id="outline-container-org74cdb15" class="outline-3">
<h3 id="org74cdb15">Planteamiento de un problema de inferencia.</h3>
<div class="outline-text-3" id="text-org74cdb15">
</div><div id="outline-container-org1b1bf3d" class="outline-4">
<h4 id="org1b1bf3d">Modelo</h4>
<div class="outline-text-4" id="text-org1b1bf3d">
<p>
Un <b>modelo</b> es una familia paramétrica de distribuciones \(F(x,\theta)\) para un 
parámetro \(\theta\).
</p>
</div>
</div>

<div id="outline-container-org64358e3" class="outline-4">
<h4 id="org64358e3">Muestra aleatoria simple</h4>
<div class="outline-text-4" id="text-org64358e3">
<p>
Una <b>muestra aleatoria simple</b> es un vector \((X_1,\dots,X_n)\)
de variables independientes idénticamente distribuidas. 
Llamo <b>realización muestral</b> a un valor concreto obtenido al 
observar la muestra y <b>espacio muestral</b> al conjunto de todas 
las posibles realizaciones.
</p>
</div>
</div>

<div id="outline-container-org15e2f48" class="outline-4">
<h4 id="org15e2f48">Estadístico muestral</h4>
<div class="outline-text-4" id="text-org15e2f48">
<p>
Dada una muestra aleatoria simple, un <b>estadístico muestral</b> es una función sobre
ella \(T : (\mathbb{R}^n,{\cal B}^n)\longrightarrow (\mathbb{R}^k,{\cal B}^k)\) medible e independiente de cualquier parámetro desconocido.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb5f58ac" class="outline-3">
<h3 id="orgb5f58ac">Función de distribución empírica</h3>
<div class="outline-text-3" id="text-orgb5f58ac">
</div><div id="outline-container-org75576df" class="outline-4">
<h4 id="org75576df">Función de distribución empírica</h4>
<div class="outline-text-4" id="text-org75576df">
<p>
La <b>función de distribución empírica</b> es una función de distribución razonable 
que podemos obtener desde una realización muestral.
</p>

<p>
\[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(x_i < x)} \]
</p>
</div>
</div>

<div id="outline-container-org358ac5e" class="outline-4">
<h4 id="org358ac5e">Propiedades de la función de distribución empírica</h4>
<div class="outline-text-4" id="text-org358ac5e">
<p>
Fijado un \(x \in \mathbb{R}\), \(F^\ast(x)\) es una variable aleatoria siguiendo por definición una
binomial:
</p>

<p>
\[ nF^\ast(x) \longrightarrow {\cal B}(n, F(x))\]
</p>

<p>
Calculamos su <b>esperanza</b> y <b>varianza</b> desde Bernoulli como:
</p>

<ul class="org-ul">
<li>Esperanza: \(E[F^\ast(x)] = F(x)\)</li>
<li>Varianza: \(Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}\)</li>
</ul>

<p>
Aplicando entonces el Teorema Central del Límite:
</p>

<p>
\[ \frac{nF^\ast(x) - nF(x)}{\sqrt{nF(x)(1-F(x))}} \leadsto {\cal N}(0,1) \]
</p>
</div>
</div>

<div id="outline-container-org3a6bbcf" class="outline-4">
<h4 id="org3a6bbcf">Teorema de Glivenko-Cantelli</h4>
<div class="outline-text-4" id="text-org3a6bbcf">
<p>
Las funciones de distribución muestrales convergen 
casi seguramente y uniformemente a la teórica.
</p>

<p>
\[ P\left\{ \lim_{n \rightarrow \infty} 
\sup_{x \in \mathbb{R}} |F^\ast_n(x) - F(x)| = 0\right\} = 1\]
</p>
</div>
</div>
</div>

<div id="outline-container-org035fb91" class="outline-3">
<h3 id="org035fb91"><span class="todo TODO">TODO</span> Funciones características</h3>
</div>
</div>
<div id="outline-container-org637bd39" class="outline-2">
<h2 id="org637bd39">2. Muestreo de poblaciones normales</h2>
<div class="outline-text-2" id="text-org637bd39">
</div><div id="outline-container-org4560e4a" class="outline-3">
<h3 id="org4560e4a">Distribución chi cuadrado de Pearson</h3>
<div class="outline-text-3" id="text-org4560e4a">
</div><div id="outline-container-orgd050086" class="outline-4">
<h4 id="orgd050086">Distribución chi cuadrado</h4>
<div class="outline-text-4" id="text-orgd050086">
<p>
Es un caso particular de la distribución gamma que se obtiene como la
distribución de la suma de \(k\) cuadrados de variables normales, \(X \leadsto \chi^2(k) = \Gamma(k/2,1/2)\).
Al parámetro \(k\) se le llama número de grados de libertad.
</p>
</div>
</div>

<div id="outline-container-orgb25eda2" class="outline-4">
<h4 id="orgb25eda2">Función de densidad</h4>
<div class="outline-text-4" id="text-orgb25eda2">
<p>
\[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]
</p>
</div>
</div>

<div id="outline-container-org0ccdbdd" class="outline-4">
<h4 id="org0ccdbdd">Función generatriz de momentos</h4>
</div>

<div id="outline-container-org09fa7d0" class="outline-4">
<h4 id="org09fa7d0">Esperanza y varianza</h4>
<div class="outline-text-4" id="text-org09fa7d0">
<ul class="org-ul">
<li>\(E[X] = k\)</li>
<li>\(Var[X] = 2k\)</li>
</ul>
</div>
</div>

<div id="outline-container-org0a9952d" class="outline-4">
<h4 id="org0a9952d">Propiedad de reproductividad</h4>
<div class="outline-text-4" id="text-org0a9952d">
<p>
Si tengo una serie de variables independientes distribuidas por \(X_i \leadsto \chi^2(k_i)\), entonces:
</p>

<p>
\[\sum_{i=1}^n X_i = \chi^2 \left(\sum_{i=1}^n k_i \right)\]
</p>
</div>
</div>

<div id="outline-container-org2c18c27" class="outline-4">
<h4 id="org2c18c27">Relación con la distribución normal</h4>
<div class="outline-text-4" id="text-org2c18c27">
<p>
Dadas variables independientes \(X_i \leadsto {\cal N}(0,1)\),
</p>

<p>
\[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgfc24c72" class="outline-3">
<h3 id="orgfc24c72">Distribución t de Student</h3>
<div class="outline-text-3" id="text-orgfc24c72">
</div><div id="outline-container-orgc38c070" class="outline-4">
<h4 id="orgc38c070">T de Student</h4>
<div class="outline-text-4" id="text-orgc38c070">
<p>
Dadas dos variables independientes \(X \leadsto {\cal N}(0,1)\) e \(Y \leadsto \chi^2(n)\), tenemos
</p>

<p>
\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]
</p>
</div>
</div>

<div id="outline-container-org0027d71" class="outline-4">
<h4 id="org0027d71"><span class="todo TODO">TODO</span> Función de densidad</h4>
</div>
<div id="outline-container-org4bd0acc" class="outline-4">
<h4 id="org4bd0acc">Momentos</h4>
<div class="outline-text-4" id="text-org4bd0acc">
<p>
Tenemos que \(\exists E[T^k] \Leftrightarrow k < n\). Cuando existen, se tiene
</p>

<ul class="org-ul">
<li>\(E[T] = 0\)</li>
<li>\(Var[T] = \frac{n}{n-2}\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgaae86d7" class="outline-3">
<h3 id="orgaae86d7">Distribución F de Snedecor</h3>
<div class="outline-text-3" id="text-orgaae86d7">
</div><div id="outline-container-org9130612" class="outline-4">
<h4 id="org9130612">Definición</h4>
<div class="outline-text-4" id="text-org9130612">
<p>
<b>F de Snedecor</b>. Dadas dos variables independientes \(X \leadsto \chi^2(n)\) e \(Y \leadsto \chi^2(m)\),
su cociente nos da:
</p>

<p>
\[F = \frac{X/m}{Y/n} \longrightarrow F(m,n)\]
</p>
</div>
</div>
</div>

<div id="outline-container-org0b29dd6" class="outline-3">
<h3 id="org0b29dd6">Muestreo de la normal</h3>
<div class="outline-text-3" id="text-org0b29dd6">
</div><div id="outline-container-org2ba0381" class="outline-4">
<h4 id="org2ba0381">Lema de Fisher</h4>
<div class="outline-text-4" id="text-org2ba0381">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple con \(X \leadsto {\cal N}(\mu,\sigma^2)\). Definimos:
</p>

<ul class="org-ul">
<li>\(\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</li>
<li>\(S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X})\)</li>
</ul>

<p>
Los estadísticos \(\overline{X}\) y \(S^2\) son independientes cuando \(X\) tiene
distribución normal.
</p>
</div>
</div>

<div id="outline-container-org0b7394b" class="outline-4">
<h4 id="org0b7394b"><span class="todo TODO">TODO</span> Distribuciones del muestreo de una normal unidimensional</h4>
</div>
</div>
<div id="outline-container-org14eee2e" class="outline-3">
<h3 id="org14eee2e">Muestreo de dos normales</h3>
<div class="outline-text-3" id="text-org14eee2e">
</div><div id="outline-container-org660cb80" class="outline-4">
<h4 id="org660cb80">Extensión del lema de Fisher</h4>
<div class="outline-text-4" id="text-org660cb80">
<p>
Sean \((X_1,\dots,X_{n_2})\) y \((Y_1,\dots,Y_{n_1})\) muestras aleatorias simples con \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) y
con \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\). Definimos análogamente al caso univariable \(\overline{X}\), \(\overline{Y}\), \(S^2_1\), \(S^2_2\).
</p>

<p>
Los vectores \((\overline{X},\overline{Y})\) y \((S^2_1,S^2_2)\) son independientes.
</p>
</div>
</div>

<div id="outline-container-orgd53965d" class="outline-4">
<h4 id="orgd53965d"><span class="todo TODO">TODO</span> Distribuciones del muestreo de dos normales unidimensionales</h4>
</div>
</div>
</div>
<div id="outline-container-orgb3fd7b8" class="outline-2">
<h2 id="orgb3fd7b8">3. Suficiencia y completitud</h2>
<div class="outline-text-2" id="text-orgb3fd7b8">
</div><div id="outline-container-orgc263d43" class="outline-3">
<h3 id="orgc263d43">Estadísticos suficientes y completos</h3>
<div class="outline-text-3" id="text-orgc263d43">
</div><div id="outline-container-org0de69eb" class="outline-4">
<h4 id="org0de69eb">Estadístico suficiente</h4>
<div class="outline-text-4" id="text-org0de69eb">
<p>
Un estadístico \(t\) es <b>suficiente</b> para un parámetro \(\theta\) 
cuando una vez conocido no puede obtenerse más información de sobre \(\theta\) de
los datos; esto es:
</p>

<p>
\[\Pr(\theta| t,x) = \Pr(\theta|t)\]
</p>

<p>
De forma equivalente, es <b>suficiente</b> si la distribución condicionada 
al estadístico es independiente del parámetro \(\theta\):
</p>

<p>
\[\Pr(x|t,\theta) = \Pr(x|t)\]
</p>
</div>
</div>

<div id="outline-container-org63527ec" class="outline-4">
<h4 id="org63527ec">Teorema de factorización de Fisher-Neyman</h4>
<div class="outline-text-4" id="text-org63527ec">
<p>
<b>Teorema de factorización de Fisher-Neyman</b>. \(T\) es suficiente para \(\theta\)
ssi existen funciones no negativas \(g\),\(h\) tales que:
</p>

<p>
\[f_\theta(x) = h(x)g_\theta(T(x))\]
</p>

<p>
Donde \(g_\theta\) sólo depende de \(x\) a través de \(T\) y \(h\) no depende de \(\theta\).
</p>
</div>
</div>

<div id="outline-container-org147fe64" class="outline-4">
<h4 id="org147fe64">Propiedades de los estadísticos suficientes</h4>
<div class="outline-text-4" id="text-org147fe64">
<p>
<b>Propiedades de los estadísticos suficientes</b>.
</p>

<ul class="org-ul">
<li>Si \(T\) es suficiente para \(\{P_\theta \mid \theta \in \Theta\}\), lo es para \(\{P_\theta \mid \theta \in \Theta' \subset \Theta\}\).</li>
<li>Si \(T\) es suficiente y \(T = h(U(X)\), \(U\) es suficiente.</li>
<li>Toda transformación biunívoca de suficiente es suficiente.</li>
</ul>
</div>
</div>

<div id="outline-container-orgd63a4dd" class="outline-4">
<h4 id="orgd63a4dd">Estadístico completo</h4>
<div class="outline-text-4" id="text-orgd63a4dd">
<p>
Un estadístico es <b>completo</b> cuando para cualquier función medible se tiene:
</p>

<p>
\[ E_\theta [g(T)] = 0 \; \forall\theta\in\Theta \quad \Rightarrow \quad
    P_\theta(g(T) = 0) = 1\; \forall\theta\in\Theta\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgdf61e0f" class="outline-3">
<h3 id="orgdf61e0f"><span class="todo TODO">TODO</span> Suficiencia y completitud en familias exponenciales</h3>
</div>
</div>
<div id="outline-container-orgdeac23e" class="outline-2">
<h2 id="orgdeac23e">4. Estimación puntual</h2>
<div class="outline-text-2" id="text-orgdeac23e">
</div><div id="outline-container-org5475b56" class="outline-3">
<h3 id="org5475b56">Planteamiento del problema de estimación</h3>
<div class="outline-text-3" id="text-org5475b56">
</div><div id="outline-container-org058b8fa" class="outline-4">
<h4 id="org058b8fa">Estimador puntual</h4>
<div class="outline-text-4" id="text-org058b8fa">
<p>
Un <b>estimador puntual</b> es un estadístico \(T\) tomando valores en el dominio
del parámetro. 
</p>
</div>
</div>

<div id="outline-container-org1a37a9e" class="outline-4">
<h4 id="org1a37a9e">Función de pérdida y de riesgo</h4>
<div class="outline-text-4" id="text-org1a37a9e">
<p>
La <b>función de pérdida</b>, \(L\), nos dice la pérdida asociada a estimar un parámetro si 
su verdadero valor es otro. La <b>función de riesgo</b> es la 
que asocia a cada valor del parámetro, la pérdida media asociada al estimador.
</p>

<p>
\[ R^L_T(\theta) = E_\theta [L(\theta,T)]\]
</p>
</div>
</div>

<div id="outline-container-org8eb21b3" class="outline-4">
<h4 id="org8eb21b3">Estimador óptimo</h4>
<div class="outline-text-4" id="text-org8eb21b3">
<p>
El <b>estimador óptimo</b>, \(T\), dada una función de pérdida, es el que minimiza 
uniformemente la función de riesgo:
</p>

<p>
\[ R^L_T(\theta) \leq R^L_{T''}(\theta),\quad \forall \theta \in \Theta,\; \forall T''\]
</p>
</div>
</div>
</div>

<div id="outline-container-org64dc450" class="outline-3">
<h3 id="org64dc450">Estimación de menor error cuadrático</h3>
<div class="outline-text-3" id="text-org64dc450">
</div><div id="outline-container-orga1f4c89" class="outline-4">
<h4 id="orga1f4c89">Función de pérdida cuadrática</h4>
<div class="outline-text-4" id="text-orga1f4c89">
<p>
La <b>función de pérdida cuadrática</b>, \({\cal L}(\theta, t) = (t - \theta)^2\), deja la función de 
riesgo de un estimador como su <b>error cuadrático medio</b>:
</p>

<p>
\[R^L_T(\theta) = E_\theta[(T - \theta)^2]\]
</p>

<p>
Nótese que en el caso de \(E[T] = \theta\), se tiene \(R^L_T(\theta) = Var_\theta[T]\).
</p>
</div>
</div>
</div>

<div id="outline-container-org52197b5" class="outline-3">
<h3 id="org52197b5">Estimación insesgada de mínima varianza</h3>
<div class="outline-text-3" id="text-org52197b5">
</div><div id="outline-container-org6d4bb55" class="outline-4">
<h4 id="org6d4bb55">Estimador insesgado</h4>
<div class="outline-text-4" id="text-org6d4bb55">
<p>
Un estimador \(T\) de \(g(\theta)\), es <b>insesgado</b> o <b>centrado</b> si:
</p>

<p>
\(E_\theta[T(X_1,\dots,X_n)] = g(\theta)\)
</p>
</div>
</div>

<div id="outline-container-org2cced98" class="outline-4">
<h4 id="org2cced98">Estimador insesgado uniformemente de mínima varianza</h4>
<div class="outline-text-4" id="text-org2cced98">
<p>
Un estimador \(T\) insesgado y de segundo orden es <b>UMVUE</b> para \(g(\theta)\) si para cualquier otro 
estimador insesgado \(T'\) se tiene que:
</p>

<p>
\[ Var_\theta[T] \leq Var_\theta[T']\]
</p>

<p>
Propiedades:
</p>

<ul class="org-ul">
<li>Unicidad: El UMVUE de cualquier función paramétrica, si existe, es único.</li>
<li>Linealidad: Si \(T,Q\) son UMVUE para \(g,h\); \(aT+bQ\) es UMVUE para \(ag+bh\).</li>
</ul>
</div>
</div>

<div id="outline-container-orgbede4d6" class="outline-4">
<h4 id="orgbede4d6">Teorema de Raó-Blackwell</h4>
<div class="outline-text-4" id="text-orgbede4d6">
<p>
Si \(T\) es suficiente para \(\theta\) y \(S\) es un estimador insesgado de \(g(\theta)\) de segundo orden:
</p>

<ul class="org-ul">
<li>\(E[S \mid T]\) es estimador insesgado de \(g(\theta)\) de segundo orden.</li>
<li>\(Var_\theta[E[S \mid T]] \leq Var_\theta[S]\)</li>
</ul>

<p>
Es decir, \(E[S \mid T]\) será normalmente mejor estimador y nunca peor que \(S\).
</p>
</div>
</div>

<div id="outline-container-org6f83bb0" class="outline-4">
<h4 id="org6f83bb0">Teorema de Lehmann-Scheffé</h4>
<div class="outline-text-4" id="text-org6f83bb0">
<p>
Para \(T\) suficiente y completo para \(\theta\); si \(g(\theta)\) admite un estimador insesgado de 
segundo orden \(S\), entonces existe el UMVUE de \(g(\theta)\) y está dado por:
</p>

<p>
\[ E [S \mid T]\]
</p>
</div>
</div>
</div>

<div id="outline-container-org4708cd9" class="outline-3">
<h3 id="org4708cd9">Estimación eficiente</h3>
<div class="outline-text-3" id="text-org4708cd9">
</div><div id="outline-container-orgbcd1ad0" class="outline-4">
<h4 id="orgbcd1ad0">Condiciones de regularidad de Fréchet-Cramer-Rao</h4>
<div class="outline-text-4" id="text-orgbcd1ad0">
<p>
Una familia de distribuciones es <b>regular en el sentido de Fréchet-Cramer-Rao</b> 
si cumple, siendo \(\{P_\theta \; \theta\in\Theta\}\):
</p>

<ul class="org-ul">
<li>\(\Theta\) es intervalo abierto de \(\mathbb{R}\).</li>
<li>\(\forall \theta\in\Theta : \{x \mid f_\theta(x) > 0\} = \chi\)</li>
<li>Tenemos \(f_\theta(x)\) derivable respecto a \(\theta\) para todo \(x\in\chi\) con:
\[ \int_\chi \frac{d f_\theta(x)}{d\theta} dx = 
   \frac{d}{d\theta} \int_\chi f_\theta(x) dx = 
   0, \quad \forall \theta\in\Theta\]</li>
</ul>
</div>
</div>
<div id="outline-container-orgdc9ccfa" class="outline-4">
<h4 id="orgdc9ccfa"><span class="todo TODO">TODO</span> Cota de Fréchet-Cramer-Raó</h4>
</div>
<div id="outline-container-org20b3702" class="outline-4">
<h4 id="org20b3702"><span class="todo TODO">TODO</span> Estimador eficiente</h4>
</div>
<div id="outline-container-org5cb73a4" class="outline-4">
<h4 id="org5cb73a4"><span class="todo TODO">TODO</span> Caracterización de estimadores eficientes</h4>
</div>
</div>
<div id="outline-container-orgd160662" class="outline-3">
<h3 id="orgd160662"><span class="todo TODO">TODO</span> Estimación de máxima verosimilitud</h3>
</div>
<div id="outline-container-org885cb3a" class="outline-3">
<h3 id="org885cb3a"><span class="todo TODO">TODO</span> Otros métodos de estimación</h3>
</div>
</div>
<div id="outline-container-org7defb41" class="outline-2">
<h2 id="org7defb41"><span class="todo TODO">TODO</span> 5. Estimación por intervalos de confianza</h2>
</div>
<div id="outline-container-org3cb2ed0" class="outline-2">
<h2 id="org3cb2ed0"><span class="todo TODO">TODO</span> 6. Contraste de hipótesis</h2>
<div class="outline-text-2" id="text-org3cb2ed0">
</div><div id="outline-container-org7b0583a" class="outline-3">
<h3 id="org7b0583a">Planteamiento del problema</h3>
<div class="outline-text-3" id="text-org7b0583a">
</div><div id="outline-container-org7bbbfc6" class="outline-4">
<h4 id="org7bbbfc6">Problema de contraste de hipótesis</h4>
<div class="outline-text-4" id="text-org7bbbfc6">
<p>
Dada \((X_1,\dots,X_n)\) una muestra aleatoria simple de \(X \leadsto P_\theta\), para 
\(\theta \in \Theta_0 \cup \Theta_1\), llamamos:
</p>

<ul class="org-ul">
<li><b>Hipótesis nula</b>: \(H_0 : \theta \in \Theta_0\)</li>
<li><b>Hipótesis alternativa</b>: \(H_1 : \theta \in \Theta\)</li>
</ul>

<p>
a dos hipótesis posibles.
</p>
</div>
</div>

<div id="outline-container-orgf0de564" class="outline-4">
<h4 id="orgf0de564">Test de hipótesis</h4>
<div class="outline-text-4" id="text-orgf0de564">
<p>
El <b>test de hipótesis</b> es un estadístico \(\varphi\) tomando valores en \([0,1]\), que da la 
posibilidad de rechazar \(H_0\) dada una realización muestral. Se llama:
</p>

<ul class="org-ul">
<li><b>Test no aleatorizado</b>, si toma valores \(0,1\).</li>
<li><b>Test aleatorizado</b>, si toma valor distinto de \(0,1\).</li>
</ul>
</div>
</div>

<div id="outline-container-orgaef340b" class="outline-4">
<h4 id="orgaef340b">Tipos de errores de un test de hipótesis</h4>
<div class="outline-text-4" id="text-orgaef340b">
<p>
Hay dos tipos de erorres:
</p>

<ul class="org-ul">
<li><b>Error de tipo 1</b>: Rechazar \(H_0\) siendo cierta.</li>
<li><b>Error de tipo 2</b>: Aceptar \(H_0\) siendo falsa.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga7a440e" class="outline-3">
<h3 id="orga7a440e"><span class="todo TODO">TODO</span> Lema de Neyman-Pearson</h3>
</div>
<div id="outline-container-org265c5ae" class="outline-3">
<h3 id="org265c5ae"><span class="todo TODO">TODO</span> Test de la razón de verosimilitudes</h3>
</div>
<div id="outline-container-orgeb4d2b5" class="outline-3">
<h3 id="orgeb4d2b5"><span class="todo TODO">TODO</span> Dualidad entre tests de hipótesis y regiones de confianza</h3>
</div>
</div>
<div id="outline-container-org1679cec" class="outline-2">
<h2 id="org1679cec">7. Teoría general de modelos lineales</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Autor: Mario Román</p>
<p class="date">Created: 2016-12-24 Sat 00:23</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
