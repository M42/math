<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>
<!-- 2017-04-23 Sun 19:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Inferencia estadística</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Mario Román" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Inferencia estadística</h1>
<div id="table-of-contents">
<h2>&Iacute;ndice</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc5764eb">Prerrequisitos</a>
<ul>
<li><a href="#org88ef035">Distribuciones</a>
<ul>
<li><a href="#orge967534">Función generatriz de momentos</a>
<ul>
<li><a href="#orgc86c261">Cálculo de momentos</a></li>
</ul>
</li>
<li><a href="#orgfdfbfed">Función característica</a>
<ul>
<li><a href="#orgf0adbbd">Cálculo de momentos</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2192093">Varianza</a>
<ul>
<li><a href="#org45ed572">Varianza</a></li>
<li><a href="#orgc547fed">Covarianza</a></li>
<li><a href="#org1e30b99">Varianza de la suma</a></li>
<li><a href="#org49c32eb">Cauchy-Schwarz para la covarianza</a>
<ul>
<li><a href="#org76d2d71">Demostración</a></li>
<li><a href="#org30e970a">Demostración por Cauchy-Schwarz</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1d245ac">Esperanza condicional</a>
<ul>
<li><a href="#orgde4e432">Esperanza condicional en caso discreto</a></li>
<li><a href="#org996cf72">Esperanza condicional en el caso continuo</a></li>
<li><a href="#org087896d">Ley de esperanza total</a>
<ul>
<li><a href="#org2e94e33">Demostración en el caso discreto</a></li>
<li><a href="#org914f5b9">Demostración en el caso continuo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgfe5d52b">Desigualdades</a>
<ul>
<li><a href="#org5423165">Desigualdad de Chebyshev</a></li>
</ul>
</li>
<li><a href="#org690d215">Convergencia</a>
<ul>
<li><a href="#org73c99a6">Convergencia casi segura</a></li>
<li><a href="#orgae09b74">Convergencia en probabilidad</a>
<ul>
<li><a href="#orgbcf122e">Equivalentemente</a></li>
</ul>
</li>
<li><a href="#orgdc4479f">Convergencia en distribución</a>
<ul>
<li><a href="#orgdf71a4c">Equivalentemente</a></li>
</ul>
</li>
<li><a href="#org46dc453">Implicaciones</a>
<ul>
<li><a href="#org8aa2908"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org5b32272">Ley débil de los grandes números</a></li>
<li><a href="#orgda36df7">Ley fuerte de los grandes números</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org528e1ed">Distribuciones discretas</a>
<ul>
<li><a href="#org64713b4">1. Distribución uniforme</a>
<ul>
<li><a href="#org00af3b3">Definición</a></li>
</ul>
</li>
<li><a href="#org66d91a4">2. Distribución binomial</a>
<ul>
<li><a href="#orgc166885">Definición</a>
<ul>
<li><a href="#orgea90c29">Esperanza</a></li>
<li><a href="#org9d541f5">Varianza</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd753926">3. Distribución multinomial</a>
<ul>
<li><a href="#org3840848">Definición</a></li>
</ul>
</li>
<li><a href="#orgdb4b49e">4. Distribución de Poisson</a>
<ul>
<li><a href="#org96eecca">Definición</a>
<ul>
<li><a href="#org050b1bc">Es una distribución</a></li>
</ul>
</li>
<li><a href="#orgb62eeee">Función generatriz de momentos</a>
<ul>
<li><a href="#org27ad75f">Esperanza</a></li>
<li><a href="#orgae14292">Varianza</a></li>
</ul>
</li>
<li><a href="#orgc723a7d">Suma de Poisson</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org53e6171">Distribuciones continuas</a>
<ul>
<li><a href="#org1e27188">1. Distribución normal</a>
<ul>
<li><a href="#orgd21e166">Definición</a>
<ul>
<li><a href="#orgd390cc5">Imagen de la distribución</a></li>
<li><a href="#orgce91029">Es una distribución</a></li>
</ul>
</li>
<li><a href="#org0780546">Función característica</a>
<ul>
<li><a href="#orgd1f8e2f"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orgda127ba">Suma de normales</a>
<ul>
<li><a href="#orgf6e18ed"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org7c8673f">Producto por escalar</a>
<ul>
<li><a href="#org1503b51"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org1dc8553">Teorema de Cramer</a>
<ul>
<li><a href="#org400d30f"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org248b986">2. Distribución χ² de Pearson</a>
<ul>
<li><a href="#orgf72b604">Distribución chi cuadrado</a>
<ul>
<li><a href="#org2536e1f">Gráfica de la función de densidad</a></li>
<li><a href="#orgd4f7819">Función de densidad</a>
<ul>
<li><a href="#org05e98dc"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge45be08">Función generatriz de momentos</a>
<ul>
<li><a href="#org3b5e0c8"><span class="todo TODO">TODO</span> Demostración</a></li>
<li><a href="#org92d8e66">Esperanza y varianza</a>
<ul>
<li><a href="#org83b9315">Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org128d978">Propiedad de reproductividad</a></li>
<li><a href="#orgfc7e640">Relación con la normal</a>
<ul>
<li><a href="#orgc8a8282"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orgda62d89">Teorema central del límite de Lèvy</a>
<ul>
<li><a href="#orge65ccfe"><span class="todo TODO">TODO</span> Demostración&#xa0;&#xa0;&#xa0;<span class="tag"><span class="extra">extra</span></span></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgeafe0cd">3. Distribución t de Student</a>
<ul>
<li><a href="#orgfe5cdcb">Definición</a>
<ul>
<li><a href="#org806ebb8">Función de densidad</a>
<ul>
<li><a href="#org9cfc778"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org8bf8dc6">Gráfica de la función de densidad</a></li>
</ul>
</li>
<li><a href="#orgac30d6e">Momentos</a>
<ul>
<li><a href="#orgb569151"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org5388100">Aproximación por la normal</a></li>
</ul>
</li>
<li><a href="#orga83d035">4. Distribución F de Snedecor</a>
<ul>
<li><a href="#orge9555a8">Definición</a>
<ul>
<li><a href="#orga26124c">Gráfica de la función de densidad</a></li>
<li><a href="#orgd957e12">Función de densidad</a>
<ul>
<li><a href="#orgf74ebae"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdffeb69">Momentos</a>
<ul>
<li><a href="#org5d01e42"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org134516f">Propiedades</a></li>
<li><a href="#org4e28e12">Aproximación</a></li>
</ul>
</li>
<li><a href="#org8b29b05">5. Distribución exponencial</a>
<ul>
<li><a href="#orgdbb1ab2">Distribución exponencial</a>
<ul>
<li><a href="#org1fc3773">Es una distribución</a></li>
</ul>
</li>
<li><a href="#orgbf3268a">Suma de exponenciales</a></li>
<li><a href="#orgb1099a9">Caso particular de la distribución Gamma</a></li>
</ul>
</li>
<li><a href="#org1e3314f">6. Distribución de Dirichlet</a>
<ul>
<li><a href="#orgeffe7a0">Distribución de Dirichlet</a></li>
<li><a href="#org4aba9d9">Momentos</a>
<ul>
<li><a href="#org63d2f15">Esperanza</a></li>
<li><a href="#orgc5cc9a3">Varianza</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org14febc2">7. Distribución Gamma</a>
<ul>
<li><a href="#orgef91c45">Función Gamma</a>
<ul>
<li><a href="#org9f054f4">La integral está definida</a></li>
</ul>
</li>
<li><a href="#orge59214b">Propiedades de la función Gamma</a>
<ul>
<li><a href="#org11b560f">Demostración</a>
<ul>
<li><a href="#orgd81d3dd">Punto 1</a></li>
<li><a href="#org24a53e3">Punto 2</a></li>
<li><a href="#org118068a">Punto 3</a></li>
<li><a href="#org9c290fc"><span class="todo TODO">TODO</span> Punto 4</a></li>
<li><a href="#org53a864b">Punto 5</a></li>
<li><a href="#orge251b03">Punto 6</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6265e08">Distribución Gamma</a>
<ul>
<li><a href="#org0e4c8c6">Imagen de la distribución</a></li>
</ul>
</li>
<li><a href="#org76e23d0">Propiedades de la distribución Gamma</a>
<ul>
<li><a href="#orgc0aafd9"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orgf535d92">Suma de Gammas</a></li>
</ul>
</li>
<li><a href="#org4508f02">8. Distribución Beta</a>
<ul>
<li><a href="#org22bd1b5">Función Beta</a>
<ul>
<li><a href="#orgc504312">Está bien definida</a></li>
</ul>
</li>
<li><a href="#org0d0ffc8">Relación con la función Gamma</a>
<ul>
<li><a href="#orga7a730d"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org05677bd">Distribución Beta</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgbc3eb9a">1. Introducción a la inferencia estadística. Estadísticos muestrales</a>
<ul>
<li><a href="#org30ff32b">Planteamiento de un problema de inferencia</a>
<ul>
<li><a href="#org9e07086">Modelo estadístico</a></li>
<li><a href="#org8b5373c">Modelo estadístico paramétrico</a></li>
<li><a href="#orga8b33e9">Modelo estadístico no paramétrico</a></li>
<li><a href="#org318af35">Muestra aleatoria simple</a>
<ul>
<li><a href="#org2729c08">Realización muestral</a></li>
<li><a href="#org99ea7f9">Espacio muestral</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org656ce46">Función de distribución empírica</a>
<ul>
<li><a href="#orgb82e86f">Función de distribución muestral</a></li>
<li><a href="#orgaa4f063">Propiedades de la función de distribución empírica</a></li>
<li><a href="#org9d57106">Teorema de Glivenko-Cantelli</a>
<ul>
<li><a href="#org683fb95">Equivalentemente</a></li>
<li><a href="#org1ded43b">Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgae2071f">Estadísticos muestrales</a>
<ul>
<li><a href="#org22a4b2c">Estadístico muestral</a></li>
<li><a href="#orgfa80bbe">Momentos muestrales no centrados</a></li>
<li><a href="#orgedab39d">Momentos muestrales centrados</a></li>
<li><a href="#org6793298">Media muestral</a></li>
<li><a href="#org836e6bd">Varianza muestral</a></li>
<li><a href="#org9a2024a">Cuasivarianza muestral</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org826af87">2. Muestreo de poblaciones normales</a>
<ul>
<li><a href="#org1f2086e">Muestreo de la normal</a>
<ul>
<li><a href="#orga1d9382">Lema de Fisher</a>
<ul>
<li><a href="#orgf74063f"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orgc93e924">A1. Inferencia de la media con varianza conocida</a>
<ul>
<li><a href="#orgd2b91da">Demostración</a></li>
</ul>
</li>
<li><a href="#org56d00d0">A2. Inferencia de la media con varianza desconocida</a>
<ul>
<li><a href="#org9395ef7">Demostración</a></li>
</ul>
</li>
<li><a href="#org0a6f79d">B1. Inferencia de la varianza con media conocida</a>
<ul>
<li><a href="#orga66b341">Demostración</a></li>
</ul>
</li>
<li><a href="#orgb13c8b5">B2. Inferencia de la varianza con media desconocida</a>
<ul>
<li><a href="#org1c9aae6">Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org07d2a90">Muestreo de dos normales</a>
<ul>
<li><a href="#orgb566d8e">Extensión del lema de Fisher</a>
<ul>
<li><a href="#orgdcdda38"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org3a1b5a6">Inferencia sobre diferencia de medias con varianzas conocidas</a>
<ul>
<li><a href="#org861642e">Demostración</a></li>
</ul>
</li>
<li><a href="#org18e2bff">Inferencia sobre diferencia de medias con varianzas iguales</a>
<ul>
<li><a href="#org73a1175">Demostración</a></li>
<li><a href="#org5afdd57">Demostración alternativa</a></li>
</ul>
</li>
<li><a href="#orgdfe25cb">Inferencia sobre cociente de varianzas con media conocida</a>
<ul>
<li><a href="#org0077015">Demostración</a></li>
</ul>
</li>
<li><a href="#orgf4b0cb6">Inferencia sobre cociente de varianzas con media desconocida</a>
<ul>
<li><a href="#org542ed91">Demostración</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org53db6b7">3. Suficiencia y completitud</a>
<ul>
<li><a href="#orgf19ba0e">Estadísticos suficientes</a>
<ul>
<li><a href="#org212bb83">Estadístico suficiente</a>
<ul>
<li><a href="#orgd6ef8a5">Definición equivalente</a></li>
</ul>
</li>
<li><a href="#org233c6ca">Teorema de factorización de Fisher-Neyman</a>
<ul>
<li><a href="#orgb6dbdc9"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orgc6987ff">Propiedades de los estadísticos suficientes</a>
<ul>
<li><a href="#org9f7c392">Demostración</a>
<ul>
<li><a href="#org20575d2">Punto 1</a></li>
<li><a href="#org925c685">Punto 2</a></li>
<li><a href="#orgbb15a11">Punto 3</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4f4102b">Estadísticos completos</a>
<ul>
<li><a href="#org93a0551">Familia de distribuciones completa</a></li>
<li><a href="#org7661d9f">Estadístico completo</a></li>
</ul>
</li>
<li><a href="#org61f0a36">Suficiencia y completitud en familias exponenciales</a>
<ul>
<li><a href="#org4c62c3c">Familia exponencial k-paramétrica</a></li>
<li><a href="#org7840f33">Teorema de suficiencia y complitud</a>
<ul>
<li><a href="#orgd6c3d7d"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org4b63d85">Ejemplo: la normal para la media</a></li>
<li><a href="#org83f7ffc">Ejemplo: la normal para la varianza</a></li>
<li><a href="#orgd52935b">Ejemplo: distribución de Poisson</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org94084de">4. Estimación puntual</a>
<ul>
<li><a href="#orgfd87e2f">Planteamiento del problema de estimación</a>
<ul>
<li><a href="#orgc0011c9">Estimador puntual</a></li>
<li><a href="#org985ca7b">Función de pérdida y de riesgo</a></li>
<li><a href="#org47a7a24">Función de riesgo</a></li>
<li><a href="#org6d8eb3b">Estimador óptimo</a></li>
<li><a href="#orgc27fa32"><span class="todo TODO">TODO</span> Ejemplo de estimador óptimo</a></li>
</ul>
</li>
<li><a href="#org5e5ce25">Estimación de menor error cuadrático</a>
<ul>
<li><a href="#orgbbb2d2d">Función de pérdida cuadrática</a></li>
</ul>
</li>
<li><a href="#org5b15f7e">Estimación insesgada de mínima varianza</a>
<ul>
<li><a href="#org34dd927">Estimador insesgado</a></li>
<li><a href="#org841346c">UMVUE: Estimador insesgado uniformemente de mínima varianza</a>
<ul>
<li><a href="#org6f5cbc6">De segundo orden</a></li>
</ul>
</li>
<li><a href="#org3f94d15">Propiedades del UMVUE</a>
<ul>
<li><a href="#org7a64cad">Unicidad</a></li>
<li><a href="#org9e3225d"><span class="todo TODO">TODO</span> Linealidad</a></li>
</ul>
</li>
<li><a href="#orgc8f931c">Teorema de Raó-Blackwell</a>
<ul>
<li><a href="#orgc897d3d">Demostración</a></li>
</ul>
</li>
<li><a href="#org8d751a4">Teorema de Lehmann-Scheffé</a>
<ul>
<li><a href="#org6742a86">Demostración</a></li>
</ul>
</li>
<li><a href="#org6250747">Cálculo del UMVUE</a></li>
</ul>
</li>
<li><a href="#orgbc96112">Estimación eficiente</a>
<ul>
<li><a href="#org3eaf2fe">Condiciones de regularidad de Fréchet-Cramer-Rao</a></li>
<li><a href="#org90e3ed0">Función de información de Fisher</a></li>
<li><a href="#org2f4494b">Propiedades de la función de información</a>
<ul>
<li><a href="#org1cb92a8">Demostración</a>
<ul>
<li><a href="#org39a90d4">Punto 3</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org91a6e24">Función de información bajo Cramer-Raó</a>
<ul>
<li><a href="#orgdf21237">Demostración</a></li>
</ul>
</li>
<li><a href="#orgd5f6493">Estadístico regular</a></li>
<li><a href="#org4690b21">Cota de Fréchet-Cramer-Raó</a>
<ul>
<li><a href="#org3a85669">Demostración</a>
<ul>
<li><a href="#org891bf35">Primer punto</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf205fa2">Estimador eficiente</a></li>
<li><a href="#org91cc5bc">Caracterización de estimadores eficientes</a>
<ul>
<li><a href="#org140e0de">Demostración</a>
<ul>
<li><a href="#orgd64427c">Primera implicación</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org959e93c">Ejemplo: distribución binomial</a>
<ul>
<li><a href="#orgc2eaaaf">Es regular</a></li>
<li><a href="#org22ca892">Función de información</a></li>
<li><a href="#org80739f8">Caracterización del estimador eficiente</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgcd596a5">Estimación de máxima verosimilitud</a>
<ul>
<li><a href="#orgb31d4f6">Función de verosimilitud</a></li>
<li><a href="#org8795384">Estimador de máxima verosimilitud</a></li>
<li><a href="#orga33a8d4">Ecuación de máxima verosimilitud</a>
<ul>
<li><a href="#org6d9bfd7">Demostración</a></li>
</ul>
</li>
<li><a href="#org94c7bed">Propiedades del estimador de máxima verosimilitud</a>
<ul>
<li><a href="#org9af6116"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orge26bcf6">Relación con estadísticos suficientes</a>
<ul>
<li><a href="#org0d08710">Demostración</a></li>
</ul>
</li>
<li><a href="#orgb5ae354">Relación con estimadores eficientes</a>
<ul>
<li><a href="#orged1eece"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org5ea31c2">Función de verosimilitud de una función paramétrica</a></li>
<li><a href="#orgb703295">Estimador de máxima verosimilitud de una función paramétrica</a></li>
<li><a href="#org0a3025b">Teorema de invarianza de Zenha</a>
<ul>
<li><a href="#orgb577dd6"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgfc04915">Método de los momentos</a>
<ul>
<li><a href="#org16c43b7">Descripción</a>
<ul>
<li><a href="#org1d91dac">Momentos poblacionales</a></li>
<li><a href="#orgda60c10">Momentos muestrales</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2846ea8">Método de mínimos cuadrados</a>
<ul>
<li><a href="#org71fc016">Descripción</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4234235">5. Estimación por intervalos de confianza</a>
<ul>
<li><a href="#org0016bcd">Definiciones y métodos de construcción</a>
<ul>
<li><a href="#org9b6b745">Intervalo de confianza</a></li>
<li><a href="#orga532e5a">Intervalo de confianza de menor longitud esperada uniformemente</a></li>
<li><a href="#org1057dc2">Intervalos mediante desigualdad de Chevychev</a>
<ul>
<li><a href="#org4438f0f"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#orge9c3e3a">Pivote para un parámetro</a></li>
<li><a href="#org5ca1064">Intervalos obtenidos mediante el método pivotal</a>
<ul>
<li><a href="#org4045cd7"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org55f236c">Un pivote en distribuciones continuas</a>
<ul>
<li><a href="#orgca2fbb1"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org3b01743">Un pivote dado un estadístico</a>
<ul>
<li><a href="#orgf35bf1c"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4a19fa7">Ejemplos de intervalos de confianza</a>
<ul>
<li><a href="#orge94e1f8">A1. Intervalo para la media de una normal con varianza conocida</a>
<ul>
<li><a href="#org461cf4a">Pivote</a></li>
<li><a href="#org945382a">Intervalos candidatos</a></li>
<li><a href="#orgd7ad032">Longitud media</a></li>
<li><a href="#orga13ed1d">Minimización</a></li>
<li><a href="#orgea8b44b">Conclusión</a></li>
</ul>
</li>
<li><a href="#orgee1a1eb">A2. Intervalo para la media de una normal con varianza desconocida</a>
<ul>
<li><a href="#org1cea7f8">Pivote</a></li>
<li><a href="#org2eb9068">Intervalos candidatos</a></li>
<li><a href="#orgeffcbdc">Longitud media</a></li>
<li><a href="#org580b753">Minimización</a></li>
<li><a href="#org75ebdf8">Conclusión</a></li>
</ul>
</li>
<li><a href="#org829c481">B1. Intervalo para la varianza de una normal con media conocida</a>
<ul>
<li><a href="#orgd07d205">Pivote</a></li>
<li><a href="#orgd9dbe03">Intervalos candidatos</a></li>
<li><a href="#orgc51b28c">Minimización</a></li>
<li><a href="#org47aec9b">Conclusión</a></li>
</ul>
</li>
<li><a href="#orgc57b8ad">B2. Intervalo para la varianza de una normal con media desconocida</a>
<ul>
<li><a href="#org7d2bc67">Pivote</a></li>
</ul>
</li>
<li><a href="#org716466c">C1. Intervalo para la diferencia de medias de normales de varianza dada</a>
<ul>
<li><a href="#orgb9d5706">Pivote</a></li>
</ul>
</li>
<li><a href="#org0efa616">C2. Intervalo para la diferencia de medias de normales de varianza igual</a>
<ul>
<li><a href="#orga2a61e1">Pivote</a></li>
<li><a href="#org6ce1987">Intervalos candidatos</a></li>
<li><a href="#org0bc6063">Minimización</a></li>
<li><a href="#org6511c01">Conclusión</a></li>
</ul>
</li>
<li><a href="#orge6eadbf">D1. Intervalo para el cociente de varianzas de normales de media dada</a>
<ul>
<li><a href="#org8ecdb0c">Pivote</a></li>
<li><a href="#org6aa70b5">Intervalos candidatos</a></li>
<li><a href="#org38292dc">Minimización</a></li>
<li><a href="#org64c1237">Conclusión</a></li>
</ul>
</li>
<li><a href="#org32ddd3e">D2. Intervalo para el cociente de varianzas de normales de media desconocida</a>
<ul>
<li><a href="#orgdc0372d">Pivote</a></li>
<li><a href="#org13c712d">Intervalos candidatos</a></li>
<li><a href="#orgefd1a6e">Minimización</a></li>
<li><a href="#org7fc3efd">Conclusión</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4b6cdd1"><span class="todo TODO">TODO</span> Intervalos unilaterales</a></li>
</ul>
</li>
<li><a href="#org60695db">6. Contraste de hipótesis</a>
<ul>
<li><a href="#orge05f528">Planteamiento del problema</a>
<ul>
<li><a href="#orgfdec99c">Problema de contraste de hipótesis</a></li>
<li><a href="#org39550bb">Test de hipótesis</a></li>
<li><a href="#org3cf1107">Tipos de errores de un test de hipótesis</a></li>
<li><a href="#orgb99241d">Función de potencia de un test</a></li>
<li><a href="#org9f0c2c5">Tamaño del test</a></li>
<li><a href="#orgf1e7f74">Nivel de significación de un test</a></li>
<li><a href="#org7289311">Test uniformemente más potente</a></li>
</ul>
</li>
<li><a href="#org5f50696">Lema de Neyman-Pearson</a>
<ul>
<li><a href="#org7f12c76">El problema de contraste</a></li>
<li><a href="#orgfb06133">Lema de Neyman-Pearson</a>
<ul>
<li><a href="#org16ba848">Demostración</a>
<ul>
<li><a href="#orgaf79cc8">Punto 1</a></li>
<li><a href="#orgd2c3c41">Punto 3</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge410ae8"><span class="todo TODO">TODO</span> Descripción mediante p-valores</a></li>
<li><a href="#orgc4fe29d">Test de la razón de verosimilitudes</a>
<ul>
<li><a href="#org8cbc477">Test de la razón de verosimilitudes</a></li>
</ul>
</li>
<li><a href="#orge7010c0">Dualidad entre tests de hipótesis y regiones de confianza</a>
<ul>
<li><a href="#org798fca1">Dualidad</a></li>
</ul>
</li>
<li><a href="#org4d2f0d8">Ejemplos</a>
<ul>
<li><a href="#orgbf1dd14">Contrastes sobre la media de una normal con varianza conocida</a>
<ul>
<li><a href="#org8f09b9f">Hipótesis: valor de la media</a>
<ul>
<li><a href="#org63dfc54">Cálculo</a></li>
</ul>
</li>
<li><a href="#org0f20095">Hipótesis: media menor que un valor</a>
<ul>
<li><a href="#org8e9c285">Cálculo</a></li>
</ul>
</li>
<li><a href="#orgb3a9931">Hipótesis: media mayor que un valor</a>
<ul>
<li><a href="#org15830bf">Cálculo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0596573">Contrastes sobre la varianza de una normal con media conocida</a>
<ul>
<li><a href="#orga54712c">Hipótesis: valor de la varianza</a>
<ul>
<li><a href="#orgf492bda">Cálculo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org447fb5c">Contrastes sobre la varianza de una normal con media desconocida</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org656d6be">7. Teoría general de modelos lineales</a>
<ul>
<li><a href="#org131bcd0">Modelo lineal general y modelo de Gauss Markov</a>
<ul>
<li><a href="#org5c06303">Modelo lineal general</a>
<ul>
<li><a href="#org65b8e4c">Vector observable</a></li>
<li><a href="#org3d58a38">Matriz de diseño</a></li>
<li><a href="#org8d00146">Vector de efectos</a></li>
<li><a href="#orgd8b1625">Vector de errores</a></li>
</ul>
</li>
<li><a href="#orga7e4ee7">Modelo de Gauss-Markov</a>
<ul>
<li><a href="#org79b52c0">Enunciado vectorial</a></li>
<li><a href="#orgc989357">Objetivo del modelo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org02bfb18">Estimación de mínimos cuadrados del vector de efectos</a>
<ul>
<li><a href="#orga081b63">Modelo</a>
<ul>
<li><a href="#orga3a0996">Minimización</a></li>
</ul>
</li>
<li><a href="#org7df021e">Estimador de mínimos cuadrados de beta</a></li>
</ul>
</li>
<li><a href="#org90c443a">Funciones estimables</a>
<ul>
<li><a href="#org87bc814">Función lineal estimable</a></li>
<li><a href="#org67b2817">Teorema de Gauss-Markov</a>
<ul>
<li><a href="#org481ff3d"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org087df29">Propiedades del estimador de mínimos cuadrados en modelos de rango máximo</a>
<ul>
<li><a href="#orgd2b99fe"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0dbcd10">Modelo estimado</a>
<ul>
<li><a href="#org41e3ae2">Modelo estimado</a></li>
<li><a href="#orgc7bad59">Propiedades del modelo estimado</a></li>
<li><a href="#orgc5232ca">Varianza residual</a></li>
</ul>
</li>
<li><a href="#org44aef74">Inferencia bajo hipótesis de normalidad</a>
<ul>
<li><a href="#orge100856">Hipótesis de normalidad</a>
<ul>
<li><a href="#org892c9dc">Equivalentemente</a></li>
</ul>
</li>
<li><a href="#org83cc5aa">Función de máxima verosimilitud</a></li>
<li><a href="#org149e533">Estimadores máximo verosímiles de efectos</a>
<ul>
<li><a href="#org5e55ec2"><span class="todo TODO">TODO</span> Demostración</a></li>
</ul>
</li>
<li><a href="#org11171e2">Estimador máximo verosímil de la varianza</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgbe3038f">8. Inferencia Bayesiana</a>
<ul>
<li><a href="#org6a431a9">8.1. Introducción</a>
<ul>
<li><a href="#orgb12c179">Ley de la probabilidad total</a></li>
<li><a href="#org06cb249">Teorema de Bayes</a></li>
<li><a href="#org96e35d3">Distribución a priori</a></li>
<li><a href="#orgeb0c738">Distribución condicionada</a></li>
<li><a href="#org7aec626">Distribución marginal</a></li>
<li><a href="#org6523947">Distribución a posteriori</a></li>
</ul>
</li>
<li><a href="#org32d406d">8.2. Estadística clásica</a></li>
<li><a href="#org691478b">8.3. Familias conjugadas</a>
<ul>
<li><a href="#org5ca33d4">Familia conjugada</a></li>
<li><a href="#org9afd4a7">Lema de caracterización de familias conjugadas</a>
<ul>
<li><a href="#orgac6f1e3">Demostración</a>
<ul>
<li><a href="#orge80d9b9">Primera implicación</a></li>
<li><a href="#org7757e43">Segunda implicación</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org58bc776">Caracterización de familias conjugadas</a></li>
<li><a href="#orge4c5ede">Ejemplos de familias conjugadas</a>
<ul>
<li><a href="#orgb195d3e">Beta para Bernoulli</a></li>
<li><a href="#orge6b7b69">Gamma para Poisson</a></li>
<li><a href="#orgf665912">Normales para normales con varianza conocida</a></li>
<li><a href="#orgee683fb">Dirichlet para multinomiales</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5193363">8.4. Distribuciones objetivas</a>
<ul>
<li><a href="#orgfb931c0">Distribución de Jeffreys</a></li>
</ul>
</li>
<li><a href="#org8377e61">8.5. Convergencia de distribuciones a posteriori</a>
<ul>
<li><a href="#org46f7df0">Convergencia en un espacio paramétrico discreto</a>
<ul>
<li><a href="#org6613476">Demostración</a></li>
</ul>
</li>
<li><a href="#orgf7fd4ab">Nota: Influencia de la distribución a priori</a></li>
<li><a href="#orgc70b50f">Nota: Estimadores bayesianos</a></li>
</ul>
</li>
<li><a href="#org3d8faaf">8.6. Test de hipóesis bayesianos</a>
<ul>
<li><a href="#org7421314">Probabilidad a posteriori de un modelo</a>
<ul>
<li><a href="#org6d0be88">Modelos</a></li>
</ul>
</li>
<li><a href="#org96c3d94">Factor de Bayes</a></li>
<li><a href="#orgd979158">Método de Leamer: motivación</a></li>
<li><a href="#org2f3a474">Método de Leamer: muestras de entrenamiento</a></li>
<li><a href="#org7f5ce1a">Método de Leamer</a></li>
</ul>
</li>
<li><a href="#org5da64c0"><span class="todo TODO">TODO</span> 8.7. Probabilidades subjetivas</a></li>
</ul>
</li>
<li><a href="#org0d93f93">Ejercicios</a>
<ul>
<li><a href="#orgcc13569">Tema 1. Introducción a la inferencia estadística</a>
<ul>
<li><a href="#orge5db0bd">Ejercicio 1</a>
<ul>
<li><a href="#orgdab2a2c">Punto 1</a></li>
<li><a href="#org90c3768">Punto 2</a></li>
</ul>
</li>
<li><a href="#orgdb67317">Ejercicio 2</a>
<ul>
<li><a href="#org3a885f9">Punto 1</a></li>
<li><a href="#orgc600652">Punto 2</a></li>
</ul>
</li>
<li><a href="#orgd984552">Ejercicio 4</a>
<ul>
<li><a href="#org2d7ee8c">Probabilidad de que difieran</a>
<ul>
<li><a href="#org385fe52">Cálculos</a></li>
</ul>
</li>
<li><a href="#org58ca85e"><span class="todo TODO">TODO</span> Tamaño muestral</a>
<ul>
<li><a href="#orgfd2d0ba">Cálculos</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org700feb0">Ejercicio 7</a>
<ul>
<li><a href="#org67893fe">Punto 1</a></li>
<li><a href="#org6ba7a28">Punto 2</a></li>
<li><a href="#org8b37b8f">Punto 3</a></li>
</ul>
</li>
<li><a href="#org4983dd2">Ejercicio 10</a></li>
</ul>
</li>
<li><a href="#org29be098">Tema 2. Distribuciones en el muestreo de poblaciones normales</a>
<ul>
<li><a href="#orgb04c660">Ejercicio 1</a>
<ul>
<li><a href="#org11f942a">Probabilidad de la Cuasivarianza</a>
<ul>
<li><a href="#org5fb849c">Cálculos</a></li>
</ul>
</li>
<li><a href="#org96bf7b9">Probabilidad de la Media Muestral</a>
<ul>
<li><a href="#orgab5c2e2">Cálculos</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org27c3641">Ejercicio 3</a>
<ul>
<li><a href="#org4951602">Cálculos</a></li>
</ul>
</li>
<li><a href="#org3d54b72">Ejercicio 7</a></li>
</ul>
</li>
<li><a href="#org1842b61">Tema 3. Suficiencia y complitud</a>
<ul>
<li><a href="#org70bd7e7">Ejercicio 1</a>
<ul>
<li><a href="#org92a1a77">Usando la definición</a></li>
<li><a href="#org0a803e6">Usando el teorema de factorización</a></li>
</ul>
</li>
<li><a href="#org672fa9a">Ejercicio 3</a></li>
</ul>
</li>
<li><a href="#org3d10af5">Tema 4. Estimación puntual. Métodos de estimación</a>
<ul>
<li><a href="#orgd22c451">Ejercicio 2</a>
<ul>
<li><a href="#orgae6a776">Punto 1. Es insesgado.</a></li>
<li><a href="#org116c806">Punto 1. Es el UMVUE.</a></li>
<li><a href="#org7c95781">Punto 2.</a></li>
<li><a href="#org47c0036"><span class="todo TODO">TODO</span> Punto 3.</a></li>
</ul>
</li>
<li><a href="#orge395b6d">Ejercicio 3</a></li>
<li><a href="#org96c2c93">Ejercicio 7</a>
<ul>
<li><a href="#org824d803">Punto 1</a></li>
<li><a href="#org892b84f"><span class="done DONE">DONE</span> Punto 2</a></li>
</ul>
</li>
<li><a href="#org0bafbe6">Ejercicio 8</a>
<ul>
<li><a href="#orge825c41"><span class="done DONE">DONE</span> Punto 1</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc2aa37c">Tema 5. Intervalos de confianza</a>
<ul>
<li><a href="#org6eed77b">Ejercicio 1</a></li>
<li><a href="#orge7ed95f">Ejercicio 2</a>
<ul>
<li><a href="#orgc54a11f">Primer punto</a></li>
<li><a href="#orgb903f8a">Segundo punto</a></li>
</ul>
</li>
<li><a href="#orga1c865c">Ejercicio 5</a></li>
<li><a href="#org18b333b">Ejercicio 7</a>
<ul>
<li><a href="#org853fbb9">Primer punto</a></li>
</ul>
</li>
<li><a href="#org2d4f2ae">Ejercicio 9</a></li>
</ul>
</li>
<li><a href="#orgf721f3a">Tema 6. Contraste de hipotésis</a>
<ul>
<li><a href="#orgebcd250">Ejercicio 1</a></li>
<li><a href="#orgb2e7239">Ejercicio 2</a>
<ul>
<li><a href="#org093e17b">Tamaño del test</a></li>
<li><a href="#orga6f5a94">Potencia frente alternativas</a></li>
</ul>
</li>
<li><a href="#orgb40cbbb">Ejercicio 3</a>
<ul>
<li><a href="#orga88bd1f">Desarrollo teórico</a></li>
<li><a href="#org625e79b">Aplicación</a></li>
</ul>
</li>
<li><a href="#orge4679de">Ejercicio 4</a></li>
<li><a href="#orgc1204a3"><span class="done DONE">DONE</span> Ejercicio 5</a></li>
<li><a href="#orgd508ec6">Ejercicio 6</a></li>
<li><a href="#orga0dbc18">Ejercicio 8</a></li>
<li><a href="#orgff46d03">Ejercicio 9</a></li>
<li><a href="#org0374612">Ejercicio 10</a></li>
<li><a href="#org9636e8d">Ejercicio 13</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgc5764eb" class="outline-2">
<h2 id="orgc5764eb">Prerrequisitos</h2>
<div class="outline-text-2" id="text-orgc5764eb">
</div><div id="outline-container-org88ef035" class="outline-3">
<h3 id="org88ef035">Distribuciones</h3>
<div class="outline-text-3" id="text-org88ef035">
</div><div id="outline-container-orge967534" class="outline-4">
<h4 id="orge967534">Función generatriz de momentos</h4>
<div class="outline-text-4" id="text-orge967534">
<p>
Se define para una variable aleatoria \(X\) con función de distribución
\(f\) como:
</p>

<p>
\[
M_X(t) = 
\mathbb{E}(e^{tX}) =
\int_\Omega e^{tx}f(x) \;dx
\]
</p>
</div>

<div id="outline-container-orgc86c261" class="outline-5">
<h5 id="orgc86c261">Cálculo de momentos</h5>
<div class="outline-text-5" id="text-orgc86c261">
<p>
Se cumple que:
</p>

<p>
\[
\mathbb{E}[X^n] = \frac{\partial^n}{\partial t^n} M_X(0)
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgfdfbfed" class="outline-4">
<h4 id="orgfdfbfed">Función característica</h4>
<div class="outline-text-4" id="text-orgfdfbfed">
<p>
Se define para una variable aleatoria \(X\) con función de distribución \(f\):
</p>

<p>
\[
\varphi_X(t) = \mathbb{E}[e^{itX}] = \int_\Omega e^{itx}f(x)\;dx
\]
</p>
</div>

<div id="outline-container-orgf0adbbd" class="outline-5">
<h5 id="orgf0adbbd">Cálculo de momentos</h5>
<div class="outline-text-5" id="text-orgf0adbbd">
<p>
Se cumple que:
</p>

<p>
\[
\varphi_X^{(n)}(0) = i^n\mathbb{E}[X^n]
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org2192093" class="outline-3">
<h3 id="org2192093">Varianza</h3>
<div class="outline-text-3" id="text-org2192093">
</div><div id="outline-container-org45ed572" class="outline-4">
<h4 id="org45ed572">Varianza</h4>
<div class="outline-text-4" id="text-org45ed572">
<p>
La varianza se define equivalentemente como:
</p>

<p>
\[Var(X) = E\Big[(X-EX)^2\Big] = E[X^2] - E[X]^2\]
</p>
</div>
</div>

<div id="outline-container-orgc547fed" class="outline-4">
<h4 id="orgc547fed">Covarianza</h4>
<div class="outline-text-4" id="text-orgc547fed">
<p>
La covarianza se define equivalentemente como:
</p>

<p>
\[cov(X,Y) = E[(X-EX)(Y-EY)] = E[XY] - E[X]E[Y]\]
</p>

<p>
Nótese que \(cov(X,X) = Var(X)\). Nótese además se comporta como el 
<a href="https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products">producto interno</a> de un espacio prehilbertiano.
</p>
</div>
</div>

<div id="outline-container-org1e30b99" class="outline-4">
<h4 id="org1e30b99">Varianza de la suma</h4>
<div class="outline-text-4" id="text-org1e30b99">
<p>
La varianza de una suma cumple:
</p>

<p>
\[
Var(X+Y) = Var(X) + Var(Y) + 2cov(X,Y)
\]
</p>

<p>
En el caso general:
</p>

<p>
\[Var\left(\sum X_i\right) = \sum_i\sum_j cov(X_i,X_j)\]
</p>
</div>
</div>

<div id="outline-container-org49c32eb" class="outline-4">
<h4 id="org49c32eb">Cauchy-Schwarz para la covarianza</h4>
<div class="outline-text-4" id="text-org49c32eb">
<p>
Se tiene la desigualdad:
</p>

<p>
\[cov(X,Y)^2 \leq Var(X)Var(Y)
\]
</p>
</div>

<div id="outline-container-org76d2d71" class="outline-5">
<h5 id="org76d2d71">Demostración</h5>
<div class="outline-text-5" id="text-org76d2d71">
<p>
Sabiendo que la varianza es siempre no negativa:
</p>

<p>
\[
0 \leq Var\left(X - \frac{cov(X,Y)}{Var(Y)} Y\right) =
Var(X) - \frac{\left(cov(X,Y)\right)^2}{Var(Y)}
\]
</p>
</div>
</div>

<div id="outline-container-org30e970a" class="outline-5">
<h5 id="org30e970a">Demostración por Cauchy-Schwarz</h5>
<div class="outline-text-5" id="text-org30e970a">
<p>
Se comprueba que la covarianza da un producto escalar que genera
un <a href="https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products">espacio cociente</a> prehilbertiano. Aplicamos Cauchy-Schwarz.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org1d245ac" class="outline-3">
<h3 id="org1d245ac">Esperanza condicional</h3>
<div class="outline-text-3" id="text-org1d245ac">
</div><div id="outline-container-orgde4e432" class="outline-4">
<h4 id="orgde4e432">Esperanza condicional en caso discreto</h4>
<div class="outline-text-4" id="text-orgde4e432">
<p>
Definimos la esperanza condicional de dos variables discretas como:
</p>

<p>
\[\mathbb{E}[X|Y] = \sum_x xP(X=x\mid Y=y) = \sum_x x\frac{P(X=x, Y=y)}{P(Y=y)}\]
</p>
</div>
</div>

<div id="outline-container-org996cf72" class="outline-4">
<h4 id="org996cf72">Esperanza condicional en el caso continuo</h4>
<div class="outline-text-4" id="text-org996cf72">
<p>
Más generalmente se define para el caso continuo:
</p>

<p>
\[
\mathbb{E}[X|Y] = \int_X x f_{X|Y}(x|y) dx = \int_X x \frac{f_{X,Y}(x,y)}{f_Y(y)} dx
\]
</p>
</div>
</div>

<div id="outline-container-org087896d" class="outline-4">
<h4 id="org087896d">Ley de esperanza total</h4>
<div class="outline-text-4" id="text-org087896d">
<p>
La esperanza condicional cumple:
</p>

<p>
\[\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]\]
</p>
</div>

<div id="outline-container-org2e94e33" class="outline-5">
<h5 id="org2e94e33">Demostración en el caso discreto</h5>
<div class="outline-text-5" id="text-org2e94e33">
<p>
Se tiene:
</p>

<p>
\[\begin{aligned}
E[E[X|Y]] &= \int_Y f(y)  \left(\int_X x \frac{f(x,y)}{f(y)} dx \right) dy \\ñ
&= \int_X x \int_Y f(x,y) dy dx \\&= \int_X x f(x) dx = E[X]
\end{aligned}\]
</p>

<p>
Nótese que asumimos una conmutatividad de las integrales discretas.
</p>
</div>
</div>

<div id="outline-container-org914f5b9" class="outline-5">
<h5 id="org914f5b9">Demostración en el caso continuo</h5>
<div class="outline-text-5" id="text-org914f5b9">
<p>
Puede consultarse la <a href="https://en.wikipedia.org/wiki/Law_of_total_expectation#Proof_in_the_general_case">Ley de la esperanza total</a>.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgfe5d52b" class="outline-3">
<h3 id="orgfe5d52b">Desigualdades</h3>
<div class="outline-text-3" id="text-orgfe5d52b">
</div><div id="outline-container-org5423165" class="outline-4">
<h4 id="org5423165">Desigualdad de Chebyshev</h4>
<div class="outline-text-4" id="text-org5423165">
<p>
Para una variable aleatoria \(X\) de segundo orden:
</p>

<p>
\[
P(|X-\mathbb{E}[X]| \geq a) \leq \frac{Var(X)}{a^2}
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org690d215" class="outline-3">
<h3 id="org690d215">Convergencia</h3>
<div class="outline-text-3" id="text-org690d215">
</div><div id="outline-container-org73c99a6" class="outline-4">
<h4 id="org73c99a6">Convergencia casi segura</h4>
<div class="outline-text-4" id="text-org73c99a6">
<p>
Una sucesión de variables aleatorias converge de forma casi segura a
otra \(X_n \overset{c.s.}\longrightarrow X\) cuando el conjunto de sucesos que lo hacen tiene 
probabilidad 1.
</p>

<p>
\[
P\left(\lim_{n\to\infty} X_n = X\right) = 1
\]
</p>
</div>
</div>

<div id="outline-container-orgae09b74" class="outline-4">
<h4 id="orgae09b74">Convergencia en probabilidad</h4>
<div class="outline-text-4" id="text-orgae09b74">
<p>
Una sucesión de variables aleatorias converge en probabilidad a
otra \(X_n \overset{P}\longrightarrow X\) cuando:
</p>

<p>
\[
\lim_{n\to\infty} P\left(|X_n-X| \geq \varepsilon\right) = 0
\]
</p>

<p>
para cualquier \(\varepsilon\).
</p>
</div>

<div id="outline-container-orgbcf122e" class="outline-5">
<h5 id="orgbcf122e">Equivalentemente</h5>
<div class="outline-text-5" id="text-orgbcf122e">
<p>
Si consideramos su complemento:
</p>

<p>
\[
\lim_{n\to\infty} P(|X_n-X| < \varepsilon) = 1
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgdc4479f" class="outline-4">
<h4 id="orgdc4479f">Convergencia en distribución</h4>
<div class="outline-text-4" id="text-orgdc4479f">
<p>
Una sucesión de variables aleatorias converge en ley o en distribución 
a otra \(X_n \overset{d}\longrightarrow X\), si se tiene que, dadas sus funciones de distribución,
convergen en los puntos en los que es continua:
</p>

<p>
\[
\forall x: F \mbox{ continua en } x:
\quad
\lim_{n\to\infty} F_n(x) = F(x)
\]
</p>
</div>

<div id="outline-container-orgdf71a4c" class="outline-5">
<h5 id="orgdf71a4c">Equivalentemente</h5>
<div class="outline-text-5" id="text-orgdf71a4c">
<p>
Se tiene \(X_n\overset{d}\longrightarrow X\) si para cualquier \(t\) real:
</p>

<p>
\[
\lim_{n\to\infty} E\left[ e^{tX_n} \right] = E\left[e^{tX}\right]
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org46dc453" class="outline-4">
<h4 id="org46dc453">Implicaciones</h4>
<div class="outline-text-4" id="text-org46dc453">
<p>
La convergencia casi segura implica la convergencia en probabilidad, que
implica a su vez la convergencia en distribución.
</p>
</div>

<div id="outline-container-org8aa2908" class="outline-5">
<h5 id="org8aa2908"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org5b32272" class="outline-4">
<h4 id="org5b32272">Ley débil de los grandes números</h4>
<div class="outline-text-4" id="text-org5b32272">
<p>
Si \(X_1,X_2,\dots\) es una sucesión infinita de variables aleatorias 
independientes con la misma esperanza y varianza, entonces:
</p>

<p>
\[
\overline{X}_n = \frac{1}{n}(X_1+\dots+X_n)
\]
</p>

<p>
converge en probabilidad a \(\mu\):
</p>

<p>
\[
\lim_{n\to\infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0
\]
</p>
</div>
</div>

<div id="outline-container-orgda36df7" class="outline-4">
<h4 id="orgda36df7">Ley fuerte de los grandes números</h4>
<div class="outline-text-4" id="text-orgda36df7">
<p>
Si \(X_1,X_2,\dots\) es una sucesión infinita de variables aleatorias 
independientes e idénticamente distribuidas con \(E\left[|X_i|\right] < \infty\) y
valor esperado \(\mu\), entonces:
</p>

<p>
\[
P\left(
\lim_{n\to\infty} \overline{X}_n = \mu
\right) = 1
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org528e1ed" class="outline-2">
<h2 id="org528e1ed">Distribuciones discretas</h2>
<div class="outline-text-2" id="text-org528e1ed">
</div><div id="outline-container-org64713b4" class="outline-3">
<h3 id="org64713b4">1. Distribución uniforme</h3>
<div class="outline-text-3" id="text-org64713b4">
</div><div id="outline-container-org00af3b3" class="outline-4">
<h4 id="org00af3b3">Definición</h4>
<div class="outline-text-4" id="text-org00af3b3">
<p>
Se define sobre un conjunto finito de valores \(\{x_i\}\) con la misma 
probabilidad como:
</p>

<p>
\[f(x_i|n) = \frac{1}{n}\]
</p>
</div>
</div>
</div>
<div id="outline-container-org66d91a4" class="outline-3">
<h3 id="org66d91a4">2. Distribución binomial</h3>
<div class="outline-text-3" id="text-org66d91a4">
</div><div id="outline-container-orgc166885" class="outline-4">
<h4 id="orgc166885">Definición</h4>
<div class="outline-text-4" id="text-orgc166885">
<p>
Determina la probabilidad de \(x\) aciertos en \(n\) experimentos de Bernoulli.
La función de distribución de \(B(x|n,p)\) es:
</p>

<p>
\[
f(x|n,p) = {n \choose x}p^x (1-p)^{n-x}
\]
</p>
</div>

<div id="outline-container-orgea90c29" class="outline-5">
<h5 id="orgea90c29">Esperanza</h5>
<div class="outline-text-5" id="text-orgea90c29">
<p>
\[\mathbb{E}[X] = np\]
</p>
</div>
</div>

<div id="outline-container-org9d541f5" class="outline-5">
<h5 id="org9d541f5">Varianza</h5>
<div class="outline-text-5" id="text-org9d541f5">
<p>
\[Var[X] = np(1-p)\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd753926" class="outline-3">
<h3 id="orgd753926">3. Distribución multinomial</h3>
<div class="outline-text-3" id="text-orgd753926">
</div><div id="outline-container-org3840848" class="outline-4">
<h4 id="org3840848">Definición</h4>
<div class="outline-text-4" id="text-org3840848">
<p>
Deriva de una binomial con \(k\) salidas distintas de probabilidades \(p_1,\dots,p_k\)
como:
</p>

<p>
\[
f(X|n,p_1,\dots,p_k) = \frac{n!}{X_1!X_2!\dots X_n!}p^{X_1}p^{X_2}\dots p^{X_k}
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgdb4b49e" class="outline-3">
<h3 id="orgdb4b49e">4. Distribución de Poisson</h3>
<div class="outline-text-3" id="text-orgdb4b49e">
</div><div id="outline-container-org96eecca" class="outline-4">
<h4 id="org96eecca">Definición</h4>
<div class="outline-text-4" id="text-org96eecca">
<p>
Definimos la distribución de Poisson \(Poi(\lambda)\) como:
</p>

<p>
\[
f(n|\lambda) = \frac{e^{-\lambda}\lambda^n}{n!}
\]
</p>
</div>

<div id="outline-container-org050b1bc" class="outline-5">
<h5 id="org050b1bc">Es una distribución</h5>
<div class="outline-text-5" id="text-org050b1bc">
<p>
Comprobamos que suma la unidad:
</p>

<p>
\[
\sum_{n=1}^\infty f(n|\lambda) =
\sum_{n=1}^\infty e^{-\lambda}\frac{\lambda^n}{n!} = 1
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgb62eeee" class="outline-4">
<h4 id="orgb62eeee">Función generatriz de momentos</h4>
<div class="outline-text-4" id="text-orgb62eeee">
<p>
La función generatriz se calcula como:
</p>

<p>
\[
M_X(t) = \sum_{n=1}^\infty e^{-\lambda}\frac{(e^t\lambda)^n}{n!}
= e^{\lambda(e^t-1)}
\]
</p>
</div>

<div id="outline-container-org27ad75f" class="outline-5">
<h5 id="org27ad75f">Esperanza</h5>
<div class="outline-text-5" id="text-org27ad75f">
<p>
Desde la función generadora:
</p>

<p>
\[
\mathbb{E}[X] = \frac{\partial M_X}{\partial t}(0) = \lambda
\]
</p>
</div>
</div>

<div id="outline-container-orgae14292" class="outline-5">
<h5 id="orgae14292">Varianza</h5>
<div class="outline-text-5" id="text-orgae14292">
<p>
Desde la función generadora:
</p>

<p>
\[
Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \lambda
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgc723a7d" class="outline-4">
<h4 id="orgc723a7d">Suma de Poisson</h4>
<div class="outline-text-4" id="text-orgc723a7d">
<p>
Para \(X \leadsto Poi(\lambda_1)\), \(Y \leadsto Poi(\lambda_2)\) independientes, su suma sigue la
distribución con el parámetro suma.
</p>

<p>
\[
X + Y \leadsto Poi(\lambda_1+\lambda_2)
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org53e6171" class="outline-2">
<h2 id="org53e6171">Distribuciones continuas</h2>
<div class="outline-text-2" id="text-org53e6171">
</div><div id="outline-container-org1e27188" class="outline-3">
<h3 id="org1e27188">1. Distribución normal</h3>
<div class="outline-text-3" id="text-org1e27188">
</div><div id="outline-container-orgd21e166" class="outline-4">
<h4 id="orgd21e166">Definición</h4>
<div class="outline-text-4" id="text-orgd21e166">
<p>
Definimos la distribución normal \({\cal N}(\mu,\sigma^2)\) como aquella con función de
densidad:
</p>

<p>
\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
</p>
</div>

<div id="outline-container-orgd390cc5" class="outline-5">
<h5 id="orgd390cc5">Imagen de la distribución</h5>
<div class="outline-text-5" id="text-orgd390cc5">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #e5786d;">library</span>(ggplot2)
<span style="color: #e5786d;">library</span>(ggfortify)
gg = <span style="color: #92a65e; font-weight: bold;">NULL</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> (i <span style="color: #8ac6f2; font-weight: bold;">in</span> c(1,2,3,4,5)) {
    gg = ggdistribution(dnorm, seq(-3, 3, 0.1),
                        mean = 0, sd = 1+0.1*i,
                        colour=topo.colors(5)[i], p=gg)
}
print(gg + labs(title = <span style="color: #95e454;">"Distribuci&#243;n normal, variando &#963;&#178;."</span>))
</pre>
</div>


<div class="figure">
<p><img src="images/normal.png" alt="normal.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgce91029" class="outline-5">
<h5 id="orgce91029">Es una distribución</h5>
<div class="outline-text-5" id="text-orgce91029">
<p>
Tenemos que comprobar que integra la unidad sobre los reales, y
de hecho, tomando cambio de variable \(y = (x-\mu)/\sqrt{2\sigma^2}\) queda:
</p>

<p>
\[\begin{aligned}
\int^{+\infty}_{-\infty} 
\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2} =
\frac{1}{\sqrt{\pi}}\int^{+\infty}_{-\infty} 
e^{-y^2} = 1
\end{aligned}\]
</p>

<p>
Que es la <a href="https://en.wikipedia.org/wiki/Gaussian_integral">integral de Gauss</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-org0780546" class="outline-4">
<h4 id="org0780546">Función característica</h4>
<div class="outline-text-4" id="text-org0780546">
<p>
La función característica de \({\cal N}(\mu,\sigma^2)\) es:
</p>

<p>
\[\varphi_X(t) = e^{it\mu - t^2\sigma^2/2}\]
</p>
</div>

<div id="outline-container-orgd1f8e2f" class="outline-5">
<h5 id="orgd1f8e2f"><span class="todo TODO">TODO</span> Demostración</h5>
<div class="outline-text-5" id="text-orgd1f8e2f">
<p>
Usamos la definición de función característica y completamos
cuadrados para tener:
</p>

<p>
\[\begin{aligned}
\varphi_X(t) &= 
\mathbb{E}\left[e^{itX}\right] &= 
\int_{-\infty}^{+\infty}
e^{itx}\frac{1}{\sqrt{2\pi\sigma^2}} 
e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx \\&=
\frac{1}{\sqrt{2\pi\sigma^2}} 
\int_{-\infty}^{+\infty}
e^{it\mu}
\end{aligned}\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgda127ba" class="outline-4">
<h4 id="orgda127ba">Suma de normales</h4>
<div class="outline-text-4" id="text-orgda127ba">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) e \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\). Entonces \(X+Y\leadsto {\cal N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\).
</p>
</div>

<div id="outline-container-orgf6e18ed" class="outline-5">
<h5 id="orgf6e18ed"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org7c8673f" class="outline-4">
<h4 id="org7c8673f">Producto por escalar</h4>
<div class="outline-text-4" id="text-org7c8673f">
<p>
Si \(X \leadsto {\cal N}(\mu,\sigma^2)\), entonces \(kX \leadsto {\cal N}(k\mu,k\sigma^2)\).
</p>
</div>

<div id="outline-container-org1503b51" class="outline-5">
<h5 id="org1503b51"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
<div id="outline-container-org1dc8553" class="outline-4">
<h4 id="org1dc8553">Teorema de Cramer</h4>
<div class="outline-text-4" id="text-org1dc8553">
<p>
Sean \(X,Y\) independientes. Si \(X+Y\) es normal, \(X\) e \(Y\) son normales.
</p>
</div>

<div id="outline-container-org400d30f" class="outline-5">
<h5 id="org400d30f"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
</div>
<div id="outline-container-org248b986" class="outline-3">
<h3 id="org248b986">2. Distribución χ² de Pearson</h3>
<div class="outline-text-3" id="text-org248b986">
</div><div id="outline-container-orgf72b604" class="outline-4">
<h4 id="orgf72b604">Distribución chi cuadrado</h4>
<div class="outline-text-4" id="text-orgf72b604">
<p>
Es un caso particular de la distribución gamma, \(X \leadsto \chi^2(k) = \Gamma(k/2,1/2)\).
Al parámetro \(k\) se le llama <b>número de grados de libertad</b>.
</p>
</div>

<div id="outline-container-org2536e1f" class="outline-5">
<h5 id="org2536e1f">Gráfica de la función de densidad</h5>
<div class="outline-text-5" id="text-org2536e1f">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #e5786d;">library</span>(ggplot2)
<span style="color: #e5786d;">library</span>(ggfortify)
gg = <span style="color: #92a65e; font-weight: bold;">NULL</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> (i <span style="color: #8ac6f2; font-weight: bold;">in</span> c(1,2,3,4,5)) {
    gg = ggdistribution(dchisq, seq(0, 6, 0.1),
                        df = i,
                        colour=topo.colors(5)[i], p=gg)
}
print(gg + labs(title = <span style="color: #95e454;">"Distribuci&#243;n &#967;&#178;(n), variando n."</span>))
</pre>
</div>


<div class="figure">
<p><img src="images/chi.png" alt="chi.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgd4f7819" class="outline-5">
<h5 id="orgd4f7819">Función de densidad</h5>
<div class="outline-text-5" id="text-orgd4f7819">
<p>
\[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]
</p>
</div>

<div id="outline-container-org05e98dc" class="outline-6">
<h6 id="org05e98dc"><span class="todo TODO">TODO</span> Demostración</h6>
</div>
</div>
</div>
<div id="outline-container-orge45be08" class="outline-4">
<h4 id="orge45be08">Función generatriz de momentos</h4>
<div class="outline-text-4" id="text-orge45be08">
<p>
\[M_X(t) = \frac{1}{(1-2t)^{k/2}}\], para \(t < 1/2\).
</p>
</div>

<div id="outline-container-org3b5e0c8" class="outline-5">
<h5 id="org3b5e0c8"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
<div id="outline-container-org92d8e66" class="outline-5">
<h5 id="org92d8e66">Esperanza y varianza</h5>
<div class="outline-text-5" id="text-org92d8e66">
<ul class="org-ul">
<li>\(E[X] = k\)</li>
<li>\(Var[X] = 2k\)</li>
</ul>
</div>

<div id="outline-container-org83b9315" class="outline-6">
<h6 id="org83b9315">Demostración</h6>
<div class="outline-text-6" id="text-org83b9315">
<p>
Se calculan desde la función generatriz.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org128d978" class="outline-4">
<h4 id="org128d978">Propiedad de reproductividad</h4>
<div class="outline-text-4" id="text-org128d978">
<p>
Si tengo una serie de variables independientes distribuidas 
por \(X_i \leadsto \chi^2(k_i)\), entonces:
</p>

<p>
\[\sum_{i=1}^n X_i \leadsto \chi^2 \left(\sum_{i=1}^n k_i \right)\]
</p>
</div>
</div>

<div id="outline-container-orgfc7e640" class="outline-4">
<h4 id="orgfc7e640">Relación con la normal</h4>
<div class="outline-text-4" id="text-orgfc7e640">
<p>
Dadas variables independientes \(X_i \leadsto {\cal N}(0,1)\),
</p>

<p>
\[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]
</p>
</div>

<div id="outline-container-orgc8a8282" class="outline-5">
<h5 id="orgc8a8282"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-orgda62d89" class="outline-4">
<h4 id="orgda62d89">Teorema central del límite de Lèvy</h4>
<div class="outline-text-4" id="text-orgda62d89">
<p>
Para valores pequeños, pueden usarse tablas. Para valores grandes
de \(n\), podemos aproximarla mediante el Teorema Central del Límite
como:
</p>

<p>
\[ \chi^2(n) \approx {\cal N}(n,2n)\]
</p>
</div>

<div id="outline-container-orge65ccfe" class="outline-5">
<h5 id="orge65ccfe"><span class="todo TODO">TODO</span> Demostración&#xa0;&#xa0;&#xa0;<span class="tag"><span class="extra">extra</span></span></h5>
</div>
</div>
</div>
<div id="outline-container-orgeafe0cd" class="outline-3">
<h3 id="orgeafe0cd">3. Distribución t de Student</h3>
<div class="outline-text-3" id="text-orgeafe0cd">
</div><div id="outline-container-orgfe5cdcb" class="outline-4">
<h4 id="orgfe5cdcb">Definición</h4>
<div class="outline-text-4" id="text-orgfe5cdcb">
<p>
Dadas dos variables independientes \(X \leadsto {\cal N}(0,1)\) e \(Y \leadsto \chi^2(n)\),
tenemos:
</p>

<p>
\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]
</p>
</div>

<div id="outline-container-org806ebb8" class="outline-5">
<h5 id="org806ebb8">Función de densidad</h5>
<div class="outline-text-5" id="text-org806ebb8">
<p>
\[ f(t) 
= \frac
{\Gamma\left(\frac{n+1}{2}\right)}
{\Gamma\left(\frac{n}{2}\right) \sqrt{n\pi}} 
\left(
1 + \frac{t^2}{n}
\right)^{-\frac{n+1}{2}}
\], \(t \in \mathbb{R}\)
</p>
</div>

<div id="outline-container-org9cfc778" class="outline-6">
<h6 id="org9cfc778"><span class="todo TODO">TODO</span> Demostración</h6>
</div>
</div>

<div id="outline-container-org8bf8dc6" class="outline-5">
<h5 id="org8bf8dc6">Gráfica de la función de densidad</h5>
<div class="outline-text-5" id="text-org8bf8dc6">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #e5786d;">library</span>(ggplot2)
<span style="color: #e5786d;">library</span>(ggfortify)
gg = <span style="color: #92a65e; font-weight: bold;">NULL</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> (i <span style="color: #8ac6f2; font-weight: bold;">in</span> c(1,2,3,4,5)) {
    gg = ggdistribution(dt, seq(0, 3, 0.1),
                        df=i,
                        colour=topo.colors(5)[i], p=gg)
}
print(gg + labs(title = <span style="color: #95e454;">"T de Student, variando n."</span>))
</pre>
</div>


<div class="figure">
<p><img src="images/tstudent.png" alt="tstudent.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgac30d6e" class="outline-4">
<h4 id="orgac30d6e">Momentos</h4>
<div class="outline-text-4" id="text-orgac30d6e">
<p>
Tenemos que \(\exists E[T^k] \iff k < n\). Cuando existen, se tiene
</p>

<ul class="org-ul">
<li>\(E[T] = 0\)</li>
<li>\(Var[T] = \frac{n}{n-2}\)</li>
</ul>
</div>

<div id="outline-container-orgb569151" class="outline-5">
<h5 id="orgb569151"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org5388100" class="outline-4">
<h4 id="org5388100">Aproximación por la normal</h4>
<div class="outline-text-4" id="text-org5388100">
<p>
Tabulada para \(n\) pequeños y aproximada por \({\cal N}(0,1)\) para valores
grandes.
</p>
</div>
</div>
</div>
<div id="outline-container-orga83d035" class="outline-3">
<h3 id="orga83d035">4. Distribución F de Snedecor</h3>
<div class="outline-text-3" id="text-orga83d035">
</div><div id="outline-container-orge9555a8" class="outline-4">
<h4 id="orge9555a8">Definición</h4>
<div class="outline-text-4" id="text-orge9555a8">
<p>
<b>F de Snedecor</b>. Dadas dos variables independientes \(X \leadsto \chi^2(m)\) e
\(Y \leadsto \chi^2(n)\), su cociente nos da:
</p>

<p>
\[F = \frac{X/m}{Y/n} \leadsto F(m,n)\]
</p>
</div>

<div id="outline-container-orga26124c" class="outline-5">
<h5 id="orga26124c">Gráfica de la función de densidad</h5>
<div class="outline-text-5" id="text-orga26124c">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #e5786d;">library</span>(ggplot2)
<span style="color: #e5786d;">library</span>(ggfortify)
gg = <span style="color: #92a65e; font-weight: bold;">NULL</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> (i <span style="color: #8ac6f2; font-weight: bold;">in</span> c(1,2,3,4,5)) {
    gg = ggdistribution(df, seq(0, 2, 0.04),
                        df1=i, df2=i-1,
                        colour=topo.colors(5)[i], p=gg)
}
print(gg + labs(title = <span style="color: #95e454;">"F de Snedecor, variando n y m."</span>))
</pre>
</div>


<div class="figure">
<p><img src="images/fsnedecor.png" alt="fsnedecor.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgd957e12" class="outline-5">
<h5 id="orgd957e12">Función de densidad</h5>
<div class="outline-text-5" id="text-orgd957e12">
<p>
\[g(t)
= \frac
{\Gamma(\frac{m+n}{2})}
{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}
\left(\frac{m}{n}\right)^{\frac{m}{2}}
t^{m/2-1}
\left(1+\frac{m}{n}t\right)^{-\frac{m+n}{2}}\], para \(t>0\).
</p>
</div>

<div id="outline-container-orgf74ebae" class="outline-6">
<h6 id="orgf74ebae"><span class="todo TODO">TODO</span> Demostración</h6>
</div>
</div>
</div>

<div id="outline-container-orgdffeb69" class="outline-4">
<h4 id="orgdffeb69">Momentos</h4>
<div class="outline-text-4" id="text-orgdffeb69">
<p>
Tenemos que \(\exists E[T^k] \iff k < n/2\).
</p>

<ul class="org-ul">
<li>\(n > 2 \Rightarrow \exists E[F] = \frac{n}{n-2}\)</li>
<li>\(n > 4 \Rightarrow \exists Var[F] = \frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)}\)</li>
</ul>
</div>

<div id="outline-container-org5d01e42" class="outline-5">
<h5 id="org5d01e42"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org134516f" class="outline-4">
<h4 id="org134516f">Propiedades</h4>
<div class="outline-text-4" id="text-org134516f">
<p>
\[ F \leadsto F(m,n) \iff F^{-1} \leadsto F(n,m)\]
\[T \leadsto t(n) \iff T^2 \leadsto F(1,n)\]
</p>
</div>
</div>

<div id="outline-container-org4e28e12" class="outline-4">
<h4 id="org4e28e12">Aproximación</h4>
<div class="outline-text-4" id="text-org4e28e12">
<p>
La distribución está tabulada y las tablas incluyen aproximaciones 
para valores grandes de \(n\) y \(m\).
</p>
</div>
</div>
</div>

<div id="outline-container-org8b29b05" class="outline-3">
<h3 id="org8b29b05">5. Distribución exponencial</h3>
<div class="outline-text-3" id="text-org8b29b05">
</div><div id="outline-container-orgdbb1ab2" class="outline-4">
<h4 id="orgdbb1ab2">Distribución exponencial</h4>
<div class="outline-text-4" id="text-orgdbb1ab2">
<p>
Dado un \(\lambda>0\), definimos la distribución exponencial, \(\operatorname{Exp}(\lambda)\), como aquella
con función de densidad:
</p>

<p>
\[f(x) = \lambda e^{-\lambda x}\qquad \forall x \in \mathbb{R}^+_0\]
</p>
</div>

<div id="outline-container-org1fc3773" class="outline-5">
<h5 id="org1fc3773">Es una distribución</h5>
<div class="outline-text-5" id="text-org1fc3773">
<p>
Trivialmente integrando:
</p>

<p>
\[
\int_0^\infty \lambda e^{-\lambda x} = 
-\left[ e^{-\lambda x} \right]^\infty_0 = 1
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgbf3268a" class="outline-4">
<h4 id="orgbf3268a">Suma de exponenciales</h4>
<div class="outline-text-4" id="text-orgbf3268a">
<p>
La suma de variables exponenciales es una distribución Gamma:
</p>
</div>
</div>

<div id="outline-container-orgb1099a9" class="outline-4">
<h4 id="orgb1099a9">Caso particular de la distribución Gamma</h4>
<div class="outline-text-4" id="text-orgb1099a9">
<p>
La exponencial es un caso particular de la distribución Gamma:
</p>

<p>
\[
Exp(\lambda) = \Gamma(1,\lambda)
\]
</p>
</div>
</div>
</div>
<div id="outline-container-org1e3314f" class="outline-3">
<h3 id="org1e3314f">6. Distribución de Dirichlet</h3>
<div class="outline-text-3" id="text-org1e3314f">
</div><div id="outline-container-orgeffe7a0" class="outline-4">
<h4 id="orgeffe7a0">Distribución de Dirichlet</h4>
<div class="outline-text-4" id="text-orgeffe7a0">
<p>
Dado un vector de reales \(\alpha_1,\alpha_2,\dots,\alpha_n\), definimos la distribución \(Dir(\alpha)\) 
como la que tiene función de densidad:
</p>

<p>
\[
f(x) = \frac{1}{B(\alpha)} \prod_{i=1}^K x_i^{\alpha_i-1}
\]
</p>

<p>
donde,
</p>

<p>
\[
B(\alpha) =
\frac
{\prod_{i=1}^K \Gamma(\alpha_i)}
{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}
\]
</p>
</div>
</div>

<div id="outline-container-org4aba9d9" class="outline-4">
<h4 id="org4aba9d9">Momentos</h4>
<div class="outline-text-4" id="text-org4aba9d9">
</div><div id="outline-container-org63d2f15" class="outline-5">
<h5 id="org63d2f15">Esperanza</h5>
<div class="outline-text-5" id="text-org63d2f15">
<p>
\[
E[X_i] = \frac{\alpha_i}{\sum_k \alpha_k}
\]
</p>
</div>
</div>

<div id="outline-container-orgc5cc9a3" class="outline-5">
<h5 id="orgc5cc9a3">Varianza</h5>
<div class="outline-text-5" id="text-orgc5cc9a3">
<p>
\[
Var[X_i] = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}
\]
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org14febc2" class="outline-3">
<h3 id="org14febc2">7. Distribución Gamma</h3>
<div class="outline-text-3" id="text-org14febc2">
</div><div id="outline-container-orgef91c45" class="outline-4">
<h4 id="orgef91c45">Función Gamma</h4>
<div class="outline-text-4" id="text-orgef91c45">
<p>
Se define la función gamma \(\Gamma : (0,\infty) \longrightarrow (0,\infty)\) como:
</p>

<p>
\[\Gamma(\alpha) = \int^\infty_0 t^{\alpha-1}e^{-t} dt\]
</p>
</div>

<div id="outline-container-org9f054f4" class="outline-5">
<h5 id="org9f054f4">La integral está definida</h5>
<div class="outline-text-5" id="text-org9f054f4">
<p>
Por un lado, \(t^{a-1}e^{-t} < t^{a-1}\), integrable en \([0,b]\). Por otro lado,
</p>

<p>
\[\lim_{t \to \infty} \frac{t^{\alpha-1}e^{-t}}{e^{-t/2}} = 0\]
</p>

<p>
Por lo que \(t^{\alpha-1}e^{-t} < e^{-t/2}\) integrable, a partir de algún punto.
Partimos la integral como:
</p>

<p>
\[\int_0^b t^{\alpha-1}e^{-t}dt + \int^{\infty}_b t^{\alpha-1}e^{-t}dt
< \infty\]
</p>
</div>
</div>
</div>

<div id="outline-container-orge59214b" class="outline-4">
<h4 id="orge59214b">Propiedades de la función Gamma</h4>
<div class="outline-text-4" id="text-orge59214b">
<p>
Sea \(\alpha > 0\), se verifica:
</p>

<ol class="org-ol">
<li>\(\Gamma(1) = 1\).</li>
<li>\(\Gamma(\alpha+1) = \alpha\Gamma(\alpha)\).</li>
<li>\(\Gamma(n+1) = n!\) para \(n \in \mathbb{N}\).</li>
<li>\(\Gamma(\alpha)\Gamma(1-\alpha) = \frac{\pi}{\sin(\alpha\pi)}\) para \(0<\alpha<1\).</li>
<li>\(\Gamma(1/2) = \sqrt{\pi}\).</li>
<li>\(\Gamma(\alpha) = \beta^\alpha \int^\infty_0 t^{\alpha-1}e^{-\beta t} dt\) para \(\beta > 0\).</li>
</ol>
</div>

<div id="outline-container-org11b560f" class="outline-5">
<h5 id="org11b560f">Demostración</h5>
<div class="outline-text-5" id="text-org11b560f">
</div><div id="outline-container-orgd81d3dd" class="outline-6">
<h6 id="orgd81d3dd">Punto 1</h6>
<div class="outline-text-6" id="text-orgd81d3dd">
<p>
Trivial.
</p>
</div>
</div>
<div id="outline-container-org24a53e3" class="outline-6">
<h6 id="org24a53e3">Punto 2</h6>
<div class="outline-text-6" id="text-org24a53e3">
<p>
Integral por partes.
</p>
</div>
</div>
<div id="outline-container-org118068a" class="outline-6">
<h6 id="org118068a">Punto 3</h6>
<div class="outline-text-6" id="text-org118068a">
<p>
Inducción sobre los dos primeros apartados.
</p>
</div>
</div>
<div id="outline-container-org9c290fc" class="outline-6">
<h6 id="org9c290fc"><span class="todo TODO">TODO</span> Punto 4</h6>
</div>
<div id="outline-container-org53a864b" class="outline-6">
<h6 id="org53a864b">Punto 5</h6>
<div class="outline-text-6" id="text-org53a864b">
<p>
Trivial desde el punto anterior.
</p>
</div>
</div>
<div id="outline-container-orge251b03" class="outline-6">
<h6 id="orge251b03">Punto 6</h6>
<div class="outline-text-6" id="text-orge251b03">
<p>
Cambio de variable \(\varphi(t) = \beta t\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org6265e08" class="outline-4">
<h4 id="org6265e08">Distribución Gamma</h4>
<div class="outline-text-4" id="text-org6265e08">
<p>
Dados \(\alpha,\beta > 0\), definimos la distribución Gamma \(\Gamma(\alpha,\beta)\) como aquella con
función de densidad:
</p>

<p>
\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\]
</p>

<p>
para \(x>0\). 
</p>
</div>

<div id="outline-container-org0e4c8c6" class="outline-5">
<h5 id="org0e4c8c6">Imagen de la distribución</h5>
<div class="outline-text-5" id="text-org0e4c8c6">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #e5786d;">library</span>(ggplot2)
<span style="color: #e5786d;">library</span>(ggfortify)
gg = <span style="color: #92a65e; font-weight: bold;">NULL</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> (i <span style="color: #8ac6f2; font-weight: bold;">in</span> c(1,2,3,4,5)) {
    gg = ggdistribution(dgamma, seq(0, 4, 0.05),
                        shape = i, rate = 1,
                        colour=topo.colors(5)[i], p=gg)
}
print(gg + labs(title = <span style="color: #95e454;">"Distribuci&#243;n gamma, variando &#945;."</span>))
</pre>
</div>


<div class="figure">
<p><img src="images/gamma.png" alt="gamma.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org76e23d0" class="outline-4">
<h4 id="org76e23d0">Propiedades de la distribución Gamma</h4>
<div class="outline-text-4" id="text-org76e23d0">
<p>
La función de densidad de una distribución \(\Gamma(\alpha,\beta)\) verifica:
</p>

<ol class="org-ol">
<li>Cuando \(0<\alpha<1\), \(f\) es decreciente y \(\lim_{x\to 0} f(x) = \infty\).</li>
<li>Cuando \(\alpha = 1\), \(f\) es decreciente y \(f(0)=1\).</li>
<li>Cuando \(\alpha>1\), \(f\) es creciente en \([0,(\alpha-1)/\beta]\) y decreciente 
en \([(\alpha-1)/\beta,\infty]\).</li>
</ol>

<p>
Y sobre convexidad y concavidad se tiene:
</p>

<ol class="org-ol">
<li>Si \(0<\alpha\leq 1\), es convexa.</li>
<li>Si \(1 < \alpha \leq 2\), es cóncava en \([0,(\alpha-1+\sqrt{\alpha+1})/\beta]\) y convexa
en \([(\alpha-1+\sqrt{\alpha+1})/\beta,\infty]\).</li>
<li>Si \(2 < \alpha\), es cóncava en \([(\alpha-1-\sqrt{\alpha+1})/\beta,(\alpha-1+\sqrt{\alpha+1}/\beta)]\) y
convexa en todo el resto del dominio.</li>
</ol>
</div>

<div id="outline-container-orgc0aafd9" class="outline-5">
<h5 id="orgc0aafd9"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
<div id="outline-container-orgf535d92" class="outline-4">
<h4 id="orgf535d92">Suma de Gammas</h4>
<div class="outline-text-4" id="text-orgf535d92">
<p>
Para \(X \leadsto \Gamma(\alpha_1,\beta)\), \(Y \leadsto \Gamma(\alpha_2,\beta)\), independientes:
</p>

<p>
\[
X+Y \leadsto \Gamma(\alpha_1+\alpha_2,\beta)
\]
</p>
</div>
</div>
</div>
<div id="outline-container-org4508f02" class="outline-3">
<h3 id="org4508f02">8. Distribución Beta</h3>
<div class="outline-text-3" id="text-org4508f02">
</div><div id="outline-container-org22bd1b5" class="outline-4">
<h4 id="org22bd1b5">Función Beta</h4>
<div class="outline-text-4" id="text-org22bd1b5">
<p>
Se define la función beta \(\beta : (0,\infty) \longrightarrow (0,\infty)\) como:
</p>

<p>
\[\beta(x,y) = \int^1_0 t^{x-1}(1-t)^{y-1}dt\]
</p>
</div>

<div id="outline-container-orgc504312" class="outline-5">
<h5 id="orgc504312">Está bien definida</h5>
<div class="outline-text-5" id="text-orgc504312">
<p>
Por la <a href="#org0d0ffc8">relación con la función Gamma</a> sabemos que debe estar
bien definida.
</p>
</div>
</div>
</div>

<div id="outline-container-org0d0ffc8" class="outline-4">
<h4 id="org0d0ffc8">Relación con la función Gamma</h4>
<div class="outline-text-4" id="text-org0d0ffc8">
<p>
Para cada \(x,y\) se tiene:
</p>

<p>
\[\frac{\Gamma(x)\Gamma(y)}{\Gamma(xy)} = \beta(x,y)\]
</p>
</div>

<div id="outline-container-orga7a730d" class="outline-5">
<h5 id="orga7a730d"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org05677bd" class="outline-4">
<h4 id="org05677bd">Distribución Beta</h4>
<div class="outline-text-4" id="text-org05677bd">
<p>
Dados \(p,q>0\), definimos la distribución Beta \(\beta(p,q)\) como aquella con 
función de densidad:
</p>

<p>
\[f(x) = \frac{1}{\beta(p,q)} x^{p-1}(1-x)^{q-1}\]
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbc3eb9a" class="outline-2">
<h2 id="orgbc3eb9a">1. Introducción a la inferencia estadística. Estadísticos muestrales</h2>
<div class="outline-text-2" id="text-orgbc3eb9a">
</div><div id="outline-container-org30ff32b" class="outline-3">
<h3 id="org30ff32b">Planteamiento de un problema de inferencia</h3>
<div class="outline-text-3" id="text-org30ff32b">
</div><div id="outline-container-org9e07086" class="outline-4">
<h4 id="org9e07086">Modelo estadístico</h4>
<div class="outline-text-4" id="text-org9e07086">
<p>
Un modelo estadístico \((X,{\cal P})\) consta de:
</p>

<ul class="org-ul">
<li>\(X : (\Omega, {\cal A},{\cal P}) \longrightarrow (\mathbb{R},{\cal B},P_X)\) variable aleatoria que describe el 
objeto de estudio.</li>
<li>\({\cal P}\) familia de distribuciones que pueden ser la de \(X\).</li>
</ul>
</div>
</div>

<div id="outline-container-org8b5373c" class="outline-4">
<h4 id="org8b5373c">Modelo estadístico paramétrico</h4>
<div class="outline-text-4" id="text-org8b5373c">
<p>
Cuando se conoce la forma funcional de \(P_X\) y sólo desconocemos un 
parámetro tenemos una familia paramétrica de distribuciones \(F(x,\theta)\) 
para \(\theta\).
</p>
</div>
</div>

<div id="outline-container-orga8b33e9" class="outline-4">
<h4 id="orga8b33e9">Modelo estadístico no paramétrico</h4>
<div class="outline-text-4" id="text-orga8b33e9">
<p>
Cuando la forma funcional de \(P_X\) es desconocida.
</p>
</div>
</div>

<div id="outline-container-org318af35" class="outline-4">
<h4 id="org318af35">Muestra aleatoria simple</h4>
<div class="outline-text-4" id="text-org318af35">
<p>
Una muestra aleatoria simple es un vector \((X_1,\dots,X_n)\)
de variables independientes idénticamente distribuidas. 
</p>
</div>

<div id="outline-container-org2729c08" class="outline-5">
<h5 id="org2729c08">Realización muestral</h5>
<div class="outline-text-5" id="text-org2729c08">
<p>
Una realización muestral a un valor concreto obtenido al
observar la muestra.
</p>
</div>
</div>

<div id="outline-container-org99ea7f9" class="outline-5">
<h5 id="org99ea7f9">Espacio muestral</h5>
<div class="outline-text-5" id="text-org99ea7f9">
<p>
Conjunto de todas las posibles realizaciones.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org656ce46" class="outline-3">
<h3 id="org656ce46">Función de distribución empírica</h3>
<div class="outline-text-3" id="text-org656ce46">
</div><div id="outline-container-orgb82e86f" class="outline-4">
<h4 id="orgb82e86f">Función de distribución muestral</h4>
<div class="outline-text-4" id="text-orgb82e86f">
<p>
La <b>función de distribución empírica</b> es una función de 
distribución razonable que podemos obtener desde una 
realización muestral.
</p>

<p>
\[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(X_i < x)} \]
</p>
</div>
</div>

<div id="outline-container-orgaa4f063" class="outline-4">
<h4 id="orgaa4f063">Propiedades de la función de distribución empírica</h4>
<div class="outline-text-4" id="text-orgaa4f063">
<p>
Fijado un \(x \in \mathbb{R}\), \(F^\ast(x)\) es una variable aleatoria siguiendo por 
definición una binomial:
</p>

<p>
\[ nF^\ast(x) \leadsto {\cal B}(n, F(x))\]
</p>

<p>
Calculamos su <b>esperanza</b> y <b>varianza</b> desde Bernoulli como:
</p>

<ul class="org-ul">
<li>Esperanza: \(E[F^\ast(x)] = F(x)\)</li>
<li>Varianza: \(Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}\)</li>
</ul>

<p>
Aplicando entonces el Teorema Central del Límite:
</p>

<p>
\[ \frac{F^\ast(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \leadsto {\cal N}(0,1) \]
</p>
</div>
</div>

<div id="outline-container-org9d57106" class="outline-4">
<h4 id="org9d57106">Teorema de Glivenko-Cantelli</h4>
<div class="outline-text-4" id="text-org9d57106">
<p>
Las funciones de distribución muestrales convergen 
casi seguramente uniformemente a la teórica.
</p>

<p>
\[ P\left\{ \lim_{n \rightarrow \infty} 
\sup_{x \in \mathbb{R}} |F^\ast_n(x) - F(x)| = 0\right\} = 1\]
</p>
</div>

<div id="outline-container-org683fb95" class="outline-5">
<h5 id="org683fb95">Equivalentemente</h5>
<div class="outline-text-5" id="text-org683fb95">
<p>
Con probabilidad 1 se tiene que, al tomar sucesivas observaciones 
independientes y considerar las correspondientes funciones de 
distribución muestrales:
</p>

<p>
\[\forall x \in \mathbb{R}: \forall \epsilon>0: \exists n_\epsilon : \forall n \geq n_\epsilon:
\quad F^\ast_n(x) - \epsilon < F_X(x) < F^\ast_n(x) + \epsilon\]
</p>
</div>
</div>

<div id="outline-container-org1ded43b" class="outline-5">
<h5 id="org1ded43b">Demostración</h5>
<div class="outline-text-5" id="text-org1ded43b">
<p>
<a href="http://matematicas.unex.es/~nogales/estadisticamatematica/TGC.pdf">Teorema de Glivenko-Cantelli</a>.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgae2071f" class="outline-3">
<h3 id="orgae2071f">Estadísticos muestrales</h3>
<div class="outline-text-3" id="text-orgae2071f">
</div><div id="outline-container-org22a4b2c" class="outline-4">
<h4 id="org22a4b2c">Estadístico muestral</h4>
<div class="outline-text-4" id="text-org22a4b2c">
<p>
Dada una muestra aleatoria simple, un <b>estadístico muestral</b> es una 
función sobre ella \(T : (\mathbb{R}^n,{\cal B}^n)\longrightarrow (\mathbb{R}^k,{\cal B}^k)\) medible e independiente 
de cualquier parámetro desconocido.
</p>
</div>
</div>

<div id="outline-container-orgfa80bbe" class="outline-4">
<h4 id="orgfa80bbe">Momentos muestrales no centrados</h4>
<div class="outline-text-4" id="text-orgfa80bbe">
<p>
Para cada \(k \in \mathbb{N}\):
</p>

<p>
\[A_k = \frac{1}{n}\sum_{i=1}^n X_i^k\]
</p>
</div>
</div>

<div id="outline-container-orgedab39d" class="outline-4">
<h4 id="orgedab39d">Momentos muestrales centrados</h4>
<div class="outline-text-4" id="text-orgedab39d">
<p>
Para cada \(k \in \mathbb{N}\):
</p>

<p>
\[B_k = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^k\]
</p>
</div>
</div>

<div id="outline-container-org6793298" class="outline-4">
<h4 id="org6793298">Media muestral</h4>
<div class="outline-text-4" id="text-org6793298">
<p>
Caso particular,
</p>

<p>
\[A_1 = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X}\]
</p>
</div>
</div>

<div id="outline-container-org836e6bd" class="outline-4">
<h4 id="org836e6bd">Varianza muestral</h4>
<div class="outline-text-4" id="text-org836e6bd">
<p>
Caso particular,
</p>

<p>
\[B_2 = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2\]
</p>
</div>
</div>

<div id="outline-container-org9a2024a" class="outline-4">
<h4 id="org9a2024a">Cuasivarianza muestral</h4>
<div class="outline-text-4" id="text-org9a2024a">
<p>
\[S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org826af87" class="outline-2">
<h2 id="org826af87">2. Muestreo de poblaciones normales</h2>
<div class="outline-text-2" id="text-org826af87">
</div><div id="outline-container-org1f2086e" class="outline-3">
<h3 id="org1f2086e">Muestreo de la normal</h3>
<div class="outline-text-3" id="text-org1f2086e">
</div><div id="outline-container-orga1d9382" class="outline-4">
<h4 id="orga1d9382">Lema de Fisher</h4>
<div class="outline-text-4" id="text-orga1d9382">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple con \(X \leadsto {\cal N}(\mu,\sigma^2)\).
Los estadísticos \(\overline{X}\) y \(S^2\) son independientes.
</p>
</div>

<div id="outline-container-orgf74063f" class="outline-5">
<h5 id="orgf74063f"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-orgc93e924" class="outline-4">
<h4 id="orgc93e924">A1. Inferencia de la media con varianza conocida</h4>
<div class="outline-text-4" id="text-orgc93e924">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\), y \(\overline{X}\) su media muestral:
</p>

<p>
\[
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)
\]
</p>
</div>

<div id="outline-container-orgd2b91da" class="outline-5">
<h5 id="orgd2b91da">Demostración</h5>
<div class="outline-text-5" id="text-orgd2b91da">
<p>
Usando las propiedades de la suma de normales y la linealidad de la
esperanza y cuadracidad de la varianza tenemos:
</p>

<p>
\[
\overline{X} \leadsto {\cal N}(\mu,\sigma^2/n)
\]
</p>

<p>
Desde donde simplemente normalizamos.
</p>
</div>
</div>
</div>

<div id="outline-container-org56d00d0" class="outline-4">
<h4 id="org56d00d0">A2. Inferencia de la media con varianza desconocida</h4>
<div class="outline-text-4" id="text-org56d00d0">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\) con \(\overline{X}\) su media muestral y \(S^2\) su cuasivarianza muestral,
entonces:
</p>

<p>
\[
\frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]
</p>
</div>

<div id="outline-container-org9395ef7" class="outline-5">
<h5 id="org9395ef7">Demostración</h5>
<div class="outline-text-5" id="text-org9395ef7">
<p>
Por la definición de t de Student, sabiendo:
</p>

<p>
\[
\frac{\overline{X} - \mu}{S/\sqrt{n}}
=
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)S^2}{\sigma^2}/n-1}}
\leadsto
t(n-1)
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org0a6f79d" class="outline-4">
<h4 id="org0a6f79d">B1. Inferencia de la varianza con media conocida</h4>
<div class="outline-text-4" id="text-org0a6f79d">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\), entonces:
</p>

<p>
\[
\sum_{i=1}^n \left(\frac{X_i-\mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]
</p>
</div>

<div id="outline-container-orga66b341" class="outline-5">
<h5 id="orga66b341">Demostración</h5>
<div class="outline-text-5" id="text-orga66b341">
<p>
Usando que la suma de cuadrados de normales estándar es una
distribución chi cuadrado.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb13c8b5" class="outline-4">
<h4 id="orgb13c8b5">B2. Inferencia de la varianza con media desconocida</h4>
<div class="outline-text-4" id="text-orgb13c8b5">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\) con \(S^2\) su cuasivarianza muestral, entonces:
</p>

<p>
\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]
</p>
</div>

<div id="outline-container-org1c9aae6" class="outline-5">
<h5 id="org1c9aae6">Demostración</h5>
<div class="outline-text-5" id="text-org1c9aae6">
<p>
Usamos la independencia entre \(X_i-\overline{X}\) y \(\overline{X}-\mu\) para escribir:
</p>

<p>
\[
\sum_{i=1}^n (X_i - \mu)^2
=
\sum_{i=1}^n (X_i - \overline{X})^2 +
\sum_{i=1}^n (\overline{X} - \mu)^2
\]
</p>

<p>
Ahora bien, sabemos que:
</p>

<p>
\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]
</p>

<p>
\[
n\left(\frac{\overline{X} - \mu}{\sigma}\right)^2}
\leadsto
\chi^2(1)
\]
</p>

<p>
Y desde aquí, por unicidad de las funciones generadoras de momentos
se tiene:
</p>

<p>
\[
\sum_{i=1}^n \frac{(X_i-\overline{X})^2}{\sigma^2}
\leadsto
\chi^2(n-1)
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org07d2a90" class="outline-3">
<h3 id="org07d2a90">Muestreo de dos normales</h3>
<div class="outline-text-3" id="text-org07d2a90">
</div><div id="outline-container-orgb566d8e" class="outline-4">
<h4 id="orgb566d8e">Extensión del lema de Fisher</h4>
<div class="outline-text-4" id="text-orgb566d8e">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) y \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\) independientes.
Los vectores \((\overline{X},\overline{Y})\) y \((S^2_1,S^2_2)\) son independientes.
</p>
</div>

<div id="outline-container-orgdcdda38" class="outline-5">
<h5 id="orgdcdda38"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
<div id="outline-container-org3a1b5a6" class="outline-4">
<h4 id="org3a1b5a6">Inferencia sobre diferencia de medias con varianzas conocidas</h4>
<div class="outline-text-4" id="text-org3a1b5a6">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) y \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\) independientes:
</p>

<p>
\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1}{\sigma_1^2} + \frac{(n_2-1)S^2_2}{\sigma_2^2}}
\sqrt{\frac{\sigma_1^2/n_1 + \sigma_2^2/n_2}{n_1+n_2-2}}
}
\leadsto
t(n_1+n_2-2)
\]
</p>
</div>

<div id="outline-container-org861642e" class="outline-5">
<h5 id="org861642e">Demostración</h5>
<div class="outline-text-5" id="text-org861642e">
<p>
El numerador sigue una distribución \({\cal N}(0,\sigma^2_1/n_1+\sigma^2_2/n_2)\), así que lo
dividimos para una normal estándar. Cada uno de los sumandos de
la otra raíz forma una chi cuadrada, que al sumarse da \(\chi(n_1+n_2-2)\).
</p>

<p>
Usamos entonces la definición de t de Student.
</p>
</div>
</div>
</div>
<div id="outline-container-org18e2bff" class="outline-4">
<h4 id="org18e2bff">Inferencia sobre diferencia de medias con varianzas iguales</h4>
<div class="outline-text-4" id="text-org18e2bff">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma^2)\), \(Y \leadsto {\cal N}(\mu_2,\sigma^2)\) independientes:
</p>

<p>
\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1+ (n_2-1)S^2_2}{n_1+n_2-2}}
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]
</p>
</div>

<div id="outline-container-org73a1175" class="outline-5">
<h5 id="org73a1175">Demostración</h5>
<div class="outline-text-5" id="text-org73a1175">
<p>
Desde el caso anterior, tomando las varianzas iguales.
</p>
</div>
</div>

<div id="outline-container-org5afdd57" class="outline-5">
<h5 id="org5afdd57">Demostración alternativa</h5>
<div class="outline-text-5" id="text-org5afdd57">
<p>
El numerador sigue una \({\cal N}(0,\sigma^2(1/n_1+1/n_2))\). Podemos dividirlo por
la raíz de la varianza e incluir otra varianza en la raíz de las
cuasivarianzas muestrales para tener una \(\chi^2(n_1+n_2-2)\).
</p>

<p>
Aplicamos definición de t de Student.
</p>
</div>
</div>
</div>
<div id="outline-container-orgdfe25cb" class="outline-4">
<h4 id="orgdfe25cb">Inferencia sobre cociente de varianzas con media conocida</h4>
<div class="outline-text-4" id="text-orgdfe25cb">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) y \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\) independientes con muestras de
tamaños \(n_1\) y \(n_2\):
</p>

<p>
\[
\frac
{\sum_{i=1}^{n_1}(X_i-\mu_1)^2 / n_1\sigma_1^2}
{\sum_{i=1}^{n_2}(X_i-\mu_2)^2 / n_2\sigma_2^2}
\leadsto
F(n_1,n_2)
\]
</p>
</div>

<div id="outline-container-org0077015" class="outline-5">
<h5 id="org0077015">Demostración</h5>
<div class="outline-text-5" id="text-org0077015">
<p>
Desde la definición de la F de Snedecor, sabiendo que cada factor
es una chi cuadrada.
</p>
</div>
</div>
</div>

<div id="outline-container-orgf4b0cb6" class="outline-4">
<h4 id="orgf4b0cb6">Inferencia sobre cociente de varianzas con media desconocida</h4>
<div class="outline-text-4" id="text-orgf4b0cb6">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\) y \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\) independientes:
</p>

<p>
\[
\frac
{S_1^2/\sigma_1^2}
{S_2^2/\sigma_2^2}
\leadsto
F(n-1,m-1)
\]
</p>
</div>

<div id="outline-container-org542ed91" class="outline-5">
<h5 id="org542ed91">Demostración</h5>
<div class="outline-text-5" id="text-org542ed91">
<p>
Aplicando la definición de F de Snedecor y sabiendo que son dos
distribuciones chi cuadrado.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org53db6b7" class="outline-2">
<h2 id="org53db6b7">3. Suficiencia y completitud</h2>
<div class="outline-text-2" id="text-org53db6b7">
</div><div id="outline-container-orgf19ba0e" class="outline-3">
<h3 id="orgf19ba0e">Estadísticos suficientes</h3>
<div class="outline-text-3" id="text-orgf19ba0e">
</div><div id="outline-container-org212bb83" class="outline-4">
<h4 id="org212bb83">Estadístico suficiente</h4>
<div class="outline-text-4" id="text-org212bb83">
<p>
Un estadístico \(t\) es <b>suficiente</b> para un parámetro \(\theta\) cuando una vez 
conocido no puede obtenerse más información de sobre \(\theta\) de los datos;
esto es:
</p>

<p>
\[\Pr(\theta| t,x) = \Pr(\theta|t)\]
</p>
</div>

<div id="outline-container-orgd6ef8a5" class="outline-5">
<h5 id="orgd6ef8a5">Definición equivalente</h5>
<div class="outline-text-5" id="text-orgd6ef8a5">
<p>
De forma equivalente, es <b>suficiente</b> si la distribución condicionada 
al estadístico es independiente del parámetro \(\theta\):
</p>

<p>
\[\Pr(x|t,\theta) = \Pr(x|t)\]
</p>
</div>
</div>
</div>

<div id="outline-container-org233c6ca" class="outline-4">
<h4 id="org233c6ca">Teorema de factorización de Fisher-Neyman</h4>
<div class="outline-text-4" id="text-org233c6ca">
<p>
\(T\) es suficiente para una familia \(\theta \in \Theta\) ssi existen funciones no negativas
\(g\),\(h\) tales que la distribución \(f_\theta\) es:
</p>

<p>
\[f_\theta(x) = h(x)g_\theta(T(x))\]
</p>

<p>
Donde \(g_\theta\) sólo depende de \(x\) a través de \(T\) y \(h\) no depende de \(\theta\).
</p>
</div>

<div id="outline-container-orgb6dbdc9" class="outline-5">
<h5 id="orgb6dbdc9"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
<div id="outline-container-orgc6987ff" class="outline-4">
<h4 id="orgc6987ff">Propiedades de los estadísticos suficientes</h4>
<div class="outline-text-4" id="text-orgc6987ff">
<p>
Los estadísticos suficientes cumplen:
</p>

<ol class="org-ol">
<li>Si \(T\) es suficiente para \(\{P_\theta \mid \theta \in \Theta\}\), lo es para \(\{P_\theta \mid \theta \in \Theta' \subset \Theta\}\).</li>
<li>Si \(T\) es suficiente y \(T = h'(U)\), \(U\) es suficiente.</li>
<li>Toda transformación biunívoca de suficiente es suficiente.</li>
</ol>
</div>

<div id="outline-container-org9f7c392" class="outline-5">
<h5 id="org9f7c392">Demostración</h5>
<div class="outline-text-5" id="text-org9f7c392">
</div><div id="outline-container-org20575d2" class="outline-6">
<h6 id="org20575d2">Punto 1</h6>
<div class="outline-text-6" id="text-org20575d2">
<p>
Si cumple la factorización para un conjunto, lo cumple para también
un subconjunto.
</p>
</div>
</div>

<div id="outline-container-org925c685" class="outline-6">
<h6 id="org925c685">Punto 2</h6>
<div class="outline-text-6" id="text-org925c685">
<p>
Por el teorema de factorización:
</p>

<p>
\[f_\theta(x) = h(x)g_\theta(h'(U(x)))\]
</p>
</div>
</div>

<div id="outline-container-orgbb15a11" class="outline-6">
<h6 id="orgbb15a11">Punto 3</h6>
<div class="outline-text-6" id="text-orgbb15a11">
<p>
Trivial desde lo anterior usando la inversa.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org4f4102b" class="outline-3">
<h3 id="org4f4102b">Estadísticos completos</h3>
<div class="outline-text-3" id="text-org4f4102b">
</div><div id="outline-container-org93a0551" class="outline-4">
<h4 id="org93a0551">Familia de distribuciones completa</h4>
<div class="outline-text-4" id="text-org93a0551">
<p>
Una familia \(\{P_\theta \mid \theta \in \Theta\}\) es completa si dada \(X \leadsto P_\theta\) se tiene que
para cada \(g\) medible:
</p>

<p>
\[
E_\theta[g(X)] = 0,\;\forall\theta\in\Theta
\implies
P_\theta(g(X) = 0) = 1,\;\forall\theta\in\Theta
\]
</p>
</div>
</div>

<div id="outline-container-org7661d9f" class="outline-4">
<h4 id="org7661d9f">Estadístico completo</h4>
<div class="outline-text-4" id="text-org7661d9f">
<p>
Un estadístico \(T\) es <b>completo</b> cuando para cualquier función medible \(g\),
se tiene:
</p>

<p>
\[ E_\theta [g(T)] = 0, \; \forall\theta\in\Theta
\implies
P_\theta(g(T) = 0) = 1,\; \forall\theta\in\Theta\]
</p>
</div>
</div>
</div>

<div id="outline-container-org61f0a36" class="outline-3">
<h3 id="org61f0a36">Suficiencia y completitud en familias exponenciales</h3>
<div class="outline-text-3" id="text-org61f0a36">
</div><div id="outline-container-org4c62c3c" class="outline-4">
<h4 id="org4c62c3c">Familia exponencial k-paramétrica</h4>
<div class="outline-text-4" id="text-org4c62c3c">
<p>
Una familia \(\{P_\theta : \theta \in \Theta\}\) es exponencial k-paramétrica si:
</p>

<ol class="org-ol">
<li>\(\Theta\) es intervalo de \(\mathbb{R}^k\).</li>
<li>Los valores de la variable no dependen de \(\theta\), esto es:
\(\{{ x \mid f_{\theta}(x) > 0 \} = \{{ x \mid f_{\theta'}(x) > 0 \}\) para cualesquiera \(\theta,\theta' \in \Theta\).</li>
<li><p>
La familia es de la forma:
</p>

<p>
\[f_\theta(x) = exp\left\{\sum_{h=1}^k {Q_h(\theta) T_h(x) + S(x) + D(\theta)}\right\}\]
</p></li>
</ol>
</div>
</div>

<div id="outline-container-org7840f33" class="outline-4">
<h4 id="org7840f33">Teorema de suficiencia y complitud</h4>
<div class="outline-text-4" id="text-org7840f33">
<p>
Si una familia \(\{P_\theta : \theta \in \Theta\}\) es exponencial k-paramétrica, cualquier muestra
aleatoria simple también lo es:
</p>

<p>
\[
f^n_\theta(x_1,\dots,x_n) = 
exp\left\{
\sum^k_{h=1} Q_h(\theta) \left(
\sum^n_{i=1} T_h(x_i)
\right) +
\sum^n_{i=1} S(x_i) + nD(\theta)
\right\}
\]
</p>

<p>
Teniéndose además:
</p>

<ol class="org-ol">
<li>\((\sum_i T_1(X_i), \dots \sum_i T_k(X_i))\) estadístico <b>suficiente</b> para \(\theta\).</li>
<li>Si \(k \leq n\), y \((Q_1(\Theta), \dots Q_k(\Theta))\) contiene un abierto;
\((\sum_i T_1(X_i), \dots \sum_i T_k(X_i))\) es <b>completo</b>.</li>
</ol>
</div>

<div id="outline-container-orgd6c3d7d" class="outline-5">
<h5 id="orgd6c3d7d"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org4b63d85" class="outline-4">
<h4 id="org4b63d85">Ejemplo: la normal para la media</h4>
<div class="outline-text-4" id="text-org4b63d85">
<p>
La familia \(\{{\cal N}(\mu,\sigma^2) \mid \mu \in \mathbb{R}\}\) es uniparamétrica escribiendo la función
de distribución como:
</p>

<p>
\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\left( \frac{x^2}{2\sigma^2} - 2\frac{x\mu}{2\sigma^2} + \frac{\mu^2}{2\sigma^2} \right)
\right\}
\]
</p>

<p>
De aquí tenemos el \(T(x) = x\) suficiente para \(\mu\). Y con para una muestra 
tenemos \(T(x_1,\dots,x_n) = x_1 + \dots + x_n\).
</p>
</div>
</div>

<div id="outline-container-org83f7ffc" class="outline-4">
<h4 id="org83f7ffc">Ejemplo: la normal para la varianza</h4>
<div class="outline-text-4" id="text-org83f7ffc">
<p>
La familia \(\{{\cal N}(\mu,\sigma^2) \mid \sigma\in\mathbb{R}\}\) es uniparamétrica escribiendo la función
de distribución como:
</p>

<p>
\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\frac{1}{2\sigma^2}\left(x^2 - 2x\mu + \mu^2 \right)
\right\}
\]
</p>

<p>
Así \(T(x) = \sum_{i=1}^n (x_i - \mu)^2\) es suficiente. Además es completo porque
tenemos que \(Q(\sigma) = -\frac{1}{2\sigma^2}\) tiene en la imagen un intervalo abierto.
</p>
</div>
</div>

<div id="outline-container-orgd52935b" class="outline-4">
<h4 id="orgd52935b">Ejemplo: distribución de Poisson</h4>
<div class="outline-text-4" id="text-orgd52935b">
<p>
La familia de Poisson \(\{Poi(\lambda) \mid \lambda \in \mathbb{R}\}\) tiene como estimador suficiente 
del parámetro a la suma de las muestras. Tenemos:
</p>

<p>
\[
f_\lambda(x) = \frac{1}{\prod x_i!} e^{-n\lambda} \lambda^{\sum x_i}
\]
</p>

<p>
Luego por Fisher-Neyman, sabemos que \(\sum x_i\) es suficiente.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org94084de" class="outline-2">
<h2 id="org94084de">4. Estimación puntual</h2>
<div class="outline-text-2" id="text-org94084de">
</div><div id="outline-container-orgfd87e2f" class="outline-3">
<h3 id="orgfd87e2f">Planteamiento del problema de estimación</h3>
<div class="outline-text-3" id="text-orgfd87e2f">
</div><div id="outline-container-orgc0011c9" class="outline-4">
<h4 id="orgc0011c9">Estimador puntual</h4>
<div class="outline-text-4" id="text-orgc0011c9">
<p>
Un estimador puntual de \(\theta\) es un estadístico \(T\) tomando valores en el 
dominio del parámetro, \(\Theta\).
</p>
</div>
</div>

<div id="outline-container-org985ca7b" class="outline-4">
<h4 id="org985ca7b">Función de pérdida y de riesgo</h4>
<div class="outline-text-4" id="text-org985ca7b">
<p>
La <b>función de pérdida</b>, \(L(\theta,t)\), nos dice la pérdida asociada a estimar 
un parámetro si su verdadero valor es otro.
</p>

<p>
\[
L : \Theta \times \Theta \longrightarrow \Theta
\]
</p>
</div>
</div>

<div id="outline-container-org47a7a24" class="outline-4">
<h4 id="org47a7a24">Función de riesgo</h4>
<div class="outline-text-4" id="text-org47a7a24">
<p>
La <b>función de riesgo</b> es la que asocia a cada valor del parámetro la 
pérdida media asociada al estimador.
</p>

<p>
\[ R^L_T(\theta) = E_\theta [L(\theta,T)] \]
</p>
</div>
</div>

<div id="outline-container-org6d8eb3b" class="outline-4">
<h4 id="org6d8eb3b">Estimador óptimo</h4>
<div class="outline-text-4" id="text-org6d8eb3b">
<p>
El <b>estimador óptimo</b>, \(T\), dada una función de pérdida, es el que minimiza 
uniformemente la función de riesgo:
</p>

<p>
\[ R^L_T(\theta) \leq R^L_{T''}(\theta),\quad \forall \theta \in \Theta,\; \forall T''\]
</p>
</div>
</div>

<div id="outline-container-orgc27fa32" class="outline-4">
<h4 id="orgc27fa32"><span class="todo TODO">TODO</span> Ejemplo de estimador óptimo</h4>
</div>
</div>
<div id="outline-container-org5e5ce25" class="outline-3">
<h3 id="org5e5ce25">Estimación de menor error cuadrático</h3>
<div class="outline-text-3" id="text-org5e5ce25">
</div><div id="outline-container-orgbbb2d2d" class="outline-4">
<h4 id="orgbbb2d2d">Función de pérdida cuadrática</h4>
<div class="outline-text-4" id="text-orgbbb2d2d">
<p>
La función de pérdida cuadrática, \({\cal L}(\theta, t) = (t - \theta)^2\), hace a la función de 
riesgo de un estimador su error cuadrático medio:
</p>

<p>
\[R^L_T(\theta) = E_\theta[(T - \theta)^2]\]
</p>

<p>
Nótese que en el caso de \(E[T] = \theta\), se tiene \(R^L_T(\theta) = Var_\theta[T]\).
</p>
</div>
</div>
</div>

<div id="outline-container-org5b15f7e" class="outline-3">
<h3 id="org5b15f7e">Estimación insesgada de mínima varianza</h3>
<div class="outline-text-3" id="text-org5b15f7e">
</div><div id="outline-container-org34dd927" class="outline-4">
<h4 id="org34dd927">Estimador insesgado</h4>
<div class="outline-text-4" id="text-org34dd927">
<p>
Un estimador \(T\) de \(g(\theta)\), es <b>insesgado</b> o <b>centrado</b> si:
</p>

<p>
\(E_\theta[T] = g(\theta)\)
</p>
</div>
</div>

<div id="outline-container-org841346c" class="outline-4">
<h4 id="org841346c">UMVUE: Estimador insesgado uniformemente de mínima varianza</h4>
<div class="outline-text-4" id="text-org841346c">
<p>
Un estimador \(T\) insesgado y de segundo orden es <b>UMVUE</b> para \(g(\theta)\) si para 
cualquier otro estimador insesgado \(T'\) se tiene que:
</p>

<p>
\[ Var_\theta[T] \leq Var_\theta[T']\]
</p>
</div>

<div id="outline-container-org6f5cbc6" class="outline-5">
<h5 id="org6f5cbc6">De segundo orden</h5>
<div class="outline-text-5" id="text-org6f5cbc6">
<p>
Lo llamamos de segundo orden cuando existe el momento de segundo orden:
</p>

<p>
\[
\exists \mathbb{E}_\theta[T^2(X_1,\dots,X_n)]
\quad
\forall \theta \in \Theta
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org3f94d15" class="outline-4">
<h4 id="org3f94d15">Propiedades del UMVUE</h4>
<div class="outline-text-4" id="text-org3f94d15">
<p>
El estimador UMVUE cumple:
</p>

<ul class="org-ul">
<li>Unicidad: El UMVUE de cualquier función paramétrica, si existe, es único.</li>
<li>Linealidad: Si \(T,Q\) son UMVUE para \(g,h\); \(aT+bQ\) es UMVUE para \(ag+bh\).</li>
</ul>
</div>

<div id="outline-container-org7a64cad" class="outline-5">
<h5 id="org7a64cad">Unicidad</h5>
<div class="outline-text-5" id="text-org7a64cad">
<p>
Si existieran dos UMVUE con \(Var(T) = Var(T')\), tendríamos:
</p>

<p>
\[\begin{aligned}
Var\left(\frac{1}{2}(T+T')\right) &= 
\frac{1}{4}
\left(
Var(T) + Var(T') + 2cov(T,T')
\right) \\& \leq
\frac{1}{4}
\left(
Var(T) + Var(T') + 2\sqrt{Var(T)Var(T')}
\right) \\& = Var(T)
\end{aligned}\]
</p>

<p>
La igualdad se da por ser UMVUE, y entonces, \(cov(T,T') = Var(T)\).
De aquí \(cov(T-T',T-T') = 0\), haciendo constante la diferencia entre los
dos. La diferencia entre ellos debe ser constantemente \(0\) por ser ambos 
insesgados.
</p>
</div>
</div>

<div id="outline-container-org9e3225d" class="outline-5">
<h5 id="org9e3225d"><span class="todo TODO">TODO</span> Linealidad</h5>
</div>
</div>
<div id="outline-container-orgc8f931c" class="outline-4">
<h4 id="orgc8f931c">Teorema de Raó-Blackwell</h4>
<div class="outline-text-4" id="text-orgc8f931c">
<p>
Si \(T\) es suficiente para \(\theta\) y \(S\) es un estimador insesgado de \(g(\theta)\) de 
segundo orden:
</p>

<ul class="org-ul">
<li>\(E[S \mid T]\) es estimador insesgado de \(g(\theta)\) de segundo orden.</li>
<li>\(Var_\theta[E[S \mid T]] \leq Var_\theta[S]\)</li>
</ul>

<p>
Es decir, \(E[S \mid T]\) será normalmente mejor estimador y nunca peor que \(S\).
</p>
</div>

<div id="outline-container-orgc897d3d" class="outline-5">
<h5 id="orgc897d3d">Demostración</h5>
<div class="outline-text-5" id="text-orgc897d3d">
<p>
Sabemos \(E[S|T] = E[S] = \theta\) por la <a href="#org087896d">ley de esperanza total</a>. La desigualdad
entre varianzas la vemos como:
</p>

<p>
\[\begin{aligned}
E\Big[(E[S|T] - \theta)^2 \Big] &= 
E\Big[E[S-\theta | T]^2 \Big] \leq
E\Big[E[(S-\theta)^2|T] \Big] = E\Big[(S-\theta)^2\Big]
\end{aligned}\]
</p>

<p>
Donde volvemos a usar la ley de esperanza total. La desigualdad viene
de que la varianza es positiva, o de la desigualdad de Jensen para el
cuadrado.
</p>
</div>
</div>
</div>

<div id="outline-container-org8d751a4" class="outline-4">
<h4 id="org8d751a4">Teorema de Lehmann-Scheffé</h4>
<div class="outline-text-4" id="text-org8d751a4">
<p>
Para \(T\) suficiente y completo para \(\theta\); si \(g(\theta)\) admite un estimador insesgado 
de segundo orden \(S\), entonces existe el UMVUE de \(g(\theta)\) y está dado por:
</p>

<p>
\[ \mathbb{E} [S \mid T]\]
</p>

<p>
De otra forma, un estimador insesgado que es función de estimador completo
y suficiente es el UMVUE.
</p>
</div>

<div id="outline-container-org6742a86" class="outline-5">
<h5 id="org6742a86">Demostración</h5>
<div class="outline-text-5" id="text-org6742a86">
<p>
Por <a href="#orgc8f931c">Raó-Blackwell</a>, sabemos que es un estimador insesgado; y que, dado
cualquier otro estimador insesgado \(Q\), tenemos que:
</p>

<p>
\[ Var[E[Q|T]] \leq Var[Q]\]
</p>

<p>
Ahora bien, dado otro, tendríamos:
</p>

<p>
\[
E\Big[ E[S|T] - E[Q|T] \Big] = 0
\]
</p>

<p>
Y como \(T\) es completo y ambos son dependientes de \(T\), eso implica que:
</p>

<p>
\[P\Big(
E[S|T] - E[Q|T] = 0
\Big) = 1\]
</p>

<p>
Por lo tanto, ambos son el UMVUE.
</p>
</div>
</div>
</div>

<div id="outline-container-org6250747" class="outline-4">
<h4 id="org6250747">Cálculo del UMVUE</h4>
<div class="outline-text-4" id="text-org6250747">
<p>
Dado \(T\) suficiente y completo. Para calcular el UMVUE de \(g(\theta)\) podemos:
</p>

<ol class="org-ol">
<li>Buscar un estimador insesgado y de segundo orden cualquiera de \(g(\theta)\).
Entonces \(\mathbb{E}[S|T]\) será el UMVUE.</li>
<li>Buscar \(h(T)\) tal que \(\mathbb{E}_\theta[h(T)] = g(\theta)\), un estimador insesgado que es
sólo función de \(T\). Se cumplirá \(\mathbb{E}[h(T)|T] = h(T)\).</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgbc96112" class="outline-3">
<h3 id="orgbc96112">Estimación eficiente</h3>
<div class="outline-text-3" id="text-orgbc96112">
</div><div id="outline-container-org3eaf2fe" class="outline-4">
<h4 id="org3eaf2fe">Condiciones de regularidad de Fréchet-Cramer-Rao</h4>
<div class="outline-text-4" id="text-org3eaf2fe">
<p>
Una familia \(\{P_\theta \mid \theta\in\Theta\}\) es <b>regular</b> en el sentido de Fréchet-Cramer-Rao
si cumple que:
</p>

<ol class="org-ol">
<li>\(\Theta\) es intervalo abierto de \(\mathbb{R}\).</li>
<li>\(\forall \theta,\theta'\in\Theta : \{x \mid f_\theta(x) > 0\} = \{x \mid f_{\theta'}(x) > 0\} = \chi\)</li>
<li><p>
Tenemos \(f_\theta(x)\) derivable respecto a \(\theta\) para todo \(x \in \chi\) con:
</p>

<p>
\[ \int_\chi \frac{d f_\theta(x)}{d\theta} dx = 
     \frac{d}{d\theta} \int_\chi f_\theta(x) dx = 
     0, \quad \forall \theta\in\Theta\]
</p>

<p>
O, cuando la distribución es discreta:
</p>

<p>
\[\sum_\chi \frac{d f_\theta(x)}{d\theta}
     = 
     \frac{d}{d\theta}\sum_\chi f_\theta(x)
     = 
     0\]
</p></li>
</ol>
</div>
</div>

<div id="outline-container-org90e3ed0" class="outline-4">
<h4 id="org90e3ed0">Función de información de Fisher</h4>
<div class="outline-text-4" id="text-org90e3ed0">
<p>
Si \(\{P_\theta : \theta \in \Theta\}\) es regular, definimos la <b>función de información</b> asociada
a \(X\) como:
</p>

<p>
\[I_X(\theta) = E_\theta\left[\left( \frac{d}{d\theta} \ln(f_\theta(X))
\right)^2\right]\]
</p>

<p>
Y la función de información asociada a una muestra como:
</p>

<p>
\[
\[I_{X_1,\dots,X_n}(\theta) = 
E_\theta\left[\left( \frac{d}{d\theta}\ln(f_\theta(X_1,\dots,X_n))
\right)^2\right]\]
</p>
</div>
</div>

<div id="outline-container-org2f4494b" class="outline-4">
<h4 id="org2f4494b">Propiedades de la función de información</h4>
<div class="outline-text-4" id="text-org2f4494b">
<p>
La función de información tiene como propiedades:
</p>

<ol class="org-ol">
<li>\(I_X(\theta) \geq 0\).</li>

<li>En el caso \(I_X(\theta) = 0\), \(f_\theta(X)\) no depende de \(\theta\).</li>

<li>\[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = 0\].</li>

<li>\[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = I_X(\theta) \].</li>

<li>\[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = 0\].</li>

<li>\[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = I_{X_1,\dots,X_n}(\theta) \].</li>

<li>Aditividad, \(I_{X_1,\dots,X_n}(\theta) = nI_X(\theta)\).</li>
</ol>
</div>

<div id="outline-container-org1cb92a8" class="outline-5">
<h5 id="org1cb92a8">Demostración</h5>
<div class="outline-text-5" id="text-org1cb92a8">
</div><div id="outline-container-org39a90d4" class="outline-6">
<h6 id="org39a90d4">Punto 3</h6>
<div class="outline-text-6" id="text-org39a90d4">
<p>
Derivando y asumiendo las condiciones de regularidad:
</p>

<p>
\[
\mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ln f_\theta(x)\right]
=
\int_\chi \left(\frac{\partial}{\partial\theta} \ln f_\theta(x)\right) f_\theta(x)\; dx
=
\int_\chi \frac{\partial}{\partial\theta} f_\theta(x)\;dx
= 0
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org91a6e24" class="outline-4">
<h4 id="org91a6e24">Función de información bajo Cramer-Raó</h4>
<div class="outline-text-4" id="text-org91a6e24">
<p>
Bajo las hipótesis de regularidad fuertes de Fréchet-Cramer-Raó:
</p>

<p>
\[
I(\theta) = E_{\theta}\left[
-\frac{\partial^2}{\partial\theta^2} \log f_\theta(X)
\right]
\]
</p>

<p>
Es decir, debemos exigir que:
</p>

<p>
\[
\int_X \frac{\partial^2}{\partial\theta^2} f_\theta(x) dx =
\frac{\partial^2}{\partial\theta^2} \int_X  f_\theta(x) dx
\]
</p>
</div>

<div id="outline-container-orgdf21237" class="outline-5">
<h5 id="orgdf21237">Demostración</h5>
<div class="outline-text-5" id="text-orgdf21237">
<p>
Notando primero la siguiente igualdad:
</p>

<p>
\[
\frac{\partial^2}{\partial\theta^2}\log f_\theta(x) =
\frac{1}{f_\theta(x)}\frac{\partial^2}{\partial\theta^2}f_\theta(x) -
\left(\frac{\partial}{\partial\theta}\log f_\theta(x)\right)^2
\]
</p>

<p>
Y simplemente tomamos esperanzas, sabiendo que por las condiciones de
regularidad:
</p>

<p>
\[
\mathbb{E}\left[
\frac{1}{f_\theta(x)}
\frac{\partial^2}{\partial\theta^2} f_\theta(x)
\right]
=
\frac{\partial^2}{\partial\theta^2}
\int_X f_\theta(x)\;dx 
= 
\frac{\partial^2}{\partial\theta^2}
1
=
0
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgd5f6493" class="outline-4">
<h4 id="orgd5f6493">Estadístico regular</h4>
<div class="outline-text-4" id="text-orgd5f6493">
<p>
Un estadístico \(T\) es regular en el sentido de Fréchet-Cramer-Raó, si
siendo una distribución discreta:
</p>

<p>
\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\sum_{x \in \chi^n} T(x)f_\theta(x) \\&= 
\sum_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x)
\end{aligned}\]
</p>

<p>
O, siendo una distribución continua:
</p>

<p>
\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\int_{x \in \chi^n} T(x)f_\theta(x) \;dx \\&= 
\int_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x) \;dx
\end{aligned}\]
</p>
</div>
</div>

<div id="outline-container-org4690b21" class="outline-4">
<h4 id="org4690b21">Cota de Fréchet-Cramer-Raó</h4>
<div class="outline-text-4" id="text-org4690b21">
<p>
Si \(\{P_\theta \mid \theta \in \Theta\}\) es regular, la función de información se acota
\(0 < I_X(\theta) < \infty\), y \(T\) es un estadístico regular, de segundo orden e
insesgado en una función derivable \(g(\theta)\), se tiene:
</p>

<ol class="org-ol">
<li>\[Var_\theta[T] \geq \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]</li>

<li><p>
Para todo \(\theta \in \Theta\) tal que \(g'(\theta) \neq 0\):
</p>

<p>
\[Var_\theta[T] = \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]
</p>

<p>
ssi existe \(a(\theta) \neq 0\) tal que:
</p>

<p>
\[P_\theta\left(
     \frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = 
     a(\theta)[T(X_1,\dots,X_n) - g(\theta)]
     \right) = 1\]
</p></li>
</ol>
</div>

<div id="outline-container-org3a85669" class="outline-5">
<h5 id="org3a85669">Demostración</h5>
<div class="outline-text-5" id="text-org3a85669">
</div><div id="outline-container-org891bf35" class="outline-6">
<h6 id="org891bf35">Primer punto</h6>
<div class="outline-text-6" id="text-org891bf35">
<p>
Llamamos primero:
</p>

<p>
\[V_\theta = \frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\]
</p>

<p>
Tenemos que:
</p>

<ul class="org-ul">
<li>\(Var_\theta(V_\theta) = I_{X_1,\dots,X_n}(\theta)\)</li>
<li>\(\mathbb{E}(V_\theta) = 0\)</li>
<li>\(Cov(T,V_\theta) = \mathbb{E}[TV_\theta] - \mathbb{E}[T]\mathbb{E}[V]\)</li>
</ul>

<p>
Calculando:
</p>

<p>
\[\begin{aligned}
Cov(T,V_\theta) 
&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\right)
f_\theta(x_1,\dots,x_n)\;dx 
\\&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} \int_{\chi^n} 
T(x_1,\dots,x_n)
\left(f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} g(\theta)
\end{aligned}\]
</p>

<p>
Y finalmente aplicamos la desigualdad de Cauchy-Schwartz a la 
covarianza entre \(T,V_\theta\) para tener:
</p>

<p>
\[
Cov(T,V_\theta) \leq Var[T]Var[V_\theta]
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf205fa2" class="outline-4">
<h4 id="orgf205fa2">Estimador eficiente</h4>
<div class="outline-text-4" id="text-orgf205fa2">
<p>
Sea \(\{P_\theta \mid \theta \in \Theta\}\) regular, con la función de información acotada 
\(0 < I_X(\theta) < \infty\) y \(g(\theta)\) función paramétrica derivable. Un estimador \(T\) de \(g(\theta)\) 
es <b>eficiente</b> si es insesgado, regular, y su varianza alcanza la
cota de Fréchet-Cramer-Raó en todo punto:
</p>

<p>
\[Var_\theta[T] = \frac{(g'(\theta))^2}{I_{X_1,\dots,X_n}(\theta)},
\qquad \forall\theta \in \Theta\]
</p>
</div>
</div>

<div id="outline-container-org91cc5bc" class="outline-4">
<h4 id="org91cc5bc">Caracterización de estimadores eficientes</h4>
<div class="outline-text-4" id="text-org91cc5bc">
<p>
Sea \(\{P_\theta \mid \theta \in \Theta\}\) regular, con \(0 < I_X(\theta) < \infty\) y \(g(\theta)\) función paramétrica
derivable y <b>no constante</b>. Un estimador \(T\) es eficiente ssi existe un \(a(\theta)\)
cumpliendo:
</p>

<ol class="org-ol">
<li>\[P_\theta\left(\frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = a(\theta)[T(X_1,\dots,X_n) - g(\theta)]\right) = 1\]</li>

<li>\[I_{X_1,\dots,X_n}(\theta) = a(\theta)g'(\theta)\]</li>
</ol>
</div>

<div id="outline-container-org140e0de" class="outline-5">
<h5 id="org140e0de">Demostración</h5>
<div class="outline-text-5" id="text-org140e0de">
</div><div id="outline-container-orgd64427c" class="outline-6">
<h6 id="orgd64427c">Primera implicación</h6>
<div class="outline-text-6" id="text-orgd64427c">
<p>
Si tenemos un \(T\) eficiente, tomamos el \(T\) de la cota y \(g'(\theta)\neq 0\),
de ahí tenemos la primera igualdad.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org959e93c" class="outline-4">
<h4 id="org959e93c">Ejemplo: distribución binomial</h4>
<div class="outline-text-4" id="text-org959e93c">
<p>
Dada la familia de distribuciones \(\{B(k_0,p) \mid p \in (0,1)\}\), veremos que es
regular.
</p>
</div>

<div id="outline-container-orgc2eaaaf" class="outline-5">
<h5 id="orgc2eaaaf">Es regular</h5>
<div class="outline-text-5" id="text-orgc2eaaaf">
<p>
El intervalo \(p \in (0,1)\) es abierto y \(\chi = (0,\dots,k_0)\).
</p>

<p>
Si la expresamos exponencialmente es más fácil calcular su derivada
y relacionarla con una esperanza:
</p>

<p>
\[
f_p(x)
= 
\exp\left\{\log{k_0\choose x} + x \log p + (k_0-x)\log (1-p)\right\}
\]
</p>

<p>
Tenemos:
</p>

<p>
\[
\sum_\chi \frac{\partial f_p(x)}{\partial p}
=
\sum_\chi \frac{x-pk_0}{p(1-p)} f_p(x)
=
\mathbb{E}\left[\frac{X-pk_0}{p(1-p)} \right] = 0
\]
</p>
</div>
</div>

<div id="outline-container-org22ca892" class="outline-5">
<h5 id="org22ca892">Función de información</h5>
<div class="outline-text-5" id="text-org22ca892">
<p>
Desde la derivada podemos calcular la función de información:
</p>

<p>
\[
I_{X_1,\dots,X_n}(p) = \frac{nk_0}{p(1-p)}
\]
</p>
</div>
</div>

<div id="outline-container-org80739f8" class="outline-5">
<h5 id="org80739f8">Caracterización del estimador eficiente</h5>
<div class="outline-text-5" id="text-org80739f8">
<p>
Calcularemos para usar la caracterización:
</p>

<p>
\[
\frac{\partial}{\partial p} \ln f_p(x_1,\dots,x_n)
=
\sum \frac{\partial}{\partial p} f_p(x_i)
=
n\frac{\overline{x}-pk_0}{p(1-p)}
\]
</p>

<p>
Así, para cumplir la caracterización, tiene sentido tomar:
</p>

<ul class="org-ul">
<li>\(g(p) = pk_0\)</li>
<li>\(T(X_1,\dots,X_n) = \overline{X}\)</li>
<li>\(a(p) = \frac{n}{p(1-p)}\)</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgcd596a5" class="outline-3">
<h3 id="orgcd596a5">Estimación de máxima verosimilitud</h3>
<div class="outline-text-3" id="text-orgcd596a5">
</div><div id="outline-container-orgb31d4f6" class="outline-4">
<h4 id="orgb31d4f6">Función de verosimilitud</h4>
<div class="outline-text-4" id="text-orgb31d4f6">
<p>
Para cada realización muestral se define la función de verosimilitud
de la realización, \(L_{x_1,\dots,x_n} : \Theta \longrightarrow \mathbb{R}^+_0\), como:
</p>

<p>
\[L(\theta) = f_\theta(x_1,\dots,x_n)\]
</p>
</div>
</div>

<div id="outline-container-org8795384" class="outline-4">
<h4 id="org8795384">Estimador de máxima verosimilitud</h4>
<div class="outline-text-4" id="text-org8795384">
<p>
Tenemos \(\hat\theta\) estimador de máxima verosimilitud de \(\theta\) cuando la estimación
asociada a cada realización muestral maximiza la verosimilitud:
</p>

<p>
\[L_{x_1,\dots,x_n}\left(\hat\theta(x_1,\dots,x_n)\right)
= \max_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)\]
</p>
</div>
</div>

<div id="outline-container-orga33a8d4" class="outline-4">
<h4 id="orga33a8d4">Ecuación de máxima verosimilitud</h4>
<div class="outline-text-4" id="text-orga33a8d4">
<p>
El procedimiento habitual para hallar el estimador de máxima 
verosimilitud es derivar e igualar:
</p>

<p>
\[
\frac{\partial}{\partial\theta_j} \ln L_{X_1,\dots,X_n}(\theta_1,\dots,\theta_k)
= 0
\]
</p>

<p>
Nótese que esto sólo nos da un punto crítico.
</p>
</div>

<div id="outline-container-org6d9bfd7" class="outline-5">
<h5 id="org6d9bfd7">Demostración</h5>
<div class="outline-text-5" id="text-org6d9bfd7">
<p>
Usando simplemente que el logaritmo es creciente y la caracterización
de los máximos.
</p>
</div>
</div>
</div>

<div id="outline-container-org94c7bed" class="outline-4">
<h4 id="org94c7bed">Propiedades del estimador de máxima verosimilitud</h4>
<div class="outline-text-4" id="text-org94c7bed">
<p>
Un estimador de máxima verosimilitud \(\hat\theta\) de \(\theta\) cumple:
</p>

<ol class="org-ol">
<li><p>
Consistencia:
</p>

<p>
\[\lim_{n \to \infty}\hat\theta(x_1,\dots,x_n) = \theta\]
</p></li>

<li><p>
Normalidad asintótica. Para \(n\) suficientemente grande, sus errores
pueden aproximarse por una normal:
</p>

<p>
\[\sqrt{n}(\hat\theta(x_1,\dots,x_n) - \theta) \leadsto {\cal N}(0,1/I_X(\theta))\]
</p></li>
</ol>
</div>

<div id="outline-container-org9af6116" class="outline-5">
<h5 id="org9af6116"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-orge26bcf6" class="outline-4">
<h4 id="orge26bcf6">Relación con estadísticos suficientes</h4>
<div class="outline-text-4" id="text-orge26bcf6">
<p>
Si \(\{P_\theta \mid \theta \in \Theta\}\) admite estadístico suficiente \(T\), entonces \(\hat\theta\) es función
de \(T\).
</p>
</div>

<div id="outline-container-org0d08710" class="outline-5">
<h5 id="org0d08710">Demostración</h5>
<div class="outline-text-5" id="text-org0d08710">
<p>
Por Teorema de factorización de Fisher-Neyman, tenemos que la
función de distribución se escribirá como:
</p>

<p>
\[
f_\theta(x) = h(x)g_\theta(T)
\]
</p>

<p>
Entonces para maximizarla habrá que maximizar \(g_\theta(T)\).
</p>
</div>
</div>
</div>

<div id="outline-container-orgb5ae354" class="outline-4">
<h4 id="orgb5ae354">Relación con estimadores eficientes</h4>
<div class="outline-text-4" id="text-orgb5ae354">
<p>
Si \(T\) es estimador eficiente de \(\theta\), entonces \(T\) es el único estimador 
máximo verosímil de \(\theta\).
</p>
</div>

<div id="outline-container-orged1eece" class="outline-5">
<h5 id="orged1eece"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org5ea31c2" class="outline-4">
<h4 id="org5ea31c2">Función de verosimilitud de una función paramétrica</h4>
<div class="outline-text-4" id="text-org5ea31c2">
<p>
Se define la función de verosimilitud de \(g : \Theta \longrightarrow \Lambda\) asociada a
una realización, \(M_{x_1,\dots,x_n} : \Lambda \longrightarrow \mathbb{R}^+_0\), como:
</p>

<p>
\[M_{x_1,\dots,x_n}(\lambda)
= \sup_{\theta \in g^{-1}(\lambda)} L_{x_1,\dots,x_n}(\theta) \]
</p>
</div>
</div>

<div id="outline-container-orgb703295" class="outline-4">
<h4 id="orgb703295">Estimador de máxima verosimilitud de una función paramétrica</h4>
<div class="outline-text-4" id="text-orgb703295">
<p>
Será \(\hat\lambda\) estimador máximo verosímil de \(\lambda\) cuando:
</p>

<p>
\[M_{x_1,\dots,x_n}(\hat\lambda(x_1,\dots,x_n)) 
= \max_{\lambda \in \Lambda} M_{x_1,\dots,x_n}(\lambda) \]
</p>
</div>
</div>

<div id="outline-container-org0a3025b" class="outline-4">
<h4 id="org0a3025b">Teorema de invarianza de Zenha</h4>
<div class="outline-text-4" id="text-org0a3025b">
<p>
Si \(\hat\theta\) es estimador máximo verosímil de \(\theta\), entonces \(g(\hat\theta)\) es estimador
máximo verosímil de \(g(\theta)\).
</p>
</div>

<div id="outline-container-orgb577dd6" class="outline-5">
<h5 id="orgb577dd6"><span class="todo TODO">TODO</span> Demostración</h5>
<div class="outline-text-5" id="text-orgb577dd6">
<p>
Se cumple:
</p>

<p>
\[
M(\lambda') = sup_{\theta \in g^{-1}(\lambda')} L(\theta)
\leq
sup_{\theta \in \Theta} L(\theta)
=
L(\hat\theta)
=
M(g(\hat\theta))
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgfc04915" class="outline-3">
<h3 id="orgfc04915">Método de los momentos</h3>
<div class="outline-text-3" id="text-orgfc04915">
</div><div id="outline-container-org16c43b7" class="outline-4">
<h4 id="org16c43b7">Descripción</h4>
<div class="outline-text-4" id="text-org16c43b7">
<p>
El estimador de una función dependiente en los momentos
poblacionales es el mismo dependiendo en los momentos muestrales.
</p>

<p>
\[g(\theta) = h(m_{\theta,1},\dots,m_{\theta,k}) 
\quad\Rightarrow\quad
\widehat{g(\theta)}(X_1,\dots,X_n) = h(A_1,\dots,A_k)\]
</p>
</div>

<div id="outline-container-org1d91dac" class="outline-5">
<h5 id="org1d91dac">Momentos poblacionales</h5>
<div class="outline-text-5" id="text-org1d91dac">
<p>
Definimos los momentos poblacionales como:
</p>

<p>
\[m_{\theta,j} = E_\theta[X^j]\]
</p>
</div>
</div>

<div id="outline-container-orgda60c10" class="outline-5">
<h5 id="orgda60c10">Momentos muestrales</h5>
<div class="outline-text-5" id="text-orgda60c10">
<p>
Definimos los momentos muestrales como:
</p>

<p>
\[A_j = \frac{1}{n}\sum_{i=1}^n X^j_i\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org2846ea8" class="outline-3">
<h3 id="org2846ea8">Método de mínimos cuadrados</h3>
<div class="outline-text-3" id="text-org2846ea8">
</div><div id="outline-container-org71fc016" class="outline-4">
<h4 id="org71fc016">Descripción</h4>
<div class="outline-text-4" id="text-org71fc016">
<p>
Si \(X_i\) son las observaciones aleatorias de una magnitud \(\varphi(t,\theta)\) con
errores \(\varepsilon_i\); es decir:
</p>

<p>
\[X_i = \varphi(t_i,\theta) + \varepsilon_i\]
</p>

<p>
Entonces el estimador de mínimos cuadrados de \(\theta\) es el que minimice
la suma de cuadrados de los errores:
</p>

<p>
\[\sum^n_{i=1}(X_i - \varphi(t_i,\theta))^2\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org4234235" class="outline-2">
<h2 id="org4234235">5. Estimación por intervalos de confianza</h2>
<div class="outline-text-2" id="text-org4234235">
</div><div id="outline-container-org0016bcd" class="outline-3">
<h3 id="org0016bcd">Definiciones y métodos de construcción</h3>
<div class="outline-text-3" id="text-org0016bcd">
</div><div id="outline-container-org9b6b745" class="outline-4">
<h4 id="org9b6b745">Intervalo de confianza</h4>
<div class="outline-text-4" id="text-org9b6b745">
<p>
Para \(X \leadsto P_\theta\), un intervalo de confianza \(\alpha\) para \(\theta\) es un intervalo 
aleatorio \((I_1,I_2)\) tal que para cualquier \(\theta \in \Theta\):
</p>

<p>
\[P_\theta\left(
I_1(X_1,\dots,X_n) \leq \theta \leq I_2(X_1,\dots,X_n)
\right)
\geq 1 - \alpha\]
</p>
</div>
</div>

<div id="outline-container-orga532e5a" class="outline-4">
<h4 id="orga532e5a">Intervalo de confianza de menor longitud esperada uniformemente</h4>
<div class="outline-text-4" id="text-orga532e5a">
<p>
Un interavlo \((I_1,I_2)\) es el de menor longitud esperada uniformemente 
si para cualquier otro \((I_1',I_2')\) al mismo nivel, se tiene:
</p>

<p>
\[
E_\theta[I_2(X_1,\dots,X_n) - I_1(X_1,\dots,X_n)]
\leq
E_\theta[I_2'(X_1,\dots,X_n) - I_1'(X_1,\dots,X_n)]
\]
</p>
</div>
</div>

<div id="outline-container-org1057dc2" class="outline-4">
<h4 id="org1057dc2">Intervalos mediante desigualdad de Chevychev</h4>
<div class="outline-text-4" id="text-org1057dc2">
<p>
Si \(T\) es estimador insesgado de \(\theta\) con varianza uniformemente acotada:
</p>

<ul class="org-ul">
<li>\(E_\theta[T(X_1,\dots,X_n)] = \theta\)</li>
<li>\(Var_\theta[T(X_1,\dots,X_n)] \leq c\)</li>
</ul>

<p>
Por lo que por Chevychev tenemos, dado \(k>0\), un intervalo de confianza
para \(\theta\) al nivel de confianza \(1 - c/k^2\):
</p>

<p>
\[P\left(T - k \leq \theta \leq  T + k \right) \geq 1 - c/k^2\]
</p>
</div>

<div id="outline-container-org4438f0f" class="outline-5">
<h5 id="org4438f0f"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-orge9c3e3a" class="outline-4">
<h4 id="orge9c3e3a">Pivote para un parámetro</h4>
<div class="outline-text-4" id="text-orge9c3e3a">
<p>
Un pivote es una función \(T(X_1,\dots,X_n,\theta)\) tal que fijado cualquier \(\theta\),
\(T(X_1,\dots,X_n,\theta)\) es una variable con distribución independiente de \(\theta\).
</p>
</div>
</div>

<div id="outline-container-org5ca1064" class="outline-4">
<h4 id="org5ca1064">Intervalos obtenidos mediante el método pivotal</h4>
<div class="outline-text-4" id="text-org5ca1064">
<p>
Dado un pivote \(T\) estrictamente monótono respecto a \(\theta\), y dos valores
\(\lambda_1,\lambda_2\), tales que:
</p>

<p>
\[P_\theta(\lambda_1 < T < \lambda_2) \geq 1 - \alpha\]
</p>

<p>
Tomamos las soluciones \(\hat\theta_1, \hat\theta_2\), cumpliendo \(T(X_1,\dots,\hat\theta_1) = \lambda_1\) y
\(T(X_1,\dots,\hat\theta_2) = \lambda_2\); y ellas forman un intervalo de confianza:
</p>

<ul class="org-ul">
<li>\(P_\theta(\hat\theta_1 < \theta < \hat\theta_2) \geq 1 - \alpha\), para \(T\) creciente.</li>
<li>\(P_\theta(\hat\theta_2 < \theta < \hat\theta_1) \geq 1 - \alpha\), para \(T\) decreciente.</li>
</ul>
</div>

<div id="outline-container-org4045cd7" class="outline-5">
<h5 id="org4045cd7"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org55f236c" class="outline-4">
<h4 id="org55f236c">Un pivote en distribuciones continuas</h4>
<div class="outline-text-4" id="text-org55f236c">
<p>
Si \(X\) es continua con \(F_\theta\) función de distribución, un pivote es:
</p>

<p>
\[ T(X_1,\dots,X_n,\theta) = -2 \sum_{i=1}^n \ln F_\theta(X_i) \leadsto \chi^2(2n)\]
</p>
</div>

<div id="outline-container-orgca2fbb1" class="outline-5">
<h5 id="orgca2fbb1"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
<div id="outline-container-org3b01743" class="outline-4">
<h4 id="org3b01743">Un pivote dado un estadístico</h4>
<div class="outline-text-4" id="text-org3b01743">
<p>
Sea \(S\) un estadístico de distribución continua con \(F^S_\theta\) función de 
distribución. Un pivote es:
</p>

<p>
\[T(X_1,\dots,X_n,\theta) = F^S_\theta(S(X_1,\dots,X_n)) \leadsto U(0,1)\]
</p>
</div>

<div id="outline-container-orgf35bf1c" class="outline-5">
<h5 id="orgf35bf1c"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
</div>
<div id="outline-container-org4a19fa7" class="outline-3">
<h3 id="org4a19fa7">Ejemplos de intervalos de confianza</h3>
<div class="outline-text-3" id="text-org4a19fa7">
</div><div id="outline-container-orge94e1f8" class="outline-4">
<h4 id="orge94e1f8">A1. Intervalo para la media de una normal con varianza conocida</h4>
<div class="outline-text-4" id="text-orge94e1f8">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2_0)\). El intervalo para \(\mu\) de menor longitud
media uniforme a nivel de confianza \(1-\alpha\) será:
</p>

<p>
\[\left(
\overline{X}-z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}},
\overline{X}+z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}}
\right)\]
</p>

<p>
donde \(z_{\alpha/2}\) cumple que \[P\left(Z > z_{\alpha/2}\right) = \alpha/2\] con \(Z \leadsto {\cal N}(0,1)\).
</p>
</div>

<div id="outline-container-org461cf4a" class="outline-5">
<h5 id="org461cf4a">Pivote</h5>
<div class="outline-text-5" id="text-org461cf4a">
<p>
Usamos como pivote a la normalizada:
</p>

<p>
\[T(X_1,\dots,X_n,\mu) = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \leadsto {\cal N}(0,1)\]
</p>
</div>
</div>

<div id="outline-container-org945382a" class="outline-5">
<h5 id="org945382a">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-org945382a">
<p>
Usando el pivote, tenemos el siguiente candidato a intervalo de
confianza:
</p>

<p>
\[
1 - \alpha > 
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{\sigma_0 / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}}
\right)
\]
</p>

<p>
Debiendo tenerse que, si \(\Phi\) es la función de distribución de la
normal \(\Phi(\lambda_2)-\Phi(\lambda_1) = 1 - \alpha\).
</p>
</div>
</div>

<div id="outline-container-orgd7ad032" class="outline-5">
<h5 id="orgd7ad032">Longitud media</h5>
<div class="outline-text-5" id="text-orgd7ad032">
<p>
Buscamos el que minimice la longitud media:
</p>

<p>
\[E_\mu \left[
\left( \overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}} \right) -
\left( \overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} \right)
\right] 
= (\lambda_2-\lambda_1)\frac{\sigma_0}{\sqrt{n}}\]
</p>

<p>
Por lo que tratamos de minimizar \((\lambda_2-\lambda_1)\).
</p>
</div>
</div>

<div id="outline-container-orga13ed1d" class="outline-5">
<h5 id="orga13ed1d">Minimización</h5>
<div class="outline-text-5" id="text-orga13ed1d">
<p>
Usamos multiplicadores de Lagrange para definir:
</p>

<p>
\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]
</p>

<p>
Calculando las derivadas parciales tenemos:
</p>

<p>
\[\begin{aligned}
-1-\lambda\Phi'(\lambda_1) &= 0\\
1 + \lambda\Phi'(\lambda_2) &= 0
\end{aligned}
\]
</p>

<p>
Luego debe tenerse \(\Phi'(\lambda_1) = \Phi'(\lambda_2)\). Sabiendo que \(\Phi'\) es la función
de distribución de la normal, tenemos \(\lambda_1 = \pm \lambda_2\). Como deben ser
distintos para cumplir la restricción, tenemos \(\lambda_1 = -\lambda_2\).
</p>
</div>
</div>

<div id="outline-container-orgea8b44b" class="outline-5">
<h5 id="orgea8b44b">Conclusión</h5>
<div class="outline-text-5" id="text-orgea8b44b">
<p>
La restricción nos fuerza a \(\Phi(\lambda_2) - \Phi(-\lambda_2) = 1 - \alpha\), luego estamos
buscando el \(z_{\alpha/2}\) que cumple, para una normalizada \(Z\), \(P(Z > z_{\alpha/2}) = \alpha/2\).
</p>
</div>
</div>
</div>

<div id="outline-container-orgee1a1eb" class="outline-4">
<h4 id="orgee1a1eb">A2. Intervalo para la media de una normal con varianza desconocida</h4>
<div class="outline-text-4" id="text-orgee1a1eb">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\). El intervalo para \(\mu\) de menor longitud media uniforme
a nivel de confianza \(1-\alpha\) será:
</p>

<p>
\[
\left(
\overline{X} - t_{n-1;\alpha/2}\frac{S}{\sqrt{n}},\,
\overline{X} + t_{n-1;\alpha/2}\frac{S}{\sqrt{n}}
\right)
\]
</p>

<p>
donde \(t_{n-1;\alpha/2}\) cumple que \(P(Z \leq t_{n-1;\alpha/2}) = \alpha/2\) con \(Z \leadsto T(n-1)\).
</p>
</div>

<div id="outline-container-org1cea7f8" class="outline-5">
<h5 id="org1cea7f8">Pivote</h5>
<div class="outline-text-5" id="text-org1cea7f8">
<p>
Usaremos como pivote la t de Student:
</p>

<p>
\[
T = \frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]
</p>
</div>
</div>

<div id="outline-container-org2eb9068" class="outline-5">
<h5 id="org2eb9068">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-org2eb9068">
<p>
Usando el pivote tenemos el siguiente intervalo de confianza:
</p>

<p>
\[
1 - \alpha =
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{S / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{S}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{S}{\sqrt{n}}
\right)
\]
</p>

<p>
Donde, si \(\Phi\) es la función de distribución de \(t(n-1)\), tenemos
que \(1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)\).
</p>
</div>
</div>

<div id="outline-container-orgeffcbdc" class="outline-5">
<h5 id="orgeffcbdc">Longitud media</h5>
<div class="outline-text-5" id="text-orgeffcbdc">
<p>
Queremos minimizar la longitud esperada del intervalo:
</p>

<p>
\[
\mathbb{E}\left[
\left(\overline{X}-\lambda_1\frac{S}{\sqrt{n}}\right) - 
\left(\overline{X}-\lambda_2\frac{S}{\sqrt{n}}\right)
\right]
=
(\lambda_2-\lambda_1)\mathbb{E}\left[
\frac{S}{\sqrt{n}}
\right]
\]
</p>

<p>
Buscamos por tanto minimizar \(\lambda_2-\lambda_1\).
</p>
</div>
</div>

<div id="outline-container-org580b753" class="outline-5">
<h5 id="org580b753">Minimización</h5>
<div class="outline-text-5" id="text-org580b753">
<p>
Usamos multiplicadores de Lagrange para definir:
</p>

<p>
\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]
</p>

<p>
De donde deducimos \(\Phi'(\lambda_1) = \Phi'(\lambda_2)\). Como la t de Student es simétrica
y monótona en cada mitad, tenemos \(\lambda_1 = -\lambda_2\).
</p>
</div>
</div>

<div id="outline-container-org75ebdf8" class="outline-5">
<h5 id="org75ebdf8">Conclusión</h5>
<div class="outline-text-5" id="text-org75ebdf8">
<p>
Buscamos entonces el \(t_{\alpha/2}\) que cumple \(P(Z > t_{\alpha/2}) = \alpha/2\).
</p>
</div>
</div>
</div>

<div id="outline-container-org829c481" class="outline-4">
<h4 id="org829c481">B1. Intervalo para la varianza de una normal con media conocida</h4>
<div class="outline-text-4" id="text-org829c481">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\). El intervalo para \(\sigma^2\) de menor longitud media uniforme
a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;\alpha/2}}
,\quad
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;1-\alpha/2}}
\right)
\]
</p>

<p>
Donde \(\chi^2_{n;\alpha/2}\) cumple que \(P(Z > \chi^2_{n;\alpha/2}) = \alpha/2\) para \(Z \leadsto \chi^2(n)\).
</p>
</div>

<div id="outline-container-orgd07d205" class="outline-5">
<h5 id="orgd07d205">Pivote</h5>
<div class="outline-text-5" id="text-orgd07d205">
<p>
Tomamos como pivote a la función:
</p>

<p>
\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi(n)
\]
</p>
</div>
</div>

<div id="outline-container-orgd9dbe03" class="outline-5">
<h5 id="orgd9dbe03">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-orgd9dbe03">
<p>
Usando el pivote tenemos el siguiente intervalo de confianza:
</p>

<p>
\[
1-\alpha = P\left(
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_2}
\leq
\sigma^2
\leq
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_1}
\right)
\]
</p>

<p>
Donde, si \(\Phi\) es la función de distribución de \(\chi(n)\), tenemos que
\(1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)\). Buscamos minimizar \(1/\lambda_1-1/\lambda_2\).
</p>
</div>
</div>

<div id="outline-container-orgc51b28c" class="outline-5">
<h5 id="orgc51b28c">Minimización</h5>
<div class="outline-text-5" id="text-orgc51b28c">
<p>
Usamos minimizadores de Lagrange. Definimos:
</p>

<p>
\[
F(\lambda_1,\lambda_2) = 
\frac{1}{\lambda_2}-\frac{1}{\lambda_1} + 
\lambda(\Phi(\lambda_2)-\Phi(\lambda_1) - (1-\alpha))
\]
</p>

<p>
Calculando las derivadas parciales tenemos:
</p>

<p>
\[
\lambda\Phi'(\lambda_2) - \frac{1}{\lambda_2^2} = 0
\]
\[
\lambda\Phi'(\lambda_1) - \frac{1}{\lambda_1^2} = 0
\]
</p>

<p>
Y por tanto se minimiza cuando \(\Phi(\lambda_1)/\Phi(\lambda_2) = \lambda_2^2/\lambda_1^2\). En la práctica
se usa el intervalo de colas iguales.
</p>
</div>
</div>

<div id="outline-container-org47aec9b" class="outline-5">
<h5 id="org47aec9b">Conclusión</h5>
<div class="outline-text-5" id="text-org47aec9b">
<p>
El intervalo de colas iguales nos da \(\lambda_1 = \chi^2_{n;1-\alpha/2}\) y \(\lambda_2 = \chi^2_{n;\alpha/2}\).
</p>
</div>
</div>
</div>

<div id="outline-container-orgc57b8ad" class="outline-4">
<h4 id="orgc57b8ad">B2. Intervalo para la varianza de una normal con media desconocida</h4>
<div class="outline-text-4" id="text-orgc57b8ad">
<p>
Sea \(X \leadsto {\cal N}(\mu,\sigma^2)\). El intervalo para \(\sigma^2\) de menor longitud media uniforme
a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
\frac{(n-1)S^2}{\chi^2_{n-1;\alpha/2}}
,\quad
\frac{(n-1)S^2}{\chi^2_{n-1;1-\alpha/2}}
\right)
\]
</p>

<p>
Donde \(\chi^2_{n-1;\alpha/2}\) cumple que \(P(Z > \chi^2_{n-1;\alpha/2}) = \alpha/2\) para \(Z \leadsto \chi^2(n-1)\).
</p>
</div>

<div id="outline-container-org7d2bc67" class="outline-5">
<h5 id="org7d2bc67">Pivote</h5>
<div class="outline-text-5" id="text-org7d2bc67">
<p>
Tomamos como pivote:
</p>

<p>
\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org716466c" class="outline-4">
<h4 id="org716466c">C1. Intervalo para la diferencia de medias de normales de varianza dada</h4>
<div class="outline-text-4" id="text-org716466c">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\), \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\). El intervalo para \(\mu_1-\mu_2\) de menor
longitud media uniforme a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
\overline{X}-\overline{Y} - 
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
,\quad
\overline{X}-\overline{Y} +
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
\right)
\]
</p>

<p>
Donde \(z_{\alpha/2}\) cumple que \(P(Z>z_{\alpha/2}) = \alpha/2\) para \(Z\leadsto {\cal N}(0,1)\).
</p>
</div>

<div id="outline-container-orgb9d5706" class="outline-5">
<h5 id="orgb9d5706">Pivote</h5>
<div class="outline-text-5" id="text-orgb9d5706">
<p>
Usamos como pivote:
</p>

<p>
\[
\frac
{\overline{X}-\overline{Y}-(\mu_1-\mu_2)}
{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}
\leadsto
{\cal N}(0,1ñ)
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org0efa616" class="outline-4">
<h4 id="org0efa616">C2. Intervalo para la diferencia de medias de normales de varianza igual</h4>
<div class="outline-text-4" id="text-org0efa616">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma^2)\), \(Y \leadsto {\cal N}(\mu_2, \sigma^2)\). El intervalo para \(\mu_1-\mu_2\) de menor
longitud media uniforme a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]
</p>

<p>
donde,
</p>

<p>
\[
S_p = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S^2_2}{n_1+n_2-2}}
\]
</p>

<p>
y donde \(t_{n_1+n_2-2;\alpha/2}\) cumple que \(P(Z > t_{\alpha/2}) = \alpha/2\) con \(Z \leadsto t(n_1+n_2-2)\).
</p>
</div>

<div id="outline-container-orga2a61e1" class="outline-5">
<h5 id="orga2a61e1">Pivote</h5>
<div class="outline-text-5" id="text-orga2a61e1">
<p>
Tomamos como pivote a la función:
</p>


<p>
\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
S_p
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]
</p>
</div>
</div>

<div id="outline-container-org6ce1987" class="outline-5">
<h5 id="org6ce1987">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-org6ce1987">
<p>
Usando el pivote tenemos el siguiente invervalo de confianza:
</p>

<p>
\[
1-\alpha = P\left(
\overline{X}-\overline{Y} - \lambda_1S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\leq
\mu_1-\mu_2
\leq
\overline{X}-\overline{Y} + \lambda_2S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]
</p>

<p>
Donde, si \(\Phi\) es la función de distribución de \(t(n_1+n_2-2)\), tenemos
que \(1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)\). Buscamos minimizar \(\lambda_2-\lambda_1\).
</p>
</div>
</div>

<div id="outline-container-org0bc6063" class="outline-5">
<h5 id="org0bc6063">Minimización</h5>
<div class="outline-text-5" id="text-org0bc6063">
<p>
Usaremos minimizadores de Lagrange para deducir de nuevo que
\(\Phi'(\lambda_1) = \Phi'(\lambda_2)\), por monotonía y simetricidad, \(\lambda_1=\lambda_2\).
</p>
</div>
</div>

<div id="outline-container-org6511c01" class="outline-5">
<h5 id="org6511c01">Conclusión</h5>
<div class="outline-text-5" id="text-org6511c01">
<p>
Buscamos entonces el \(t_{\alpha/2}\) que cumple \(P(Z > t_{\alpha/2}) = \alpha/2\).
</p>
</div>
</div>
</div>

<div id="outline-container-orge6eadbf" class="outline-4">
<h4 id="orge6eadbf">D1. Intervalo para el cociente de varianzas de normales de media dada</h4>
<div class="outline-text-4" id="text-orge6eadbf">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma_1^2)\), \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\). El intervalo para \(\sigma_1^2/\sigma_2^2\) de menor
longitud media uniforme a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
F_{n_2,n_1;1-\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
,
F_{n_2,n_1;\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
\right)
\]
</p>

<p>
donde, \(F_{n_2,n_1;\alpha/2}\) cumple que \(P(Z > F_{\alpha/2}) = \alpha/2\) con \(Z \leadsto F(n_2,n_1)\).
</p>
</div>

<div id="outline-container-org8ecdb0c" class="outline-5">
<h5 id="org8ecdb0c">Pivote</h5>
<div class="outline-text-5" id="text-org8ecdb0c">
<p>
Tomamos como pivote a la función:
</p>

<p>
\[
\frac
{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2 / n_2\sigma_2^2}
{\sum_{i=1}^{n_2} (X_i-\mu_1)^2 / n_1\sigma_1^2}
\leadsto
F(n_2,n_1)
\]
</p>
</div>
</div>

<div id="outline-container-org6aa70b5" class="outline-5">
<h5 id="org6aa70b5">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-org6aa70b5">
<p>
Usando el pivote tenemos el siguiente intervalo de confianza:
</p>

<p>
\[
1 - \alpha = P
\left(
\lambda_1\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\leq
\frac{\sigma^2_1}{\sigma^2_2}
\leq
\lambda_2\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\right)
\]
</p>

<p>
Donde, si \(\Phi\) es la función de distribución de \(F(n_1,n_2)\), tenemos que
\(1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)\). Buscamos minimizar \(\lambda_2-\lambda_1\).
</p>
</div>
</div>

<div id="outline-container-org38292dc" class="outline-5">
<h5 id="org38292dc">Minimización</h5>
<div class="outline-text-5" id="text-org38292dc">
<p>
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a \(\Phi'(\lambda_2) = \Phi'(\lambda_1)\). Nótese que en este caso la distribución no es
simétrica.
</p>
</div>
</div>

<div id="outline-container-org64c1237" class="outline-5">
<h5 id="org64c1237">Conclusión</h5>
<div class="outline-text-5" id="text-org64c1237">
<p>
Buscamos entonces:
</p>

<ul class="org-ul">
<li>el \(F_{n_2,n_1;1-\alpha/2}\) que cumple \(P(Z > F) = 1-\alpha/2\).</li>
<li>el \(F_{n_2,n_1;\alpha/2}\) que cumple \(P(Z>F) = \alpha/2\).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org32ddd3e" class="outline-4">
<h4 id="org32ddd3e">D2. Intervalo para el cociente de varianzas de normales de media desconocida</h4>
<div class="outline-text-4" id="text-org32ddd3e">
<p>
Sean \(X \leadsto {\cal N}(\mu_1,\sigma^2_1)\), \(Y \leadsto {\cal N}(\mu_2,\sigma_2^2)\). El intervalo para \(\sigma^2_1/\sigma^2_2\) de menor
longitud media uniforme a nivel de confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
F_{n_2-1,n_1-1; 1-\alpha/2}\frac{S_1^2}{S_2^2}
,
F_{n_2-1,n_1-1; \alpha/2}\frac{S_1^2}{S_2^2}
\right)
\]
</p>

<p>
donde, \(F_{n_2-1,n_1-1; 1-\alpha/2}\) cumple que \(P(Z > F_{\alpha/2}) = \alpha/2\) con \(Z \leadsto F(n_2-1,n_1-1)\).
</p>
</div>

<div id="outline-container-orgdc0372d" class="outline-5">
<h5 id="orgdc0372d">Pivote</h5>
<div class="outline-text-5" id="text-orgdc0372d">
<p>
Tomamos como pivote a la función:
</p>

<p>
\[
\frac{S_2^2/\sigma_2^2}{S_1^2/\sigma_1^2}
\leadsto
F(n_2-1,n_1-1)
\]
</p>
</div>
</div>

<div id="outline-container-org13c712d" class="outline-5">
<h5 id="org13c712d">Intervalos candidatos</h5>
<div class="outline-text-5" id="text-org13c712d">
<p>
Usando el pivote llegamos al siguiente intervalo de confianza:
</p>

<p>
\[
1-\alpha = P
\left(
\frac{\sigma_1^2}{\sigma_2^2}\lambda_1
\leq
\frac{S_2^2}{S_1^2}
\leq
\frac{\sigma_1^2}{\sigma_2^2}\lambda_2
\right)
\]
</p>

<p>
Donde, si \(\Phi\) es la función de distribución de \(F(n_2-1,n_1-1)\), tenemos
que \(1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)\). Buscamos minimizar \(\lambda_2-\lambda_1\).
</p>
</div>
</div>

<div id="outline-container-orgefd1a6e" class="outline-5">
<h5 id="orgefd1a6e">Minimización</h5>
<div class="outline-text-5" id="text-orgefd1a6e">
<p>
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a \(\Phi'(\lambda_2) = \Phi'(\lambda_1)\). Nótese que en este caso la distribución no es
simétrica.
</p>
</div>
</div>

<div id="outline-container-org7fc3efd" class="outline-5">
<h5 id="org7fc3efd">Conclusión</h5>
<div class="outline-text-5" id="text-org7fc3efd">
<p>
Buscamos entonces:
</p>

<ul class="org-ul">
<li>el \(F_{n_2-1,n_1-1;1-\alpha/2}\) que cumple \(P(Z > F) = 1-\alpha/2\).</li>
<li>el \(F_{n_2-1,n_1-1;\alpha/2}\) que cumple \(P(Z>F) = \alpha/2\).</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org4b6cdd1" class="outline-3">
<h3 id="org4b6cdd1"><span class="todo TODO">TODO</span> Intervalos unilaterales</h3>
</div>
</div>
<div id="outline-container-org60695db" class="outline-2">
<h2 id="org60695db">6. Contraste de hipótesis</h2>
<div class="outline-text-2" id="text-org60695db">
</div><div id="outline-container-orge05f528" class="outline-3">
<h3 id="orge05f528">Planteamiento del problema</h3>
<div class="outline-text-3" id="text-orge05f528">
</div><div id="outline-container-orgfdec99c" class="outline-4">
<h4 id="orgfdec99c">Problema de contraste de hipótesis</h4>
<div class="outline-text-4" id="text-orgfdec99c">
<p>
Dada \((X_1,\dots,X_n)\) una muestra aleatoria simple de \(X \leadsto P_\theta\), para
\(\theta \in \Theta_0 \cup \Theta_1\), llamamos:
</p>

<ul class="org-ul">
<li><b>Hipótesis nula</b>: \(H_0 : \theta \in \Theta_0\)</li>
<li><b>Hipótesis alternativa</b>: \(H_1 : \theta \in \Theta_1\)</li>
</ul>

<p>
a dos hipótesis posibles.
</p>
</div>
</div>

<div id="outline-container-org39550bb" class="outline-4">
<h4 id="org39550bb">Test de hipótesis</h4>
<div class="outline-text-4" id="text-org39550bb">
<p>
El <b>test de hipótesis</b> es un estadístico \(\varphi\) tomando valores en \([0,1]\), que 
da la posibilidad de rechazar \(H_0\) dada una realización muestral. Se 
llama:
</p>

<ul class="org-ul">
<li><b>Test no aleatorizado</b>, si toma valores \(0,1\).</li>
<li><b>Test aleatorizado</b>, si toma valor distinto de \(0,1\).</li>
</ul>
</div>
</div>

<div id="outline-container-org3cf1107" class="outline-4">
<h4 id="org3cf1107">Tipos de errores de un test de hipótesis</h4>
<div class="outline-text-4" id="text-org3cf1107">
<p>
Hay dos tipos de erorres:
</p>

<ul class="org-ul">
<li><b>Error de tipo 1</b>: Rechazar \(H_0\) siendo cierta. Falso negativo.</li>
<li><b>Error de tipo 2</b>: Aceptar \(H_0\) siendo falsa. Falso positivo.</li>
</ul>
</div>
</div>

<div id="outline-container-orgb99241d" class="outline-4">
<h4 id="orgb99241d">Función de potencia de un test</h4>
<div class="outline-text-4" id="text-orgb99241d">
<p>
Dado un test \(\varphi\), su función de potencia \(\beta_\varphi : \Theta \longrightarrow [0,1]\) se define:
</p>

<p>
\[\beta_\varphi(\theta) = E_\theta[\varphi(X_1,\dots,X_n)]\]
</p>

<p>
Que es la probabilidad media de rechazar \(H_0\) bajo \(P_\theta\).
</p>
</div>
</div>

<div id="outline-container-org9f0c2c5" class="outline-4">
<h4 id="org9f0c2c5">Tamaño del test</h4>
<div class="outline-text-4" id="text-org9f0c2c5">
<p>
El tamaño del test es \(\sup_{\theta \in \Theta_0} \beta_\varphi(\theta)\), la máxima probabilidad media de
cometer un error de tipo 1.
</p>
</div>
</div>

<div id="outline-container-orgf1e7f74" class="outline-4">
<h4 id="orgf1e7f74">Nivel de significación de un test</h4>
<div class="outline-text-4" id="text-orgf1e7f74">
<p>
Un test \(\varphi\) tiene nivel de significación \(\alpha\) si su tamaño es menor o igual
que \(\alpha\). Es decir,
</p>

<p>
\[
\forall \theta \in \Theta_0, \quad
\beta_\varphi(\theta) =
E_\theta[\varphi(X_1,\dots,X_n)] \leq
\alpha
\]
</p>
</div>
</div>

<div id="outline-container-org7289311" class="outline-4">
<h4 id="org7289311">Test uniformemente más potente</h4>
<div class="outline-text-4" id="text-org7289311">
<p>
Un test con nivel de significación \(\alpha\) es uniformemente más potente a 
dicho nivel si para cualquier otro test \(\varphi'\) con nivel de significación
\(\alpha\), se tiene:
</p>

<p>
\[\beta_{\varphi'}(\theta) \leq \beta_\varphi(\theta)
\quad \forall \theta \in \Theta_1\]
</p>
</div>
</div>
</div>

<div id="outline-container-org5f50696" class="outline-3">
<h3 id="org5f50696">Lema de Neyman-Pearson</h3>
<div class="outline-text-3" id="text-org5f50696">
</div><div id="outline-container-org7f12c76" class="outline-4">
<h4 id="org7f12c76">El problema de contraste</h4>
<div class="outline-text-4" id="text-org7f12c76">
<p>
Fijado un nivel de significación, encontrar el test uniformemente más
potente a dicho nivel.
</p>
</div>
</div>

<div id="outline-container-orgfb06133" class="outline-4">
<h4 id="orgfb06133">Lema de Neyman-Pearson</h4>
<div class="outline-text-4" id="text-orgfb06133">
<p>
Sea \(X \longrightarrow \{P_{\theta_0}, P_{\theta_1}\}\) y \((X_1,\dots,X_n)\) una muestra aleatoria simple
con funciones de densidad \(f_0,f_1\). Consideramos el problema de contraste 
con \(H_0 : \theta = \theta_0\) y \(H_1 : \theta = \theta_1\).
</p>

<ol class="org-ol">
<li><p>
Cualquier test de la forma:
</p>

<p>
\[
     \varphi(\tilde X) = 
     \threepartdef
     {1}{f_1(\tilde X) > kf_0(\tilde X)}
     {\gamma(\tilde X)}{f_1(\tilde X) = kf_0(\tilde X)}
     {0}{f_1(\tilde X) < kf_0(\tilde X)}
     \]
</p>

<p>
con \(k \in \mathbb{R}^+_0\) y \(\gamma(X_1,\dots,X_n) \in [0,1]\), es de máxima potencia entre todos
los de nivel de significación \(\alpha = E_{\theta_0}[\varphi]\), su tamaño.
</p></li>

<li>Para todo \(\alpha \in (0,1]\) existe un test de la forma anterior con
\(\gamma(X_1,\dots,X_n) = \gamma\) constante y tamaño \(\alpha\).</li>

<li>Si \(\varphi'\) es de máxima potencia al nivel de significación \(\alpha = E_{\theta_0}[\varphi']\), 
entonces \(\varphi'\) es de la forma anterior con probabilidad 1 bajo \(P_{\theta_0}\) y \(P_{\theta_1}\).</li>

<li><p>
El test de máxima potencia entre todos los de nivel de significación
0 es:
</p>

<p>
\[
     \varphi_0(\tilde X) = \twopartdef
     {1}{f_0(\tilde X) = 0}
     {0}{f_0(\tilde X) > 0}
     \]
</p></li>
</ol>
</div>

<div id="outline-container-org16ba848" class="outline-5">
<h5 id="org16ba848">Demostración</h5>
<div class="outline-text-5" id="text-org16ba848">
</div><div id="outline-container-orgaf79cc8" class="outline-6">
<h6 id="orgaf79cc8">Punto 1</h6>
<div class="outline-text-6" id="text-orgaf79cc8">
<p>
Dado otro test \(\varphi'\), como toma valores en \([0,1]\) tenemos que:
</p>

<p>
\[
(\varphi-\varphi')(f_1-kf_0) \geq 0
\]
</p>

<p>
Podemos entonces integrar para tener:
</p>

<p>
\[
\int (\varphi(x)-\varphi'(x))(f_1(x)-kf_0(x)) \;dx \geq 0
\]
</p>

<p>
Conociendo las funciones de potencia \(\int \varphi f_i = \beta_\varphi(\theta_i)\) y \(\int \varphi' f_i = \beta_{\varphi'}(\theta_i)\) , 
tenemos:
</p>

<p>
\[
\beta_{\varphi}(\theta_1) - \beta_{\varphi'}(\theta_1) +
k(\beta_{\varphi'}(\theta_0) - \beta_{\varphi}(\theta_0))
\geq 0\]
</p>

<p>
Usando ahora que \(\beta_{\varphi'}(\theta_0) \leq \beta_\varphi(\theta_0) = \alpha\), tenemos que \(\beta_\varphi(\theta_1) \geq \beta_{\varphi'}(\theta_1)\).
Así, nuestro test es el más potente uniformemente.
</p>
</div>
</div>

<div id="outline-container-orgd2c3c41" class="outline-6">
<h6 id="orgd2c3c41">Punto 3</h6>
<div class="outline-text-6" id="text-orgd2c3c41">
<p>
Cuando es de máxima potencia, se da el caso de igualdad en la última
ecuación, que lleva el caso de igualdad a la integral. Como es una
integral de términos positivos, debe ser distinta de cero sólo en
un conjunto de medida nula.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orge410ae8" class="outline-3">
<h3 id="orge410ae8"><span class="todo TODO">TODO</span> Descripción mediante p-valores</h3>
</div>
<div id="outline-container-orgc4fe29d" class="outline-3">
<h3 id="orgc4fe29d">Test de la razón de verosimilitudes</h3>
<div class="outline-text-3" id="text-orgc4fe29d">
</div><div id="outline-container-org8cbc477" class="outline-4">
<h4 id="org8cbc477">Test de la razón de verosimilitudes</h4>
<div class="outline-text-4" id="text-org8cbc477">
<p>
Sea \((X_1,\dots,X_n) \in \chi^n\) una muestra aleatoria simple de \(X \leadsto \{P_\theta \mid \theta \in \Theta_0 \cup \Theta_1\}\).
El test de razón de verosimilitudes para el problema de contraste con
\(H_0 : \theta \in \Theta_0\) y \(H_1 : \theta \in \Theta_1\); se define como:
</p>

<p>
\[
\varphi(X_1,\dots,X_n) = \twopartdef
{1}{\lambda(X_1,\dots,X_n) < c}
{0}{\lambda(X_1,\dots,X_n) \geq c}
\]
</p>

<p>
donde se define:
</p>

<p>
\[\lambda(x_1,\dots,x_n) = \frac
{\sup_{\theta \in \Theta_0} L_{x_1,\dots,x_n}(\theta)}
{\sup_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)}
\]
</p>

<p>
siendo \(L\) la función de verosimilitud y \(c \in (0,1]\) una constante que se 
determina imponiendo el tamaño o nivel de significación requerido.
</p>
</div>
</div>
</div>

<div id="outline-container-orge7010c0" class="outline-3">
<h3 id="orge7010c0">Dualidad entre tests de hipótesis y regiones de confianza</h3>
<div class="outline-text-3" id="text-orge7010c0">
</div><div id="outline-container-org798fca1" class="outline-4">
<h4 id="org798fca1">Dualidad</h4>
<div class="outline-text-4" id="text-org798fca1">
<p>
Sea \(X \leadsto \{P_\theta \mid \theta \in \Theta\}\). Para cada \(\theta_0 \in \Theta\) consideramos un conjunto \(A(\theta_0) \subseteq \chi^n\)
y para cada relización se define:
</p>

<p>
\[
\varphi_{\theta_0}(x_1,\dots,x_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } (x_1,\dots,x_n) \notin A(\theta_0) \\
0 & \mbox{if } (x_1,\dots,x_n) \in A(\theta_0) \\
\end{array} 
\right.
\]
</p>

<p>
Y con esto se define:
</p>

<p>
\[
S(x_1,\dots,x_n) = \{\theta\in \Theta \mid (x_1,\dots,x_n) \in A(\theta)\}
\]
</p>

<p>
Cada uno de los tests \(\varphi_{\theta_0}\) tiene nivel de significación \(\alpha\) ssi \(S\) es una
región de confianza para \(\theta\) a nivel de confianza \(1-\alpha\).
</p>
</div>
</div>
</div>

<div id="outline-container-org4d2f0d8" class="outline-3">
<h3 id="org4d2f0d8">Ejemplos</h3>
<div class="outline-text-3" id="text-org4d2f0d8">
</div><div id="outline-container-orgbf1dd14" class="outline-4">
<h4 id="orgbf1dd14">Contrastes sobre la media de una normal con varianza conocida</h4>
<div class="outline-text-4" id="text-orgbf1dd14">
</div><div id="outline-container-org8f09b9f" class="outline-5">
<h5 id="org8f09b9f">Hipótesis: valor de la media</h5>
<div class="outline-text-5" id="text-org8f09b9f">
<p>
Si tomamos \(H_0:\mu=\mu_0\) y \(H_1:\mu\neq\mu_0\), podemos crear un <a href="#orgc4fe29d">TRV</a> como:
</p>

<p>
\[
\varphi(X_1,\dots,X_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| > z_{\alpha/2} \\
0 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| \leq z_{\alpha/2} \\
\end{array} 
\right.
\]
</p>
</div>

<div id="outline-container-org63dfc54" class="outline-6">
<h6 id="org63dfc54">Cálculo</h6>
<div class="outline-text-6" id="text-org63dfc54">
<p>
Sabiendo que la función de verosimilitud es:
</p>

<p>
\[
L_{x_1,\dots,x_n}(\mu) = 
\frac{1}{(\sigma_0^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma_0^2}
\]
</p>

<p>
Calculamos el test:
</p>

<p>
\[
\lambda = 
\frac{\sup_{\mu = \mu_0} L(\mu)}{\sup_{\mu\in\mathbb{R}} L(\mu)} = 
\frac{L(\mu_0)}{L(\overline{x})} =
\exp\left\{\frac{-n(\overline{x}-\mu_0)^2}{2\sigma^2_0}\right\}
\]
</p>

<p>
Y podemos tomar raíces para tener otro test equivalente que, al
seguir una distribución normal, podemos ajustar para tener el
parámetro \(\alpha\) pedido.
</p>
</div>
</div>
</div>

<div id="outline-container-org0f20095" class="outline-5">
<h5 id="org0f20095">Hipótesis: media menor que un valor</h5>
<div class="outline-text-5" id="text-org0f20095">
<p>
Si tomamos \(H_0 : \mu \leq \mu_0\) y \(H_1 : \mu > \mu_0\), podemos crear un TRV como:
</p>

<p>
\[
\varphi(X_1,\dots,X_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} > z_\alpha \\
0 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \leq z_\alpha \\
\end{array} 
\right.
\]
</p>
</div>

<div id="outline-container-org8e9c285" class="outline-6">
<h6 id="org8e9c285">Cálculo</h6>
<div class="outline-text-6" id="text-org8e9c285">
<p>
Si en este caso calculamos la \(\lambda\) tenemos que:
</p>

<p>
\[
\lambda(x_1,\dots,x_n) = 
\frac{\sup_{\mu\leq\mu_0} L(\mu)}{L(\overline{x})} =
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \leq 0 \\
\frac{L(\mu_0)}{L(\overline{x})} 
& \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq 0 \\
\end{array} 
\right.
\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgb3a9931" class="outline-5">
<h5 id="orgb3a9931">Hipótesis: media mayor que un valor</h5>
<div class="outline-text-5" id="text-orgb3a9931">
<p>
Si tomamos \(H_0 : \mu \geq \mu_0\) y \(H_1 : \mu < \mu_0\), podemos crear un TRV como:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < z_{1-\alpha} \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq z_{1-\alpha}
\end{array} 
\right.
\]
</p>
</div>

<div id="outline-container-org15830bf" class="outline-6">
<h6 id="org15830bf">Cálculo</h6>
<div class="outline-text-6" id="text-org15830bf">
<p>
Si en este caso calculamos la \(\lambda\) tenemos que:
</p>

<p>
\[
\lambda(x_1,\dots,x_n) 
=
\frac{\sup_{\mu\geq\mu_0} L(\mu)}{L(\overline{x})}
=
\left\{\begin{array}{ll} 
1& \mbox{if } \overline{x} \geq \mu_0 \\
\frac{L(\mu_0)}{L(\overline{x})}& \mbox{if } \overline{x} \leq \mu_0
\end{array} 
\right.
\]
</p>

<p>
Dándonos un test:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \lambda(x_1,\dots,x_n) < c \\
0 & \mbox{if } \lambda(x_1,\dots,x_n) \geq c
\end{array} 
\right.
=
\left\{\begin{array}{ll} 
0 & \mbox{if } \overline{x} \geq \mu_0 \\
1 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} < c \\
0 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} \geq c
\end{array} 
\right.
\]
</p>

<p>
Las dos primeras condiciones colapsan cuando tomamos la raíz y
comparamos con ella:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < c \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq \min(0,c)
\end{array} 
\right.
\]
</p>

<p>
Ahora ajustamos la \(c\) para que nos dé la significancia \(\alpha\):
</p>

<p>
\[
\alpha = 
\sup_{\mu \geq \mu_0} 
P\left( \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} < c' \right)
=
\sup_{\mu \geq \mu_0}
P\left( \frac{\overline{X}-\mu}{\sigma_0/\sqrt{n}} < \frac{\mu_0-\mu}{\sigma_0/\sqrt{n}} + c' \right)
\]
</p>

<p>
Tomamos entonces como \(c' = z_{1 - \alpha}\). Nótese que es negativo.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0596573" class="outline-4">
<h4 id="org0596573">Contrastes sobre la varianza de una normal con media conocida</h4>
<div class="outline-text-4" id="text-org0596573">
</div><div id="outline-container-orga54712c" class="outline-5">
<h5 id="orga54712c">Hipótesis: valor de la varianza</h5>
<div class="outline-text-5" id="text-orga54712c">
<p>
Si tomamos \(H_0 : \sigma^2 = \sigma_0^2\) y \(H_1 : \sigma^2 \neq \sigma_0^2\), creamos un TRV como:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < \chi^2_{n;1-\alpha/2} 
   \mbox{ ó } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > \chi^2_{n;\alpha/2} \\
0& \mbox{if } \chi^2_{n;1-\alpha/2} \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq \chi^2_{n;\alpha/2}
\end{array} 
\right.
\]
</p>
</div>

<div id="outline-container-orgf492bda" class="outline-6">
<h6 id="orgf492bda">Cálculo</h6>
<div class="outline-text-6" id="text-orgf492bda">
<p>
Sabiendo que la función de verosimilitud es:
</p>

<p>
\[
L_{x_1,\dots,x_n}(\sigma) = 
\frac{1}{(\sigma^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma^2}
\]
</p>

<p>
Usamos que el estimador máximo verosímil de \(\sigma^2\) es:
</p>

<p>
\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\mu_0)^2}{n}
\]
</p>

<p>
Calculamos el test:
</p>

<p>
\[
\lambda = 
\frac{\sup_{\sigma = \sigma_0} L(\sigma)}{\sup_{\sigma\in\mathbb{R}} L(\sigma)} = 
\frac{L(\sigma_0)}{L(\widehat\sigma)} =
\left(\frac{\widehat\sigma^2}{\sigma_0^2}\right)^{n/2}
\exp\left\{
\frac{-n\widehat\sigma_0^2}{2\sigma_0^2}+\frac{n}{2}
\right\}
\]
</p>

<p>
La función \(xe^{-x}\) tiene un máximo antes de ir hacia \(-\infty\) por ambos
lados. Así, podemos escribir:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < c_1 
   \mbox{ ó } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > c_2 \\
0& \mbox{if } c_1 \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq c_2
\end{array} 
\right.
\]
</p>

<p>
Nótese que sigue una distribución \(\chi^2(n)\) como suma de \(n\) normales.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org447fb5c" class="outline-4">
<h4 id="org447fb5c">Contrastes sobre la varianza de una normal con media desconocida</h4>
<div class="outline-text-4" id="text-org447fb5c">
<p>
Se tendrá como estimador máximo verosímil a:
</p>

<p>
\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n}
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org656d6be" class="outline-2">
<h2 id="org656d6be">7. Teoría general de modelos lineales</h2>
<div class="outline-text-2" id="text-org656d6be">
</div><div id="outline-container-org131bcd0" class="outline-3">
<h3 id="org131bcd0">Modelo lineal general y modelo de Gauss Markov</h3>
<div class="outline-text-3" id="text-org131bcd0">
</div><div id="outline-container-org5c06303" class="outline-4">
<h4 id="org5c06303">Modelo lineal general</h4>
<div class="outline-text-4" id="text-org5c06303">
<p>
El modelo general lineal queda descrito por:
</p>

<p>
\[\mathbf{Y = X\beta + \varepsilon}\]
</p>
</div>

<div id="outline-container-org65b8e4c" class="outline-5">
<h5 id="org65b8e4c">Vector observable</h5>
<div class="outline-text-5" id="text-org65b8e4c">
<p>
\(Y = (Y_1,\dots,Y_n)\) es un vector aleatorio observable.
</p>
</div>
</div>

<div id="outline-container-org3d58a38" class="outline-5">
<h5 id="org3d58a38">Matriz de diseño</h5>
<div class="outline-text-5" id="text-org3d58a38">
<p>
Una matriz conocida \(X\) de dimensión \(n \times k\), cuyo rango determina el 
rango del modelo.
</p>
</div>
</div>

<div id="outline-container-org8d00146" class="outline-5">
<h5 id="org8d00146">Vector de efectos</h5>
<div class="outline-text-5" id="text-org8d00146">
<p>
Un vector desconocido \(\beta = (\beta_1,\dots,\beta_k)\).
</p>
</div>
</div>

<div id="outline-container-orgd8b1625" class="outline-5">
<h5 id="orgd8b1625">Vector de errores</h5>
<div class="outline-text-5" id="text-orgd8b1625">
<p>
Un vector aleatorio no observable \(\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)\) representando el 
error entre \(Y\) y \(X\beta\).
</p>
</div>
</div>
</div>

<div id="outline-container-orga7e4ee7" class="outline-4">
<h4 id="orga7e4ee7">Modelo de Gauss-Markov</h4>
<div class="outline-text-4" id="text-orga7e4ee7">
<p>
Modelo lineal donde las componentes del vector de errores son variables
aleatorias de segundo orden, centradas, homocedásticas (igual varianza)
e incorreladas:
</p>

<ul class="org-ul">
<li>\(E[\varepsilon_i] = 0\)</li>
<li>\(E[\varepsilon_i^2] = \sigma^2\)</li>
<li>\(E[\varepsilon_i\varepsilon_j] = 0\)</li>
</ul>
</div>

<div id="outline-container-org79b52c0" class="outline-5">
<h5 id="org79b52c0">Enunciado vectorial</h5>
<div class="outline-text-5" id="text-org79b52c0">
<p>
Las condiciones sobre el vector de errores equivalen a exigir:
</p>

<ul class="org-ul">
<li>\(E[\varepsilon] = 0\)</li>
<li>\(E[\varepsilon\varepsilon^T] = \sigma^2 I_{n \times n}\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgc989357" class="outline-5">
<h5 id="orgc989357">Objetivo del modelo</h5>
<div class="outline-text-5" id="text-orgc989357">
<p>
Inferir \(\beta\) y \(\sigma^2\) a partir de observaciones del vector \(Y\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org02bfb18" class="outline-3">
<h3 id="org02bfb18">Estimación de mínimos cuadrados del vector de efectos</h3>
<div class="outline-text-3" id="text-org02bfb18">
</div><div id="outline-container-orga081b63" class="outline-4">
<h4 id="orga081b63">Modelo</h4>
<div class="outline-text-4" id="text-orga081b63">
<p>
En el modelo de Gauss-Markov queremos minimizar la suma de
cuadrados de los errores:
</p>

<p>
\[S^2(\beta) = 
\sum^n_{i=1} \varepsilon^2_i =
\|Y - X\beta\|^2\]
</p>
</div>

<div id="outline-container-orga3a0996" class="outline-5">
<h5 id="orga3a0996">Minimización</h5>
<div class="outline-text-5" id="text-orga3a0996">
<p>
Para minimizarlo, calculamos la derivada:
</p>

<p>
\[\frac{\partial}{\partial \beta_h} S^2(\beta) = 
-2 x_{ih} \sum^n_{i=1} \left(
Y_i - \sum^k_{j=1} x_{ij}\beta_j
\right) = 0
\]
</p>

<p>
Y obtenemos las ecuaciones normales siguientes:
</p>

<p>
\[
\sum^n_{i=1} Y_i x_{ih} = \sum^n_{i=1}\sum^k_{j=1} x_{ij}x_{ih}\beta_j
\]
</p>

<p>
Que pueden expresarse matricialmente como:
</p>

<p>
\[X^TY = (X^TX)\beta\]
</p>
</div>
</div>
</div>

<div id="outline-container-org7df021e" class="outline-4">
<h4 id="org7df021e">Estimador de mínimos cuadrados de beta</h4>
<div class="outline-text-4" id="text-org7df021e">
<p>
Llamamos \(\widehat\beta\) al estimador de mínimos cuadrados de \(\beta\).
</p>

<ul class="org-ul">
<li>Existencia: existe al menos un estimador de mínimos cuadrados de \(\beta\).</li>
<li>Unicidad: garantizada cuando el modelo es de rango máximo por tenerse
la solución \[\widehat\beta(Y) = (X^TX)^{-1}X^TY\].</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org90c443a" class="outline-3">
<h3 id="org90c443a">Funciones estimables</h3>
<div class="outline-text-3" id="text-org90c443a">
</div><div id="outline-container-org87bc814" class="outline-4">
<h4 id="org87bc814">Función lineal estimable</h4>
<div class="outline-text-4" id="text-org87bc814">
<p>
Una \(\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k\) es estimable si admite un estimador insesgado,
lineal en las componentes de \(Y\). Es decir:
</p>

<p>
\[\exists \widehat\psi(Y) = c_1Y_1 + \dots + c_nY_n\]
</p>

<p>
tal que \(E[\widehat\psi(Y)] = \psi(\beta)\).
</p>
</div>
</div>

<div id="outline-container-org67b2817" class="outline-4">
<h4 id="org67b2817">Teorema de Gauss-Markov</h4>
<div class="outline-text-4" id="text-org67b2817">
<p>
Si \(\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k\) es estimable, admite un único UMVUE. 
Dicho estimador es:
</p>

<p>
\[\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y) \]
</p>

<p>
Donde \(\widehat{\beta}(Y)\) es un estimador de mínimos cuadrados de \(\beta\).
</p>
</div>

<div id="outline-container-org481ff3d" class="outline-5">
<h5 id="org481ff3d"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org087df29" class="outline-4">
<h4 id="org087df29">Propiedades del estimador de mínimos cuadrados en modelos de rango máximo</h4>
<div class="outline-text-4" id="text-org087df29">
<p>
Sea \(\widehat\beta(Y) = (X^TX)^{-1}X^TY\) el estimador de mínimos cuadrados en un modelo 
de rango máximo.
</p>

<ol class="org-ol">
<li>\(\widehat\beta_j(Y)\) es el estimador lineal insesgado de mínima varianza de \(\beta_j\).</li>
<li>Las varianzas y covarianzas vienen dadas: \(Cov(\widehat\beta(Y)) = \sigma^2(X^TX)^{-1}\).</li>
<li>Toda función lineal de las componentes de \(\beta\) es estimable con 
estimador lineal insesgado de mínima varianza
\(\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y)\).</li>
</ol>
</div>

<div id="outline-container-orgd2b99fe" class="outline-5">
<h5 id="orgd2b99fe"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>
</div>

<div id="outline-container-org0dbcd10" class="outline-3">
<h3 id="org0dbcd10">Modelo estimado</h3>
<div class="outline-text-3" id="text-org0dbcd10">
</div><div id="outline-container-org41e3ae2" class="outline-4">
<h4 id="org41e3ae2">Modelo estimado</h4>
<div class="outline-text-4" id="text-org41e3ae2">
<p>
Siendo \(\widehat\beta\) el estimador de mínimos cuadrados, llamamos:
</p>

<ul class="org-ul">
<li>Modelo estimado: \(\widehat Y = X\widehat\beta\)</li>
<li>Residuos mínimo-cuadráticos: \(R = Y - X\widehat\beta\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgc7bad59" class="outline-4">
<h4 id="orgc7bad59">Propiedades del modelo estimado</h4>
<div class="outline-text-4" id="text-orgc7bad59">
<p>
El modelo estimado cumple:
</p>

<ol class="org-ol">
<li>\(\widehat Y_i\) es el estimador lineal insesgado de mínima varianza de \(E[Y_i]\).</li>
<li>Los residuos son centrados \(E[R_i] = 0\).</li>
<li><p>
El vector de residuos es ortogonal al vector estimado:
</p>

<p>
\[X^TR = 0,\quad \widehat{Y}^TR = 0\]
</p></li>
</ol>
</div>
</div>

<div id="outline-container-orgc5232ca" class="outline-4">
<h4 id="orgc5232ca">Varianza residual</h4>
<div class="outline-text-4" id="text-orgc5232ca">
<p>
Siendo \(r\) el rango de \(X\), la <b>varianza residual</b> es un estimador
insesgado de \(\sigma^2\):
</p>

<p>
\[
S^2_R = \frac{1}{n-r}\sum_{i=1}^n R_i^2 = \frac{1}{n-r}\|Y-X\widehat\beta\|^2
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org44aef74" class="outline-3">
<h3 id="org44aef74">Inferencia bajo hipótesis de normalidad</h3>
<div class="outline-text-3" id="text-org44aef74">
</div><div id="outline-container-orge100856" class="outline-4">
<h4 id="orge100856">Hipótesis de normalidad</h4>
<div class="outline-text-4" id="text-orge100856">
<p>
La hipótesis de normalidad asume que los errores se distribuyen
bajo una distribución normal:
</p>

<p>
\[
Y_i = \sum_{j=1}^k x_{ij}\beta_j + \varepsilon_i 
\leadsto 
{\cal N}\left(\sum_{j=1}^k x_{ij}\beta_j, \sigma^2 \right)
\]
</p>

<p>
Para \(Y_1,\dots,Y_n\) independientes.
</p>
</div>

<div id="outline-container-org892c9dc" class="outline-5">
<h5 id="org892c9dc">Equivalentemente</h5>
<div class="outline-text-5" id="text-org892c9dc">
<p>
Podemos expresar los errores como:
</p>

<p>
\[\varepsilon \leadsto {\cal N}(0,\sigma^2)\]
</p>
</div>
</div>
</div>

<div id="outline-container-org83cc5aa" class="outline-4">
<h4 id="org83cc5aa">Función de máxima verosimilitud</h4>
<div class="outline-text-4" id="text-org83cc5aa">
<p>
La función de máxima verosimilitud bajo la hipótesis de normalidad queda
como:
</p>

<p>
\[
L_y(\beta,\sigma^2) = 
\frac{1}{(2\pi)^{n/2}\sigma^n}
\exp\left\{
-\frac{\sum_{i=1}^n(y_i - \sum_{j=1}^k x_{ij}\beta_j)^2}{2\sigma^2}
\right\}
\]
</p>
</div>
</div>

<div id="outline-container-org149e533" class="outline-4">
<h4 id="org149e533">Estimadores máximo verosímiles de efectos</h4>
<div class="outline-text-4" id="text-org149e533">
<p>
Los estimadores máximo verosímiles de \(\beta\) son \(\widehat\beta\), estimadores de mínimos
cuadrados.
</p>
</div>

<div id="outline-container-org5e55ec2" class="outline-5">
<h5 id="org5e55ec2"><span class="todo TODO">TODO</span> Demostración</h5>
</div>
</div>

<div id="outline-container-org11171e2" class="outline-4">
<h4 id="org11171e2">Estimador máximo verosímil de la varianza</h4>
<div class="outline-text-4" id="text-org11171e2">
<p>
El estimador máximo verosímil de la varianza es:
</p>

<p>
\[
\widehat\sigma^2 = \frac{1}{n}\sum_{i=1}^n R_i^2 = \frac{n-r}{n}S^2_R
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgbe3038f" class="outline-2">
<h2 id="orgbe3038f">8. Inferencia Bayesiana</h2>
<div class="outline-text-2" id="text-orgbe3038f">
</div>

<div id="outline-container-org6a431a9" class="outline-3">
<h3 id="org6a431a9">8.1. Introducción</h3>
<div class="outline-text-3" id="text-org6a431a9">
</div><div id="outline-container-orgb12c179" class="outline-4">
<h4 id="orgb12c179">Ley de la probabilidad total</h4>
<div class="outline-text-4" id="text-orgb12c179">
<p>
La ley de la probabilidad total establece, para \(A_i\) una partición del
espacio de sucesos:
</p>

<p>
\[
P(B) = \sum_{i=1}^n P(B|A_i)P(A_i)
\]
</p>
</div>
</div>

<div id="outline-container-org06cb249" class="outline-4">
<h4 id="org06cb249">Teorema de Bayes</h4>
<div class="outline-text-4" id="text-org06cb249">
<p>
Para un espacio de probabilidad \((\Omega,{\cal A},P)\), si tenemos una partición dada
por \(A_1,\dots,A_n\) con probabilidad no nula:
</p>

<p>
\[P(A_i|B) 
=
\frac{P(A_i \cap B)}{P(B)} 
= 
\frac{P(B|A_i)P(A_i)}{P(B)}
\]
</p>
</div>
</div>

<div id="outline-container-org96e35d3" class="outline-4">
<h4 id="org96e35d3">Distribución a priori</h4>
<div class="outline-text-4" id="text-org96e35d3">
<p>
Sea \(X\) variable aleatoria con distribución \(f(x|\theta)\), con \(\theta \in \Theta\). A una 
distribución \(\pi(\theta)\) establecida con información previa sobre \(\theta\) se le
llama <b>distribución a priori</b>.
</p>
</div>
</div>

<div id="outline-container-orgeb0c738" class="outline-4">
<h4 id="orgeb0c738">Distribución condicionada</h4>
<div class="outline-text-4" id="text-orgeb0c738">
<p>
Dada una muestra \(\tilde{X} = (X_1,\dots,X_n)\) y una distribución a priori \(\pi(\theta)\),
tenemos una distribución conjunta con función de densidad:
</p>

<p>
\[
f(\tilde x,\theta) = f(\tilde x|\theta)\pi(\theta)
\]
</p>
</div>
</div>

<div id="outline-container-org7aec626" class="outline-4">
<h4 id="org7aec626">Distribución marginal</h4>
<div class="outline-text-4" id="text-org7aec626">
<p>
La distribución marginal de \(\tilde X\) la definimos como:
</p>

<p>
\[
m(\tilde x) 
= 
\int_{\Omega} f(\tilde x,\theta) d\theta
=
\int_{\Omega} f(\tilde x|\theta) \pi(\theta) d\theta
\]
</p>
</div>
</div>

<div id="outline-container-org6523947" class="outline-4">
<h4 id="org6523947">Distribución a posteriori</h4>
<div class="outline-text-4" id="text-org6523947">
<p>
Dada una realización de la muestra y una distribución a priori, definimos
una distribución a posteriori como:
</p>

<p>
\[
\pi(\theta|\tilde x) = 
\frac{f(\tilde x|\theta)\pi(\theta)}{m(\tilde x)} =
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org32d406d" class="outline-3">
<h3 id="org32d406d">8.2. Estadística clásica</h3>
<div class="outline-text-3" id="text-org32d406d">
<p>
Se destacan las siguientes diferencias con la estadística clásica.
En la estadística clásica:
</p>

<ol class="org-ol">
<li>La probabilidad se limita a sucesos con frecuencias relativas.</li>
<li>El parámetro \(\theta\) es fijo, completamente desconocido.</li>
<li>Se usan estimadores de máxima verosimilitud o insesgados.</li>
<li>Los tests de hipótesis se construyen fijando un tamaño \(\alpha\).</li>
</ol>

<p>
Mientras que en la estadística bayesiana:
</p>

<ol class="org-ol">
<li>La probabilidad se puede establecer previa a cualquier suceso.</li>
<li>El parámetro \(\theta\) es una variable aleatoria.</li>
<li>El método de muestreo es irrelevante.</li>
<li>Podemos calcular la probabilidad de que una hipótesis sea cierta.</li>
</ol>
</div>
</div>

<div id="outline-container-org691478b" class="outline-3">
<h3 id="org691478b">8.3. Familias conjugadas</h3>
<div class="outline-text-3" id="text-org691478b">
</div><div id="outline-container-org5ca33d4" class="outline-4">
<h4 id="org5ca33d4">Familia conjugada</h4>
<div class="outline-text-4" id="text-org5ca33d4">
<p>
Sea \({\cal F} = \{\pi_i(\theta) \mid i \in I\}\) una familia de distribuciones a priori. Se llama
<b>conjugada</b> respecto a una familia de densidades \(P = \{f(x|\theta) \mid \theta \in \Theta\}\) si
para cada \(\pi(\theta) \in {\cal F}\) y \(f(x\mid \theta) \in P\), se tiene que \(\pi(\theta\mid \tilde x) \in {\cal F}\).
</p>
</div>
</div>

<div id="outline-container-org9afd4a7" class="outline-4">
<h4 id="org9afd4a7">Lema de caracterización de familias conjugadas</h4>
<div class="outline-text-4" id="text-org9afd4a7">
<p>
Para \(\pi(\theta),\Pi(\theta) \in {\cal F}\), equivalen:
</p>

<ol class="org-ol">
<li>\(f(\tilde x|\theta)\pi(\theta) \propto \Pi(\theta)\)</li>
<li>\(\pi(\theta|\tilde x) = \Pi(\theta)\)</li>
</ol>
</div>

<div id="outline-container-orgac6f1e3" class="outline-5">
<h5 id="orgac6f1e3">Demostración</h5>
<div class="outline-text-5" id="text-orgac6f1e3">
</div><div id="outline-container-orge80d9b9" class="outline-6">
<h6 id="orge80d9b9">Primera implicación</h6>
<div class="outline-text-6" id="text-orge80d9b9">
<p>
Por definición:
</p>

<p>
\[
\pi(\theta|\tilde x) 
=
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
=
\frac{M\Pi(\theta)}{M \int_\Omega \Pi(\theta) d\theta}
=
\Pi(\theta)
\]
</p>

<p>
Usando que \(\int_\Omega \Pi = 1\) por ser distribución.
</p>
</div>
</div>

<div id="outline-container-org7757e43" class="outline-6">
<h6 id="org7757e43">Segunda implicación</h6>
<div class="outline-text-6" id="text-org7757e43">
<p>
Por definición:
</p>

<p>
\[
f(\tilde x|\theta)\pi(\theta) = \Pi(\theta)\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta
\]
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org58bc776" class="outline-4">
<h4 id="org58bc776">Caracterización de familias conjugadas</h4>
<div class="outline-text-4" id="text-org58bc776">
<p>
Una familia de distribuciones a priori \({\cal F}\) es conjugada respecto a \({\cal P}\) ssi
el producto de cualesquiera dos distribuciones de ambas familias vuelve
a ser una distribución de la familia de distribuciones a priori, salvo
alguna constante.
</p>

<p>
\[\forall f \in {\cal P}, \pi \in{\cal F}: \exists k:\quad 
kf(x|\theta)\pi(\theta) \in {\cal F}\]
</p>
</div>
</div>

<div id="outline-container-orge4c5ede" class="outline-4">
<h4 id="orge4c5ede">Ejemplos de familias conjugadas</h4>
<div class="outline-text-4" id="text-orge4c5ede">
</div><div id="outline-container-orgb195d3e" class="outline-5">
<h5 id="orgb195d3e">Beta para Bernoulli</h5>
<div class="outline-text-5" id="text-orgb195d3e">
<p>
La familia de distribuciones Beta es una familia conjugada para:
</p>

<ul class="org-ul">
<li>distribuciones de Bernoulli.</li>
<li>distribuciones binomiales.</li>
<li>distribuciones binomiales negativas.</li>
</ul>

<p>
Se tiene:
</p>

<ul class="org-ul">
<li>\(X \leadsto B(n,\theta)\)</li>
<li>\(\theta \leadsto \beta(p,q)\)</li>
<li>\(\theta|x \leadsto \beta(x+p,n-x+q)\)</li>
</ul>
</div>
</div>

<div id="outline-container-orge6b7b69" class="outline-5">
<h5 id="orge6b7b69">Gamma para Poisson</h5>
<div class="outline-text-5" id="text-orge6b7b69">
<p>
La familia de distribuciones Gamma es una familia conjugada para
distribuciones de Poisson.
</p>

<p>
Se tiene:
</p>

<ul class="org-ul">
<li>\(X\leadsto Poi(\theta)\)</li>
<li>\(\theta \leadsto \Gamma(\alpha,\beta)\)</li>
<li>\(\theta|\tilde x \leadsto \Gamma(\sum x_i + \alpha, n + \beta)\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgf665912" class="outline-5">
<h5 id="orgf665912">Normales para normales con varianza conocida</h5>
<div class="outline-text-5" id="text-orgf665912">
<p>
La familia de distribuciones normales es conjugada para las 
distribuciones normales de varianza conocida.
</p>

<p>
Se tiene:
</p>

<ul class="org-ul">
<li>\(X \leadsto {\cal N}(\mu,\sigma^2)\)</li>
<li>\(\mu \leadsto {\cal N}(\eta,\tau)\)</li>
<li>\(\mu|\tilde x \leadsto {\cal N}\left(\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}, \frac{\sigma^2\tau^2}{n\tau^2+\sigma^2}\right)\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgee683fb" class="outline-5">
<h5 id="orgee683fb">Dirichlet para multinomiales</h5>
<div class="outline-text-5" id="text-orgee683fb">
<p>
La familia de distribuciones de Dirichlet es conjugada para la
familia de distribuciones multinomiales.
</p>

<p>
Se tiene:
</p>

<ul class="org-ul">
<li>\(X_1,\dots,X_n \leadsto Multi(\theta_1,\dots,\theta_k)\)</li>
<li>\(\theta_1,\dots,\theta_k \leadsto Dir(\alpha_1,\dots,\alpha_k)\)</li>
<li>\(\theta_1,\dots,\theta_n|x_1,\dots,x_n \leadsto Dir(x_1+\alpha_1,\dots,x_k+\alpha_k)\)</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org5193363" class="outline-3">
<h3 id="org5193363">8.4. Distribuciones objetivas</h3>
<div class="outline-text-3" id="text-org5193363">
</div><div id="outline-container-orgfb931c0" class="outline-4">
<h4 id="orgfb931c0">Distribución de Jeffreys</h4>
<div class="outline-text-4" id="text-orgfb931c0">
<p>
Para una familia \(\{f(x|\theta) \mid \theta\in\Theta\}\), la distribución de Jeffreys se define 
como:
</p>

<p>
\[\pi^J(\theta) \propto \sqrt{{\cal I}_X(\theta)}\]
</p>

<p>
para la información de Fisher.
</p>
</div>
</div>
</div>

<div id="outline-container-org8377e61" class="outline-3">
<h3 id="org8377e61">8.5. Convergencia de distribuciones a posteriori</h3>
<div class="outline-text-3" id="text-org8377e61">
</div><div id="outline-container-org46f7df0" class="outline-4">
<h4 id="org46f7df0">Convergencia en un espacio paramétrico discreto</h4>
<div class="outline-text-4" id="text-org46f7df0">
<p>
Sea \(\Theta = \{\theta_1,\dots,\theta_k\}\), cuando el tamaño de la muestra diverge, la distribución
a posteriori degenera en \(\theta_0\), el valor verdadero del parámetro.
</p>

<p>
\[
\pi(\theta\mid X_1,\dots,X_n) 
\overset{P_{\theta_0}}{\underset{n \to \infty}\longrightarrow} \theta_0
\]
</p>

<p>
Nótese que los \(\theta_i\) deben generar distribuciones distintas para aplicar
este resultado.
</p>
</div>

<div id="outline-container-org6613476" class="outline-5">
<h5 id="org6613476">Demostración</h5>
<div class="outline-text-5" id="text-org6613476">
<p>
Dada una distribución a priori \(\pi\), llamamos \(\pi(\theta_j) = p_j \in [0,1]\). Llamamos
\(\theta_t\) al verdadero parámetro, y tomamos \(X_1,\dots,X_n\) con la distribución
dada por \(f(x|\theta_t)\).
</p>

<p>
\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n f(X_j|\theta_i)}
{\displaystyle \sum_{r=1}^k\left(\prod_{j=1}^n f(X_j|\theta_r) \right) p_r}
\]
</p>

<p>
Si multiplicamos por \(\prod_{j=1}^n f(X_j|\theta_t)\) tenemos:
</p>

<p>
\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}}
{\displaystyle \sum_{r=1}^k\left(
\prod_{j=1}^n \frac{f(X_j|\theta_r)}{f(X_j|\theta_t)}
\right) p_r}
\]
</p>

<p>
Tomando logaritmos estudiamos las variables aleatorias \(Z_j = \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}\),
que son i.i.d. y por la Ley fuerte de los grandes números, tenemos que
converge casi seguramente respecto a la probabilidad que define \(\theta_t\):
</p>

<p>
\[
\frac{1}{n}\sum_{j=1}^n \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
\mathbb{E}\left[\log \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)} \right]
\]
</p>

<p>
Que además sabemos (no trivialmente) que es negativo. Tenemos entonces
por continuidad del logaritmo que:
</p>

<p>
\[
\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
0
\]
</p>

<p>
Aplicando esto a la probabilidad a posteriori tenemos que sólo converge
a uno en el valor \(\theta_t\) y converge a cero en todos los demás.
</p>
</div>
</div>
</div>

<div id="outline-container-orgf7fd4ab" class="outline-4">
<h4 id="orgf7fd4ab">Nota: Influencia de la distribución a priori</h4>
<div class="outline-text-4" id="text-orgf7fd4ab">
<p>
Nótese que la distribución a priori ha sido independiente de la 
convergencia a la distribución a posteriori degenerada.
</p>
</div>
</div>

<div id="outline-container-orgc70b50f" class="outline-4">
<h4 id="orgc70b50f">Nota: Estimadores bayesianos</h4>
<div class="outline-text-4" id="text-orgc70b50f">
<p>
Los modelos bayesianos asignan probabilidad 1 a la hipótesis correcta
cuando el tamaño de la muestra diverge.
</p>
</div>
</div>
</div>

<div id="outline-container-org3d8faaf" class="outline-3">
<h3 id="org3d8faaf">8.6. Test de hipóesis bayesianos</h3>
<div class="outline-text-3" id="text-org3d8faaf">
</div><div id="outline-container-org7421314" class="outline-4">
<h4 id="org7421314">Probabilidad a posteriori de un modelo</h4>
<div class="outline-text-4" id="text-org7421314">
<p>
Dados dos modelos \(M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}\) y \(M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}\),
la probabilidad de que se cumpla el primero condicionada a una muestra es:
</p>

<p>
\[
\pi(M_1|x) = \frac{\pi(M_1)m(x|M_1)}{\pi(M_1)m(x|M_1) + \pi(M_2)m(x|M_2)}
\]
</p>
</div>

<div id="outline-container-org6d0be88" class="outline-5">
<h5 id="org6d0be88">Modelos</h5>
<div class="outline-text-5" id="text-org6d0be88">
<p>
Cada modelo \(M : \{f(x|\theta,M), \pi(\theta|M), \pi(M)\}\) viene dado por:
</p>

<ol class="org-ol">
<li>Una función de distribución condicionada a cada parámetro \(\theta\).</li>
<li>Una probabilidad para cada parámetro, condicionada al modelo.</li>
<li>Probabilidad de que se cumpla el modelo.</li>
</ol>

<p>
Necesitamos \(\pi(M_1)+\pi(M_2) = 1\) en el caso de comparar esos dos modelos.
</p>
</div>
</div>
</div>

<div id="outline-container-org96c3d94" class="outline-4">
<h4 id="org96c3d94">Factor de Bayes</h4>
<div class="outline-text-4" id="text-org96c3d94">
<p>
Dados dos modelos \(M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}\) y \(M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}\),
Definimos el factor de Bayes como:
</p>

<p>
\[
B_{21}(x) = \frac{m(x|M_2)}{m(x|M_1)}
\]
</p>

<p>
cociente entre distribuciones marginales.
</p>

<p>
Cuanto más alto es, más baja es la probabilidad a posteriori del modelo \(M_1\).
</p>
</div>
</div>

<div id="outline-container-orgd979158" class="outline-4">
<h4 id="orgd979158">Método de Leamer: motivación</h4>
<div class="outline-text-4" id="text-orgd979158">
<p>
Si usamos distribuciones impropias para realizar tests de hipótesis y
las multiplicamos por coeficientes para que sean integrables, el factor
de Bayes se vería afectado arbitrariamente por estos coeficientes.
</p>
</div>
</div>

<div id="outline-container-org2f3a474" class="outline-4">
<h4 id="org2f3a474">Método de Leamer: muestras de entrenamiento</h4>
<div class="outline-text-4" id="text-org2f3a474">
<p>
Una <b>muestra de entrenamiento</b> \(\tilde x_1 \subset \tilde x\) es una sublista de la muestra original.
Se llama <b>propia</b> si \(0 < m(\tilde x_1 | M) < \infty\). Se llama <b>minimal</b> si es propia y
ninguna sublista suya lo es.
</p>
</div>
</div>

<div id="outline-container-org7f5ce1a" class="outline-4">
<h4 id="org7f5ce1a">Método de Leamer</h4>
<div class="outline-text-4" id="text-org7f5ce1a">
<p>
Dada una muestra de entrenamiento, sabemos que \(\pi(\theta|M)f(\tilde x_1|\theta,M)\)
integrará y podremos usarlo como distribución a priori.
</p>

<p>
Interesa utilizar una muestra minimal para que se pierdan el menor
número de elementos en la muestra para el test de hipótesis.
</p>
</div>
</div>
</div>

<div id="outline-container-org5da64c0" class="outline-3">
<h3 id="org5da64c0"><span class="todo TODO">TODO</span> 8.7. Probabilidades subjetivas</h3>
</div>
</div>
<div id="outline-container-org0d93f93" class="outline-2">
<h2 id="org0d93f93">Ejercicios</h2>
<div class="outline-text-2" id="text-org0d93f93">
</div><div id="outline-container-orgcc13569" class="outline-3">
<h3 id="orgcc13569">Tema 1. Introducción a la inferencia estadística</h3>
<div class="outline-text-3" id="text-orgcc13569">
</div><div id="outline-container-orge5db0bd" class="outline-4">
<h4 id="orge5db0bd">Ejercicio 1</h4>
<div class="outline-text-4" id="text-orge5db0bd">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de una variable \(X\). 
Dar el espacio muestral y calcular la función masa de probabilidad 
de \((X_1,\dots,X_n)\) en cada uno de los siguientes casos:
</p>

<ol class="org-ol">
<li>\(X \longrightarrow \{{\cal B}(k_0,p); p \in (0,1)\}\) binomial</li>
<li>\(X \longrightarrow \{{\cal P}(\lambda); \lambda\in\mathbb{R}^+\}\) Poisson</li>
</ol>

</div>
</div>

<div id="outline-container-orgdab2a2c" class="outline-5">
<h5 id="orgdab2a2c">Punto 1</h5>
<div class="outline-text-5" id="text-orgdab2a2c">
<p>
El espacio muestral es \(\{0,1,\dots,k_0\}^n\), una palabra \(k_0\text{-aria}\) de \(n\) letras. 
Usando independencia:
</p>

<p>
\[P(x_1,\dots,x_n) = \prod P(x_i) 
= \prod_{i=1}^n \left({k_0 \choose x_i} p^{x_i}(1-p)^{k_0-x_i} \right)\]
</p>
</div>
</div>
<div id="outline-container-org90c3768" class="outline-5">
<h5 id="org90c3768">Punto 2</h5>
<div class="outline-text-5" id="text-org90c3768">
<p>
El espacio muestral es \(\mathbb{N}^n\), palabras en los naturales.
Usando independencia:
</p>

<p>
\[P(x_1,\dots,x_n) = \prod P(x_i) = \prod_{i=0}^n e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgdb67317" class="outline-4">
<h4 id="orgdb67317">Ejercicio 2</h4>
<div class="outline-text-4" id="text-orgdb67317">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de una variable \(X\). Dar el espacio
muestral y calcular la función masa de probabilidad de \((X_1,\dots,X_n)\) en cada uno de
los siguientes casos:
</p>

<ol class="org-ol">
<li>\(X \longrightarrow \{U(a,b); a,b\in\mathbb{R}; a < b\}\) uniforme</li>
<li>\(X\longrightarrow \{{\cal N}(\mu,\sigma^2)\}\) normal</li>
</ol>

</div>
</div>
<div id="outline-container-org3a885f9" class="outline-5">
<h5 id="org3a885f9">Punto 1</h5>
<div class="outline-text-5" id="text-org3a885f9">
<p>
El espacio muestral aquí es \([a,b]^n\), donde por independencia tengo como función
de densidad:
</p>

<p>
\[f(x_1,\dots,x_n) = \prod f(x_i) = \left(\frac{1}{b-a}\right)^n\]
</p>
</div>
</div>

<div id="outline-container-orgc600652" class="outline-5">
<h5 id="orgc600652">Punto 2</h5>
<div class="outline-text-5" id="text-orgc600652">
<p>
El espacio muestral es \(\mathbb{R}^n\), siendo la función de densidad:
</p>

<p>
\[f(x_1,\dots,x_n) = 
\prod_{i=0}^n \frac{1}{\sigma\sqrt{2\pi}} 
e^{-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2}\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgd984552" class="outline-4">
<h4 id="orgd984552">Ejercicio 4</h4>
<div class="outline-text-4" id="text-orgd984552">
<div class="statement">
<p>
Se dispone de una muestra aleatoria simple de tamaño 40 de una
distribución exponencial de media 3, ¿cuál es la probabilidad de que
los valores de la función de distribución muestral y la teórica, en
\(x=1\), difieran menos de 0.01?  Aproximadamente, ¿cuál debe ser el
tamaño muestral para que dicha probabilidad sea como mínimo 0.98?
</p>

</div>
</div>

<div id="outline-container-org2d7ee8c" class="outline-5">
<h5 id="org2d7ee8c">Probabilidad de que difieran</h5>
<div class="outline-text-5" id="text-org2d7ee8c">
<p>
Tenemos que \(nF^\ast_X(1) \leadsto {\cal B}(n,F(1))\), luego podemos calcular la probabilidad
como:
</p>

<p>
\[\begin{aligned}
P\Big( F(1) - 0.01 < F^\ast(1) < F(1) + 0.01 \Big) = \\
P\Big( 10.93 < 40F^\ast(1) < 11.73 \Big) = \\
P\Big(10 < 40F^\ast(1) < 12) =\\
P\Big(11 = 40F^\ast(1)) =\\
{40 \choose 11} F(1)^{11}(1-F(1))^{40-11} \approx\\
0.1318
\end{aligned}\]
</p>

<p>
Sabiendo que \(F(1) = 1 - e^{-1/3} \approx 0.283\) y que \(F^\ast\) es variable discreta.
</p>
</div>

<div id="outline-container-org385fe52" class="outline-6">
<h6 id="org385fe52">Cálculos</h6>
<div class="outline-text-6" id="text-org385fe52">
<div class="org-src-container">
<pre class="src src-R">f1 = 1-exp(-1/3)
f1
n = 40
n*(f1 + 0.01)
n*(f1 - 0.01)
dbinom(11,n,0.3)
</pre>
</div>

<pre class="example">
[1] 0.2834687
[1] 11.73875
[1] 10.93875
[1] 0.1318644
</pre>
</div>
</div>
</div>

<div id="outline-container-org58ca85e" class="outline-5">
<h5 id="org58ca85e"><span class="todo TODO">TODO</span> Tamaño muestral</h5>
<div class="outline-text-5" id="text-org58ca85e">
<p>
Llamamos \(\sqrt{\frac{1}{n}F(1)(1-F(1))} = \sigma_n\), y esta vez aplicamos el Teorema 
Central del Límite para tener que:
</p>

<p>
\[\frac{F^\ast(1) - F(1)}{\sigma_n} \leadsto {\cal N}(0,1)\]
</p>

<p>
Lo que buscamos es que:
</p>

<p>
\[\begin{aligned}
P\Big( -0.01 < F^\ast(1)-F(1) < 0.01 \Big) > 0.98 \\
P\Big( \frac{-0.01}{\sigma_n} < \frac{F^\ast(1)-F(1)}{\sigma_n} < \frac{0.01}{\sigma_n} \Big) > 0.98 \\
\end{aligned}\]
</p>

<p>
Dada \(\Phi\) función de distribución de la normal tipificada, 
tenemos que:
</p>

<p>
\[\begin{aligned}
\Phi\left(\frac{0.01}{\sigma_n}\right) -
\Phi\left(\frac{-0.01}{\sigma_n}\right) > 0.98 \\
2\Phi\left(\frac{0.01}{\sigma_n}\right) > 0.98 +1 \\
1 -\Phi\left(\frac{0.01}{\sigma_n}\right) < 0.01
\end{aligned}\]
</p>

<p>
Usando la tabla de la normal, tenemos:
</p>

<p>
\[\frac{0.01}{\sigma_n} \geq 2.33\]
</p>

<p>
Desde donde calculamos:
</p>

<p>
\[n = 10978\]
</p>
</div>

<div id="outline-container-orgfd2d0ba" class="outline-6">
<h6 id="orgfd2d0ba">Cálculos</h6>
<div class="outline-text-6" id="text-orgfd2d0ba">
<div class="org-src-container">
<pre class="src src-R"><span style="color: #5F7F5F;"># </span><span style="color: #99968b;">Lookup on the normal distribution table</span>
qnorm(1-0.01)
</pre>
</div>

<pre class="example">
[1] 2.326348
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org700feb0" class="outline-4">
<h4 id="org700feb0">Ejercicio 7</h4>
<div class="outline-text-4" id="text-org700feb0">
<div class="statement">
<p>
Dada una muestra aleatoria simple \((X_1,\dots,X_n)\) de una variable \(X\), obtener
la distribución en el muestreo de \(\overline{X}\) en los casos:
</p>

<ol class="org-ol">
<li>\(X \leadsto B(1,p)\)</li>
<li>\(X\leadsto P(\lambda)\)</li>
<li>\(X \leadsto exp(\lambda)\)</li>
</ol>

</div>
</div>

<div id="outline-container-org67893fe" class="outline-5">
<h5 id="org67893fe">Punto 1</h5>
<div class="outline-text-5" id="text-org67893fe">
<p>
Por independencia y suma de binomiales:
</p>

<p>
\[
\overline{X} \leadsto \frac{1}{n}B(n,p)
\]
</p>

<p>
Nótese que deja de ser una binomial.
</p>
</div>
</div>

<div id="outline-container-org6ba7a28" class="outline-5">
<h5 id="org6ba7a28">Punto 2</h5>
<div class="outline-text-5" id="text-org6ba7a28">
<p>
Por independencia y suma de Poisson:
</p>

<p>
\[
\overline{X} \leadsto \frac{1}{n}Poi(n\lambda)
\]
</p>
</div>
</div>

<div id="outline-container-org8b37b8f" class="outline-5">
<h5 id="org8b37b8f">Punto 3</h5>
<div class="outline-text-5" id="text-org8b37b8f">
<p>
Por independencia y suma de Gammas:
</p>

<p>
\[
\overline{X} = \frac{1}{n}\Gamma(n,\lambda)
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org4983dd2" class="outline-4">
<h4 id="org4983dd2">Ejercicio 10</h4>
<div class="outline-text-4" id="text-org4983dd2">
<p>
Desde la distribución de la estimación de la normal.
</p>
</div>
</div>
</div>

<div id="outline-container-org29be098" class="outline-3">
<h3 id="org29be098">Tema 2. Distribuciones en el muestreo de poblaciones normales</h3>
<div class="outline-text-3" id="text-org29be098">
</div><div id="outline-container-orgb04c660" class="outline-4">
<h4 id="orgb04c660">Ejercicio 1</h4>
<div class="outline-text-4" id="text-orgb04c660">
<div class="statement">
<p>
Se toma una muestra aleatoria simple de tamaño \(5\) de una variable aleatoria
con distribución \({\cal N}(2.5, 36)\). Calcular:
</p>

<ol class="org-ol">
<li>Probabilidad de que la cuasivarianza muestral esté comprendida entre
\(1.863\) y \(2.674\).</li>
<li>Probabilidad de que la media muestral esté comprendida entre \(1.3\) y \(3.5\),
supuesto que la cuasivarianza muestral está entre \(30\) y \(40\).</li>
</ol>

</div>
</div>

<div id="outline-container-org11f942a" class="outline-5">
<h5 id="org11f942a">Probabilidad de la Cuasivarianza</h5>
<div class="outline-text-5" id="text-org11f942a">
<p>
Buscamos:
</p>

<p>
\[\begin{aligned}
P\Big(1.863 \leq S^2 \leq 2.674 \Big) &\leq 
P\Big(\frac{n-1}{\sigma^2}1.863 \leq \frac{n-1}{\sigma^2}S^2 \leq \frac{n-1}{\sigma^2}2.674 \Big)
\end{aligned}\]
</p>

<p>
Y sabiendo que \(\frac{n-1}{\sigma^2}S^2 \leadsto \chi^2(n-1)\), sea \(\phi\) la función de distribución para
tener que la probabilidad será:
</p>

<p>
\[\phi\left(\frac{n-1}{\sigma^2}2.674\right) - 
\phi\left(\frac{n-1}{\sigma^2}1.863\right) =
0.010 - 0.005 = 0.005
\]
</p>

<p>
Consultando la tabla de Poisson.
</p>
</div>

<div id="outline-container-org5fb849c" class="outline-6">
<h6 id="org5fb849c">Cálculos</h6>
<div class="outline-text-6" id="text-org5fb849c">
<div class="org-src-container">
<pre class="src src-R">n = 5
s2 = 36
pchisq((n-1)/s2 * 2.674, df=n-1) - pchisq((n-1)/s2 * 1.863, df=n-1)
</pre>
</div>

<pre class="example">
0.00499959549303851
</pre>
</div>
</div>
</div>

<div id="outline-container-org96bf7b9" class="outline-5">
<h5 id="org96bf7b9">Probabilidad de la Media Muestral</h5>
<div class="outline-text-5" id="text-org96bf7b9">
<p>
La suposición de que la cuasivarianza muestral está entre 30 y 40 no
aporta nada porque la media y ella son estadísticos independientes por
el Lema de Fisher.
</p>

<p>
Buscamos:
</p>

<p>
\[P\Big(  
1.3 \leq \overline{X} \leq 1.5
\Big) = 
P\Big(  
\frac{1.3 - \mu}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq 
\frac{1.5 - \mu}{\sigma/\sqrt{n}}
\Big)
\]
</p>

<p>
Y como \(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)\), siendo \(\phi\) la distribución de la normal, calculamos
la probabilidad usando las tablas de la normal.
</p>

<p>
\[
\phi(-0.3726) - \phi(-0.4472) = 0.3557 - 0.3300 = 0.0257
\]
</p>
</div>

<div id="outline-container-orgab5c2e2" class="outline-6">
<h6 id="orgab5c2e2">Cálculos</h6>
<div class="outline-text-6" id="text-orgab5c2e2">
<div class="org-src-container">
<pre class="src src-R">n = 5
m = 2.5
s2 = 36
s = sqrt(36)
pnorm((1.5-m)/(s/sqrt(n))) - pnorm((1.3-m)/(s/sqrt(n)))
</pre>
</div>

<pre class="example">
0.0273336344978247
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org27c3641" class="outline-4">
<h4 id="org27c3641">Ejercicio 3</h4>
<div class="outline-text-4" id="text-org27c3641">
<div class="statement">
<p>
¿De qué tamaño mínimo habría que seleccionar una muestra de una variable
con distribución normal \({\cal N}(\mu,4)\) para poder afirmar, con probabilidad mayor
que \(0.9\), que la media muestral diferirá de la poblacional menos de \(0.1\)?
</p>

</div>

<p>
Buscamos:
</p>

<p>
\[P\Big(
\frac{-0.1}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq
\frac{0.1}{\sigma/\sqrt{n}}
\Big)\]
</p>

<p>
Sabiendo que \(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)\), calculamos:
</p>

<p>
\[\begin{aligned}
1 - 2\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\geq 0.9 \\
\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\leq 0.05 \\
\end{aligned}\]
</p>

<p>
Usando la tabla de la distribución, tenemos que:
</p>

<p>
\[\frac{0.1}{2/\sqrt{n}} = 1.65\]
</p>

<p>
Desde donde: \(n = 1089\).
</p>
</div>

<div id="outline-container-org4951602" class="outline-5">
<h5 id="org4951602">Cálculos</h5>
<div class="outline-text-5" id="text-org4951602">
<div class="org-src-container">
<pre class="src src-R">s2 = 4
s = sqrt(s2)
q = qnorm(1-0.05)
((s*q)/0.1)^2
</pre>
</div>

<pre class="example">
1082.21738163816
</pre>
</div>
</div>
</div>

<div id="outline-container-org3d54b72" class="outline-4">
<h4 id="org3d54b72">Ejercicio 7</h4>
<div class="outline-text-4" id="text-org3d54b72">
<p>
Comprobando sumas se llega a que siguen una Poisson.
</p>
</div>
</div>
</div>

<div id="outline-container-org1842b61" class="outline-3">
<h3 id="org1842b61">Tema 3. Suficiencia y complitud</h3>
<div class="outline-text-3" id="text-org1842b61">
</div><div id="outline-container-org70bd7e7" class="outline-4">
<h4 id="org70bd7e7">Ejercicio 1</h4>
<div class="outline-text-4" id="text-org70bd7e7">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de una variable \(X \leadsto B(k,p)\)
y sea \(T(X_1,\dots,X_n) = \sum^n_{i=1} X_i\). Probar, usando la definición y aplicando el
teorema de factorización, que \(T\) es suficiente para \(p\).
</p>

</div>
</div>

<div id="outline-container-org92a1a77" class="outline-5">
<h5 id="org92a1a77">Usando la definición</h5>
<div class="outline-text-5" id="text-org92a1a77">
<p>
Llamamos \(S = \sum^n_{i=1} X_i\). Veremos que \(P(x_1,\dots,x_n\mid S)\) no depende de \(p\).
En el caso \(x_1+\dots+x_n \neq S\), la probabilidad es \(0\) y claramente 
independiente de \(p\). En el otro caso, tenemos:
</p>

<p>
\[\begin{aligned}
P(x_1,\dots,x_n\mid S) = 
\frac{P(x_1,\dots,x_n)}{P(S)} =
\frac
{\prod {k \choose x_i} p^{x_i}(1-p)^{k-x_i}}
{{nk \choose S} p^{S}(1-p)^{nk-S}} =
\frac
{\prod {k \choose x_i}}
{{nk \choose S}}
\end{aligned}\]
</p>

<p>
Que no depende de \(p\). Hemos usado que \(S = \sum_{i=1}^n X_i \leadsto {\cal B}(nk,p)\) para 
calcular la probabilidad de que valga un valor concreto.
</p>
</div>
</div>

<div id="outline-container-org0a803e6" class="outline-5">
<h5 id="org0a803e6">Usando el teorema de factorización</h5>
<div class="outline-text-5" id="text-org0a803e6">
<p>
Factorizamos la función de densidad como:
</p>

<p>
\[
f(x_1,\dots,x_n) =
\prod_{i=1}^n {k \choose x_i} p^{x_i}(1-p)^{k-x_i} =
\left(p^{\sum x_i}(1-p)^{nk - \sum x_i}\right)
\left(\prod^{k}_{i=1} {k\choose x_i}\right)
\]
</p>

<p>
El primer factor sólo depende de los datos a través de la suma y el
segundo factor no depende de la probabilidad.
</p>
</div>
</div>
</div>

<div id="outline-container-org672fa9a" class="outline-4">
<h4 id="org672fa9a">Ejercicio 3</h4>
<div class="outline-text-4" id="text-org672fa9a">
<div class="statement">
<p>
Sea \((X_1,X_2,X_3)\) una muestra aleatoria simple de una variable \(X \leadsto B(1,p)\).
Probar que el estadístico \(X_1+2X_2+3X_3\) no es suficiente.
</p>

</div>

<p>
Llamamos \(S=X_1+2X_2+3X_3\). Vamos a calcular \(P(1,1,0\mid S=3)\) y 
comprobaremos que depende de \(p\). Nótese que puede llegarse a \(S=3\) de dos
formas, como \(1+2+0\) y como \(0+0+3\).
</p>

<p>
\[
P(1,1,0\mid S=3) =
\frac{P(1,1,0)}{P(S=3)} =
\frac{p^2(1-p)}{p^2(1-p)+p(1-p)^2}=
p
\]
</p>
</div>
</div>
</div>

<div id="outline-container-org3d10af5" class="outline-3">
<h3 id="org3d10af5">Tema 4. Estimación puntual. Métodos de estimación</h3>
<div class="outline-text-3" id="text-org3d10af5">
</div><div id="outline-container-orgd22c451" class="outline-4">
<h4 id="orgd22c451">Ejercicio 2</h4>
<div class="outline-text-4" id="text-orgd22c451">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de \(X \leadsto B(1,p)\) y sea 
\(T = \sum_{i=1}^n X_i\).
</p>

<ol class="org-ol">
<li><p>
Probar que si \(k \in \mathbb{N}\) y \(k\leq n\) el estadístico
</p>

<p>
\[\frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]
</p>

<p>
es un estimador insesgado de \(p^k\). ¿Es este estimador el UMVUE?
</p></li>
<li>Probar que si \(k>n\), no existe ningún estimador insesgado para \(p^k\).</li>
<li>¿Puede afirmarse que \(\frac{T}{n}\left(1-\frac{T}{n}\right)^2\) es insesgado para \(p(1-p)^2\)?</li>
</ol>

</div>
</div>

<div id="outline-container-orgae6a776" class="outline-5">
<h5 id="orgae6a776">Punto 1. Es insesgado.</h5>
<div class="outline-text-5" id="text-orgae6a776">
<p>
Llamamos \(M = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\). Comprobamos que es insesgado calculando 
la esperanza. Dividimos entre los casos \(i\leq k\) que son nulos y los demás.
Usamos \(P(T = i) = {n \choose i}p^i(1-p)^{n-i}\).
</p>

<p>
\[\begin{aligned}
\mathbb{E}[M] &=
\sum_{i=1}^n {n \choose i} p^i (1-p)^{n-i}
\frac{i(i-1)\dots(i-k+1)}{n(n-1)\dots(n-k+1)} \\&=
\sum_{i=1}^n p^i (1-p)^{n-i}
\frac{(n-k)!}{(i-k)!((n-k)-(i-k))!} \\&=
p^k \sum_{i=1}^n p^{i-k} (1-p)^{(n-k)-(i-k)}
{n-k \choose i-k} \\&= p^k
\end{aligned}\]
</p>

<p>
Usando binomio de Newton en el último paso.
</p>
</div>
</div>

<div id="outline-container-org116c806" class="outline-5">
<h5 id="org116c806">Punto 1. Es el UMVUE.</h5>
<div class="outline-text-5" id="text-org116c806">
<p>
Usaremos el teorema de Lehmann-Scheffé, sabiendo que \(M\) es un estimador
insesgado de \(p^k\); y que \(T\) es suficiente y completo para \(p\).
</p>

<p>
Para ver que \(T\) es completo, calculamos:
</p>

<p>
\[\begin{aligned}
\mathbb{E}[g(T)] 
&= \sum_{t=0}^n g(t) {n \choose t} p^t(1-p)^{n-t} \\
&= (1-p)^n \sum_{t=0}^n g(t) {n \choose t} \left(\frac{p}{1-p}\right)^t \\
\end{aligned}\]
</p>

<p>
Para que se anule siempre, debe anularse el polinomio
\(\sum g(t) {n \choose t}r^t\) para \(r \in \mathbb{R}^+\), lo que implica \(g(t) = 0\).
</p>

<p>
Pero ahora, por Lehmann-Scheffé, tenemos que el UMVUE será:
</p>

<p>
\[\mathbb{E}[M\mid T] = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]
</p>

<p>
Que es un UMVUE.
</p>
</div>
</div>

<div id="outline-container-org7c95781" class="outline-5">
<h5 id="org7c95781">Punto 2.</h5>
<div class="outline-text-5" id="text-org7c95781">
<p>
Supongamos un estimador \(Q\) que fuera insesgado para \(p^q\). Tendríamos:
</p>

<p>
\[\mathbb{E}[Q(X)] = p^q\]
</p>

<p>
Es decir, llamando \(R(T) = \sum_{\sum x_i = T} Q(X)\),
</p>

<p>
\[
\sum^n {n \choose k} R(t) p^t(1-p)^{n-t} = p^q
\]
</p>

<p>
Pero esto nos daría un polinomio de grado \(q\) sobre \(p\), que no puede ser 
nulo.
</p>
</div>
</div>

<div id="outline-container-org47c0036" class="outline-5">
<h5 id="org47c0036"><span class="todo TODO">TODO</span> Punto 3.</h5>
<div class="outline-text-5" id="text-org47c0036">
<p>
No. Si calculamos la esperanza usando linealidad obtenemos algo distinto.
</p>

<div class="org-src-container">
<pre class="src src-sage"><span style="color: #cae682;">n</span>,<span style="color: #cae682;">p</span> = var(<span style="color: #95e454;">'n p'</span>)
<span style="color: #cae682;">m1</span> = n*p
<span style="color: #cae682;">m2</span> = m1*(1-p+m1)
<span style="color: #cae682;">m3</span> = m1*(1-3*p+3*m1+2*p^2 - 3*n*p^2 + n^2*p^2)
(p - 2*m2/n^2 + m3/n^3).normalize()
</pre>
</div>

<pre class="example">
(n^2*p^3 - 2*n^2*p^2 - 3*n*p^3 + n^2*p + 5*n*p^2 + 2*p^3 - 2*n*p - 3*p^2 + p)/n^2
</pre>
</div>
</div>
</div>

<div id="outline-container-orge395b6d" class="outline-4">
<h4 id="orge395b6d">Ejercicio 3</h4>
<div class="outline-text-4" id="text-orge395b6d">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de una variable
\(X \leadsto \{{\cal P}(\lambda)\mid \lambda > 0\}\). Encontrar, si existe, el UMVUE para \(\lambda^s\), siendo
\(s \in \mathbb{N}\) arbitrario.
</p>

</div>

<p>
Por familia uniparamétrica demostramos \(T = \sum X_i\) suficiente y completo.
Probando llegamos a que \(\frac{1}{n^s}T(T-1)\dots(T-s)\) es insesgado.
</p>
</div>
</div>

<div id="outline-container-org96c2c93" class="outline-4">
<h4 id="org96c2c93">Ejercicio 7</h4>
<div class="outline-text-4" id="text-org96c2c93">
</div><div id="outline-container-org824d803" class="outline-5">
<h5 id="org824d803">Punto 1</h5>
<div class="outline-text-5" id="text-org824d803">
<p>
Se comprueba que es familia exponencial uniparamétrica. Con
\(T(X) = X\) y por tanto con \(\sum_i T(X_i)\) como estadístico suficiente.
Además, es completo porque \(Q(\Theta)\) tiene un abierto en su imagen.
</p>
</div>
</div>

<div id="outline-container-org892b84f" class="outline-5">
<h5 id="org892b84f"><span class="done DONE">DONE</span> Punto 2</h5>
</div>
</div>

<div id="outline-container-org0bafbe6" class="outline-4">
<h4 id="org0bafbe6">Ejercicio 8</h4>
<div class="outline-text-4" id="text-org0bafbe6">
</div><div id="outline-container-orge825c41" class="outline-5">
<h5 id="orge825c41"><span class="done DONE">DONE</span> Punto 1</h5>
</div>
</div>
</div>

<div id="outline-container-orgc2aa37c" class="outline-3">
<h3 id="orgc2aa37c">Tema 5. Intervalos de confianza</h3>
<div class="outline-text-3" id="text-orgc2aa37c">
</div><div id="outline-container-org6eed77b" class="outline-4">
<h4 id="org6eed77b">Ejercicio 1</h4>
<div class="outline-text-4" id="text-org6eed77b">
<p>
El mínimo es \(n=44\).
</p>
</div>
</div>
<div id="outline-container-orge7ed95f" class="outline-4">
<h4 id="orge7ed95f">Ejercicio 2</h4>
<div class="outline-text-4" id="text-orge7ed95f">
</div><div id="outline-container-orgc54a11f" class="outline-5">
<h5 id="orgc54a11f">Primer punto</h5>
<div class="outline-text-5" id="text-orgc54a11f">
<p>
Intervalo de \((170.75, 179.75)\).
</p>
</div>
</div>

<div id="outline-container-orgb903f8a" class="outline-5">
<h5 id="orgb903f8a">Segundo punto</h5>
<div class="outline-text-5" id="text-orgb903f8a">
<p>
Da un \(n \geq 865\).
</p>
</div>
</div>
</div>
<div id="outline-container-orga1c865c" class="outline-4">
<h4 id="orga1c865c">Ejercicio 5</h4>
<div class="outline-text-4" id="text-orga1c865c">
<p>
Se llega a \(t = 2.1788\).
</p>
</div>
</div>

<div id="outline-container-org18b333b" class="outline-4">
<h4 id="org18b333b">Ejercicio 7</h4>
<div class="outline-text-4" id="text-org18b333b">
</div><div id="outline-container-org853fbb9" class="outline-5">
<h5 id="org853fbb9">Primer punto</h5>
<div class="outline-text-5" id="text-org853fbb9">
<p>
Calculamos:
</p>

<ul class="org-ul">
<li>\(\overline{X} = 37.2\)</li>
<li>\(\overline{Y} = 16.88\)</li>
<li>\(S_X^2 = 482.137\)</li>
<li>\(S_Y^2 = 208.517\)</li>
<li>\(n_1 = 6\)</li>
<li>\(n_2 = 5\)</li>
</ul>

<p>
Y tenemos un intervalo de confianza para el cociente de varianzas.
Calculando primero desde las tablas (?), con \(\alpha = 0.95\):
</p>

<ul class="org-ul">
<li>\(F_{1-\alpha/2} =\)</li>
<li>\(F_{\alpha/2} =\)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2d4f2ae" class="outline-4">
<h4 id="org2d4f2ae">Ejercicio 9</h4>
<div class="outline-text-4" id="text-org2d4f2ae">
<p>
Tomamos el estimador:
</p>

<p>
\[
T = \frac{1}{n}\sum_{i=1}^n X_i
\]
</p>

<p>
Si partimos de la desigualdad de Chebyshev con la cota \(c = 1/n\) a la
varianza, llegamos a \(k = 1/\sqrt{n\alpha}\), que nos da el intervalo:
</p>

<p>
\[
\left(
\frac{1}{n} \sum X_i + \frac{1}{\sqrt{n\alpha}}
,\quad
\frac{1}{n} \sum X_i - \frac{1}{\sqrt{n\alpha}}
\right)
\]
</p>

<p>
Con la confianza \(\alpha\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgf721f3a" class="outline-3">
<h3 id="orgf721f3a">Tema 6. Contraste de hipotésis</h3>
<div class="outline-text-3" id="text-orgf721f3a">
</div><div id="outline-container-orgebcd250" class="outline-4">
<h4 id="orgebcd250">Ejercicio 1</h4>
<div class="outline-text-4" id="text-orgebcd250">
<div class="statement">
<p>
Se toma una observación de una variable con distribución de Poisson para
contrastar que la media vale 1 frente a que vale 2.
</p>

<ol class="org-ol">
<li>Construir un test no aleatorizado con nivel de significación 0.05 
para el contraste planteado. Calcular las probabilidades de cometer
error de tipo 1 y de tipo 2, el tamaño y la potencia del test frente
a la hipótesis alternativa.</li>
<li>¿Cómo debe aleatorizarse el test para alcanzar el tamaño 0.05?¿Cuál
es la potencia de este test?</li>
</ol>

</div>

<p>
Usaremos el lema de Neyman-Pearson para crear un test donde el problema
de contraste es: \(H_0 : \lambda = 2\), \(H_1 : \lambda = 1\), para una distribución \(Poi(\lambda)\).
</p>

<p>
\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) \geq kf_0(X) \\
0 & \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]
</p>

<p>
Sabemos que este test tendrá tamaño \(\mathbb{E}_{\lambda=2}[\varphi]\), calculamos:
</p>

<p>
\[f_1(x) = \frac{1}{ex!}\]
</p>

<p>
\[
f_0(x) = \frac{2^x}{e^2x!}
\]
</p>

<p>
La condición \(f_1(X) \geq kf_0(X)\) equivale a:
</p>

<p>
\[
x \leq \log_2 \frac{e}{k}
\]
</p>

<p>
Siendo \(\Phi\) la función de distribución de una Poisson \(Poi(2)\), que es la 
distribución que sigue aquí \(x\), tenemos, consultando las tablas:
</p>

<p>
\[
\mathbb{E}_{\lambda=2}[\varphi] = 
\Phi\left(\log_2\frac{e}{k}\right) = 
0.05
\]
</p>
</div>
</div>
<div id="outline-container-orgb2e7239" class="outline-4">
<h4 id="orgb2e7239">Ejercicio 2</h4>
<div class="outline-text-4" id="text-orgb2e7239">
<div class="statement">
<p>
Una urna contiene 10 bolas, blancas y negras. Para contrastar que el 
número de bolas blancas es 5 frente a que dicho número es 6 ó 7, se
extraen tres bolas con reemplazamiento y se rechaza \(H_0\) sólo si se 
obtienen 2 ó 3 bolas blancas. Calcular el tamaño de este test y la
potencia frente a alternativas.
</p>

</div>
</div>

<div id="outline-container-org093e17b" class="outline-5">
<h5 id="org093e17b">Tamaño del test</h5>
<div class="outline-text-5" id="text-org093e17b">
<p>
Llamamos \(X\) a la variable dada por cada extracción, siendo 1 si es blanca
y 0 si es negra. Vemos que \(X \leadsto B(1,\theta/10)\). Tenemos como hipótesis la
hipótesis nula \(H_0 : \theta = 5\) y la alternativa \(H_1 : \theta \in \{6,7\}\). Nuestro test está
definido por:
</p>

<p>
\[
\varphi(X_1,X_2,X_3) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum X_i = 2,3 \\
0 & \mbox{if } \sum X_i = 0,1
\end{array} 
\right.
\]
</p>

<p>
Calculamos el tamaño sabiendo que \(Z = \sum X_i \leadsto B(3,\theta/10)\):
</p>

<p>
\[
\sup_{\theta = 5} \beta_\varphi(\theta) = E_{\theta=5}[\varphi]
=
P(Z = 2) + P(Z=3) = \frac{1}{2}
\]
</p>
</div>
</div>

<div id="outline-container-orga6f5a94" class="outline-5">
<h5 id="orga6f5a94">Potencia frente alternativas</h5>
<div class="outline-text-5" id="text-orga6f5a94">
<p>
Calculamos:
</p>

<p>
\[\beta_\varphi(6)\]
\[\beta_\varphi(7)\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgb40cbbb" class="outline-4">
<h4 id="orgb40cbbb">Ejercicio 3</h4>
<div class="outline-text-4" id="text-orgb40cbbb">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) una muestra aleatoria simple de una variable aleatoria
con distribución de Poisson de parámetro \(\lambda\). Encontrar el test más potente
de tamaño \(\alpha\) para resolver el problema de contraste:
</p>

<p>
\[
H_0 : \lambda = \lambda_0
\]
\[
H_1 : \lambda = \lambda_1
\]
</p>

<p>
Aplicación: En una centralita telegónica el número de llamadas por minuto
sigue una distribución de Poisson. Si en cinco minutos se han recibido 12
llamadas, ¿puede aceptarse que el número medio de llamadas por minuto es
1.5, frente a que dicho número es 2, al nivel de significación 0.05?
Calcular la potencia del test obtenido.
</p>

</div>
</div>

<div id="outline-container-orga88bd1f" class="outline-5">
<h5 id="orga88bd1f">Desarrollo teórico</h5>
<div class="outline-text-5" id="text-orga88bd1f">
<p>
Por Neyman-Pearson, el test más potente de tamaño \(\alpha\) será de la forma:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } f_1(x_1,\dots,x_n) > kf_0(x_1,\dots,x_n) \\
\gamma & \mbox{if } f_1(x_1,\dots,x_n) = kf_0(x_1,\dots,x_n) \\
0 & \mbox{if } f_1(x_1,\dots,x_n) < kf_0(x_1,\dots,x_n)
\end{array} 
\right.
\]
</p>

<p>
La desigualdad es equivalente a:
</p>

<p>
\[
\frac{e^{-n\lambda_1}\lambda_1^{\sum x_i}}{\prod x_i!} 
> 
k \frac{e^{-n\lambda_0}\lambda_0^{\sum x_i}}{\prod x_i!} 
\]
</p>

<p>
\[
k = \frac
{n(\lambda_0-\lambda_1)-c}
{\log\left(\lambda_0/\lambda_1\right)}
> \sum x_i
\leadsto
Poi(n\lambda_0)
\]
</p>

<p>
Por tanto podemos tomar una distribución de Poisson siendo \(\rho_\alpha\) el valor
que da \(\alpha\) en la función de distribución y tener:
</p>

<p>
\[
c = n(\lambda_0-\lambda_1) + \rho_\alpha\log(\lambda_0/\lambda_1)
\]
</p>

<p>
Tenemos por tanto un test de la forma:
</p>

<p>
\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i < \rho_\alpha \\
0 & \mbox{if } \sum x_i \geq \rho_\alpha
\end{array} 
\right.
\]
</p>

<p>
El tamaño del test entonces será \(\alpha\).
</p>
</div>
</div>

<div id="outline-container-org625e79b" class="outline-5">
<h5 id="org625e79b">Aplicación</h5>
<div class="outline-text-5" id="text-org625e79b">
<p>
En este caso tenemos \(\sum x_i = 12\), para \(n=5\). Para nivel de significación
\(\alpha = 0.05\), tenemos \(P(Poi(10) > \rho_{0.05}) = 0.05\).
</p>

<p>
Tenemos:
</p>

<p>
\[
P(Poi(10) > 14) = 0.05
\]
</p>

<p>
Y como \(12 < 14\), se el test da la hipótesis alternativa.
</p>
</div>
</div>
</div>

<div id="outline-container-orge4679de" class="outline-4">
<h4 id="orge4679de">Ejercicio 4</h4>
<div class="outline-text-4" id="text-orge4679de">
<div class="statement">
<p>
Sea \((X_1,\dots,X_n)\) muestra aleatoria simple de una variable con distribución
\({\cal N}(\mu,\sigma^2_0)\). Deducir el test más potente de tamaño arbitrario para contrastar
hipótesis simples sobre \(\mu\).
</p>

</div>

<p>
Aplicaremos Neyman-Pearson para crear un test con dos hipótesis,
\(H_0 : \mu =\mu_0\), \(H_1:\mu =\mu_1\), de la forma:
</p>

<p>
\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) > kf_0(X) \\
\gamma(X) & \mbox{if } f_1(X) = kf_0(X)\\
0& \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]
</p>

<p>
Que será el de mayor potencia con nivel de significación \(E_{\mu_0}[\varphi]\).
Calculamos la significación sabiendo que la condición \(f_1 > kf_0\) nos
da:
</p>

<p>
\[
f_1(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_1)^2}{2\sigma^2}}
\]
</p>

<p>
\[
f_0(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_0)^2}{2\sigma^2}}
\]
</p>

<p>
Simplificando la condición:
</p>

<p>
\[
\sum x_i > \frac{\log k - (\mu_0^2-\mu_1^2)}{2(\mu_1-\mu_0)} = k'
\]
</p>

<p>
Y usamos la normal con \(P(Z \geq z_\alpha) = \alpha\) para obtener el valor necesario para
\(k'\) si queremos un test de tamaño \(\alpha\):
</p>

<p>
\[
k' = z_\alpha\sigma/\sqtr{n} + \mu_0
\]
</p>

<p>
El test más potente de tamaño \(\alpha\) es entonces:
</p>

<p>
\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i > k' \\
0& \mbox{if } \sum x_i \leq k'
\end{array} 
\right.
\]
</p>
</div>
</div>

<div id="outline-container-orgc1204a3" class="outline-4">
<h4 id="orgc1204a3"><span class="done DONE">DONE</span> Ejercicio 5</h4>
<div class="outline-text-4" id="text-orgc1204a3">
<div class="statement">
<p>
Deducir el test más potente de tamaño \(\alpha\) para contrastar \(H_0:\theta=\theta_0\)
frente a \(H_1:\theta=\theta_1\) y calcular su potencia. ¿Cuál es el test óptimo fijado
un nivel de significación arbitrario?
</p>

</div>
</div>
</div>

<div id="outline-container-orgd508ec6" class="outline-4">
<h4 id="orgd508ec6">Ejercicio 6</h4>
<div class="outline-text-4" id="text-orgd508ec6">
<div class="statement">
<p>
Deducir el test más potente de tamaño arbitrario para contrastar \(H_0 : \theta = \theta_0\)
frente a \(H_1 : \theta = \theta_1\), basándose en una muestra de tamaño \(n\) de una variable
aleatoria con función de densidad
</p>


<p>
\[
f_\theta(x) = \frac{\theta}{x^2}, \quad x > \theta
\]
</p>

<p>
Deducir el test óptimo para un nivel de significación arbitrario.
</p>

</div>
</div>
</div>


<div id="outline-container-orga0dbc18" class="outline-4">
<h4 id="orga0dbc18">Ejercicio 8</h4>
<div class="outline-text-4" id="text-orga0dbc18">
<p>
Minimizando y calculando se llega a:
</p>

<p>
\[
\lambda(X) =
\frac{1}{\theta_0^n} e^{(1-1/\theta_0)\sum x_i}
\]
</p>
</div>
</div>

<div id="outline-container-orgff46d03" class="outline-4">
<h4 id="orgff46d03">Ejercicio 9</h4>
<div class="outline-text-4" id="text-orgff46d03">
<div class="statement">
<p>
En base a una observación \(X \leadsto B(n,p)\), deducir el test de razón de 
verosimilitudes para contrastar la hipótesis de que el parámetro \(p\) no
supera un determinado valor, \(p_0\).
</p>

</div>
</div>
</div>

<div id="outline-container-org0374612" class="outline-4">
<h4 id="org0374612">Ejercicio 10</h4>
<div class="outline-text-4" id="text-org0374612">
<div class="statement">
<p>
Sea \(X\) variable con función de densidad
</p>

<p>
\[
f_\theta(x) = \theta x^{\theta-1},\quad 0<x<1
\]
</p>

<p>
Basándose en una observación de \(X\), deducir el test de razón de 
verosimilitudes de tamaño arbitrario para contrastar:
</p>

<p>
\[
H_0 : \theta \leq \theta_0
\]
\[
H_1 : \theta > \theta_0
\]
</p>

</div>
</div>
</div>

<div id="outline-container-org9636e8d" class="outline-4">
<h4 id="org9636e8d">Ejercicio 13</h4>
<div class="outline-text-4" id="text-org9636e8d">
<div class="statement">
<p>
Un profesor asegura que tiene un nuevo método de enseñanza mejor que el
usado tradicionalmente. Para comprobar si tiene razón se selecciona de
forma aleatoria e independiente dos grupos de alumnos, A y B, utilizándose
el nuevo método en el grupo A y el tradicional con el B. A final de curso
se hace un examen a los alumnos, obteniéndose las siguientes puntuaciones.
</p>

<ul class="org-ul">
<li>Grupo A: 6, 5, 4, 7, 3, 5.5, 6, 7, 6</li>
<li>Grupo B: 5, 4, 5, 6, 4, 6, 5, 3, 7</li>
</ul>

<p>
Supuesto que las puntuaciones de cada grupo siguen una distribución normal,
¿proporcionan estos datos evidencia para rechazar el nuevo método, con
un nivel de significación 0.05?
</p>

</div>

<p>
Usaremos la dualidad entre intervalos de confianza y tests de hipótesis
para buscar si el 0 está en un intervalo de confianza 0.95 de la diferencia
de ambas medias. Las varianzas son desconocidas pero las suponemos iguales.
</p>

<p>
El intervalo para \(\mu_1-\mu_2\) de menor longitud media uniforme a nivel de
confianza \(1-\alpha\) es:
</p>

<p>
\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]
</p>

<p>
Y simplemente calculamos.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Autor: Mario Román</p>
<p class="date">Created: 2017-04-23 Sun 19:47</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
