#+TITLE: Math Notes
#+SUBTITLE: Theory
#+AUTHOR: Mario Román
#+EMAIL: mromang08@gmail.com
#+OPTIONS: num:nil broken-links:mark
#+LANGUAGE: es
#+STARTUP: indent
#+SETUPFILE: math.setup
#+SETUPFILE: html.setup

* Papers & articles
** TODO Dependent types at work - Ana Bove, Peter Dybjer
*** 1. What are dependent types?
*** 2. Simply Typed Functional Programming in Agda
**** 2.1. Truth Values
**** 2.2. Natural numbers
***** Notion of Inductive type
      /Recursive types/ in Haskell are *inductive types* in constructive type
      theory.
***** Notion of Canonical form
      Elements on canonical form are built up by constructors only. They do not
      contain defined functions. Martin-Löf considers /lazy canonical forms/, where
      it suffices to begin with a constructor:

      #+BEGIN_SRC haskell
      Zero * Zero        -- Not a canonical form
      Succ (Zero + Zero) -- Lazy canonical form
      Succ (Succ Zero)   -- Canonical form
      #+END_SRC
      
**** 2.3. Lambda Notation and Polymorphism
     In Agda we have no type variables, we have families of functions:

     #+BEGIN_SRC 
     id : (A : Set) -> A -> A
     id = \(A : Set) -> \(x : A) -> x
     #+END_SRC

**** 2.4. Implicit Arguments
     Implicit arguments are declared by enclosing their typings within curly 
     braces.

**** 2.5. Gödel System T
     Gödel System T is a system of primitive recursive functionals. All typable
     programs in Gödel System T terminate. We can only use β-reduction and the
     definitions of:

     #+BEGIN_SRC 
     true
     false
     zero
     succ
     if_then_else
     natrec
     #+END_SRC

     We can define all primitive recursive functions, but also others such as the
     Ackermann fuction.

**** 2.6. Parametrised Types
**** 2.7. Termination-checking
     In M-L Type Theory, all recursion is *primitive recursion*; a structural
     recursion on the well-founded data types.

     As the Agda's termination-checker has not yet been documented, if Agda will
     be used as a system for formalising mathematics rigorously, it is advisable to
     stay within a well-specified subset such as Martin-Löf type theory.

     In fact, the termination checker will not recognize calls to non-constructors
     as smaller arguments. =(m-n)= will not be recognized as smaller than =m=,
     for example.

*** 3. Dependent Types
**** 3.1. Vectors of a given length
     We have to alternatives to define vectors of a given length:
     
     - *As a Recursive Family*:
       
       #+BEGIN_SRC 
       Vec : Set -> Nat -> Set
       Vec A zero = Unit
       Vec A (succ n) = A X Vec A n
       #+END_SRC

       Functions must be written by induction on the length of the vector.

     - *As an Inductive Family*:

       #+BEGIN_SRC 
       data Vec (A : Set) : Nat -> Set where
         [] : Vec A zero
	 _::_ : {n : Nat} -> A -> Vec A n -> Vec A (succ n)
       #+END_SRC
       
     We can use type-checking to define functions that work only over non-empty
     vectors, such as =tail= or =head=.

**** 3.2. Finite Sets
     This data type is useful when we want to access the element at a certain
     position in a vector.

**** 3.3. More Inductive Families
*** TODO 4. Propositions as Types
** TODO Monads for functional programming - Philip Wadler
*** 1. Introduction
*** 2. Evaluating monads
**** 2.1. Variation zero: The basic evaluator
**** 2.2. Variation one: Exceptions
**** 2.3. Variation two: State
**** 2.4. Variation three: Output
**** 2.5. A monadic evaluator
**** 2.6. Variation zero, revisited: The basic evaluator
**** 2.7. Variation one, revisited: Exceptions
**** 2.8. Variation two, revisited: State
**** 2.9. Variation three, revisited: Output
*** 3. Monad Laws
    Son equivalentes =return,join= y =return,bind=. Y además, desde cualesquiera
    de ellos, se define =map=.
*** 4. State
**** 4.1. Arrays
**** 4.2. Array transformers
**** 4.3. Array readers
     Conmutative monads.
**** 4.4. Conclusion
*** TODO 5. Parsers
** Koszul Pairs and applications - Pascual Jara, Dragoş Ştefan
*** Introduction
**** Koszul ring
*Koszul ring*. A graded ring $A$ is *Koszul* if $A^0$ is a semisimple ring 
and it has a resolution $P_\ast$ by projective graded left A-modules such 
that each $P_n$ is generated by homogeneous elements of degree $n$.

**** Graded ring
*Graded ring*. A ring that is a direct sum of abelian groups:

\[ A = \bigoplus_{n \in \mathbb{N}} A_n\]

such that $A_iA_j \subset A_{i+j}$.

***** Homogeneous Elements
A *homogeneous element* is an element of any factor $A_i$ of the 
decomposition.

*Example:* A polynomial ring $A = \mathbb{K}[x_1,x_2, \dots]$ is graded with $A_i$ 
being the abelian group of polynomials with only monomials of 
degree $i$.
# QUESTION: Do they admit a different gradation?
# We can take $A_i$ to be the group of polynomials of degree 
# *equal or less* than i!

**** Semisimple group
*Semisimple group*. A group is semisimple if it has no non-trivial 
normal abelian subgroups.

Different uses of this term can be found [[http://planetmath.org/semisimplegroup][here]].
# QUESTION: Which are we interested in?

**** Semisimple module
*Semisimple module*. It is a direct sum of simple modules, that is, 
they have no non-zero proper submodules.

**** Semisimple algebra
An associative finite dimensional algebra $A$ is *semisimple* if
$A$ is a direct product of simple algebras or equivalently, if $A$ has
trivial Jacobson radical.

*** 1. Almost-koszul pairs
**** 1.1. R-rings
***** R-Ring
*R-ring*. Associative and unital algebra. It is an associative and 
unital ring $A$ together with a morphism $u : R \longrightarrow A$.

***** Graded and connected R-rings
*Graded and connected R-rings*. A R-ring is graded if it is equipped 
with a decomposition:

\[A = \bigoplus_{n \in \mathbb{N}} A^n \]

such that multiplicaton $m^{p,q}$ maps $A^p \otimes A^q$ into $A^{p+q}$. It is *connected* 
when $A_0 = R$. It is *strongly graded* when $m^{1,p}$ is surjective. We 
call $\pi^n_A$ to the projection of $A$ onto $A^n$.

**** 1.2. R-corings
***** Definition of coalgebra
A [[https://en.wikipedia.org/wiki/Coalgebra#Formal_definition][coalgebra]] over a field $K$ is a *vector space* $V$ together with linear
maps $\Delta : V \longrightarrow V \otimes V$ and $\varepsilon : V \longrightarrow K$ such that:

 1. $(id \otimes \Delta) \circ \Delta = (\Delta \otimes id) \circ \Delta$
 2. $(id \otimes \varepsilon) \circ \Delta = id 
    = (\varepsilon \otimes id) \circ \Delta$

Sometimes, the coalgebras use [[https://en.wikipedia.org/wiki/Coalgebra#Sweedler_notation][Sweedler notation]].

***** Examples of coalgebras
****** The divided power coalgebra
Consider $K[X]$, the polynomial ring, where we define by linearity:

\[\Delta(X^n) = \sum^n_{k=0} {n \choose k} X^k \otimes X^{n-k}\]

\[\epsilon(X^n) = \twopartdef{1}{n=0}{0}{n>0}\]

When the structures of algebra and coalgebra are compatible, they
are called [[https://en.wikipedia.org/wiki/Bialgebra][bialgebras]].

***** R-coring
*R-coring*. Coassociative and counital coalgebra. It is an R-bimodule 
with a /comultiplication/ $\Delta : C \longrightarrow C \otimes C$ and 
a /counit/ $\epsilon : C \longrightarrow R$.

***** Graded corings
*Graded corings*. Decomposition $C = \bigoplus_{n \in \mathbb{N}} C_n$, 
such that:

\[\Delta(C_n) \subset \bigoplus_{p=0}^n C_p \otimes C_{n-p}\]

**** 1.3. Almost-Koszul pair
*Almost-Koszul pair*. Connected R-ring and R-coring $(A,C)$ with an 
isomorphism $\theta_{C,A} : C_1 \longrightarrow A^1$, that satisfies the relation:

\[ m^{1,1} \circ (\theta_{C,A} \otimes \theta_{C,A}) \circ \Delta_{1,1}
= 0\]

Or, using Sweedler notation, for any $c \in C_2$:

\[ \sum \theta_{C,A}(c_{(1,1)}) \theta_{C,A}(c_{(2,1)}) = 0\]

**** 1.4. Opposite Koszul pair
If $(A,C)$ is a Koszul pair, then $(A^{op},C^{op})$ are Koszul pairs with
respect to:

\[\theta_{C^{op},A^{op}} = \theta_{C,A}\]

**** 1.5. The normalized bar resolution of R
For every strongly graded R-ring A, there is a graded coring C such that
$(A,C)$ is an almost-Koszul pair.

***** The normalized right bar resolution
The exact sequence $\beta_\ast^r(A)$:

\[ 0 \longleftarrow 
R \overset{\delta_0}\longleftarrow 
A \overset{\delta_1}\longleftarrow
\overline{A} \otimes A \overset{\delta_2}\longleftarrow
\overline{A} \otimes \overline{A} \otimes A \overset{\delta_3}\longleftarrow
\overline{A} \otimes \overline{A} \otimes \overline{A} \otimes A \longleftarrow
\dots
\]

is called the *normalized right bar resolution*. Where
the $\delta$ are defined as:

 - $\delta_0 = \pi^0_A$
 - \[ \delta_n(a_1 \otimes \dots \otimes a_n \otimes a_{n+1}) 
      = \sum_{i=1}^n (-1)^i  a_1 \otimes \dots \otimes a_ia_{i+1} \otimes \dots \otimes a_{n+1}\]

***** TODO Normalized bar complex

*** 2. Koszul Pairs

*** 3. Hochschild (co)homology of Koszul rings
**** 3.1. The cyclic tensor product
***** Enveloping algebra of R
The tensor product algebra $R^e = R \otimes_\mathbb{K} R^{op}$ is called the 
*enveloping algebra* of $R$.

*** 4. Almost-Koszul pairs associated to twisted tensor products

*** 5. The Hochschlid cohomology of a twisted tensor product
* Aluffi - Algebra Chapter 0
** III. Anillos y módulos
*** 7. Complejos y homología
**** 7.1. Complejos y secuencias exactas.
 #+begin_definition
 *Complejo*. Un complejo es una serie de morfismos $d_i$ entre R-Módulos:

 \[\dots \longrightarrow M_{i+1} \longrightarrow M_i \longrightarrow M_{i-1} \longrightarrow \dots\]

 tales que $d_i \circ d_{i+1} = 0$.
 #+end_definition

 Además lo llamamos *exacto* cuando $im (d_{i+1}) = ker (d_i)$.

 #+begin_proposition
 *Exactitud de monomorfismos y epimorfismos*. Dos complejos de la forma:

 \[ \dots \longrightarrow 0 \longrightarrow L \overset{\alpha}\longrightarrow M \longrightarrow \dots \]
 \[ \dots \longrightarrow M \overset{\beta} \longrightarrow N \longrightarrow 0 \longrightarrow \dots \]

 Son exactos en $L$ y $N$ ssi $\alpha$ y $\beta$ son monomorfismo y epimorfismo, 
 respectivamente.
 #+end_proposition

 #+begin_definition
 *Secuencia exacta corta*. Una secuencia exacta corta es un complejo de la forma:

 \[ 0 \longrightarrow L \overset{\alpha}\longrightarrow M \overset{\beta}\longrightarrow N \longrightarrow 0 \]
 #+end_definition

 El primer teorema de isomorfía nos dice que $N \cong \frac{M}{ker(\beta)} = \frac{M}{im(\alpha)}$ lo que nos 
 lleva a identificar   $N \cong \frac{M}{L}$. De hecho, cada monomorfismo da lugar a una 
 secuencia exacta corta:

 \[ 0 \longrightarrow \ker(\phi) \longrightarrow M \longrightarrow im(\phi) \longrightarrow 0 \]

**** 7.2. Secuencias exactas escindidas
 #+begin_definition
 *Secuencia escindida*. Una secuencia exacta corta:

 \[ 0 \longrightarrow M_1 \longrightarrow N \longrightarrow M_2 \longrightarrow 0 \]

 es escindida si es isomorfa a una secuencia de la forma siguiente:

 \[ \begin{tikzcd}
 0   \arrow{r}{} & 
 M_1 \arrow{d}{\sim}\arrow{r}{} & 
 N   \arrow{d}{\sim}\arrow{r}{} & 
 M_2 \arrow{d}{\sim}\arrow{r}{} & 
 0 \\
 0   \arrow{r}{} & 
 M_1 \arrow{r}{} & 
 M_1 \oplus M_2   \arrow{r}{} & 
 M_2 \arrow{r}{} & 
 0
 \end{tikzcd} \]

 Es decir, hay un isomorfismo entre secuencias.
 #+end_definition

 #+begin_theorem
 *Relación entre secuencias escindidas e inversas*. Sea $\phi$ un homomorfismo;
 entonces tiene inversa izquierda ssi la secuencia siguiente escinde:

 \[ 0 \longrightarrow M \overset{\phi}\longrightarrow N \longrightarrow coker(\phi) \longrightarrow 0 \]

 Y tiene inversa derecha si la secuencia siguiente escinde:

 \[ 0 \longrightarrow ker(\phi) \longrightarrow M \overset{\phi}\longrightarrow N \longrightarrow 0 \]
 #+end_theorem

**** 7.3. Homología, y el lema de la serpiente
 #+begin_definition
 *Homología*. La i-ésima homología de un complejo,

 \[ \dots \longrightarrow M_{i+1} \overset{d_{i+1}}\longrightarrow M_i \overset{d_i}\longrightarrow M_{i-1} \longrightarrow \dots \]

 es el R-módulo:

 \[H_i(M) = \frac{ker(d_i)}{im(d_{i+1})}\]
 #+end_definition

 La homología mide lo que se aleja de ser exacto en un punto determinado, y
 es $0$ cuando el complejo es exacto. Puede verse como una generalización de
 kernel y cokernel; que los realiza en este caso extremo:

 \[ 0 \longrightarrow M_1 \overset{\phi}\longrightarrow M_0 \longrightarrow 0 \]

 En el que $H_1(M) \cong ker(\phi)$ y $H_0(M) \cong coker(\phi)$.

 #+begin_theorem
 *Lema de la serpiente*. Teniendo dos secuencias exactas en el diagrama 
 conmutativo siguiente:

 \[ \begin{tikzcd}
 0 \rar & L_1 \rar{\alpha_1}\arrow{d}{\lambda} & M_1 \rar{\beta_1}\arrow{d}{\mu} & N_1 \rar\arrow{d}{\eta} & 0 \\
 0 \rar & L_0 \rar{\alpha_0}                   & M_0 \rar{\beta_0}               & N_0 \rar                & 0
 \end{tikzcd} \]

 Existe una secuencia exacta de la forma:

 \[ 0 \overset{}\longrightarrow 
 ker(\lambda) \overset{}\longrightarrow 
 ker(\mu) \overset{}\longrightarrow 
 ker(\eta) \overset{\delta}\longrightarrow 
 coker(\lambda) \overset{}\longrightarrow 
 coker(\mu) \overset{}\longrightarrow 
 coker(\eta) \overset{}\longrightarrow 
 0\]
 #+end_theorem

 El diagrama desde el que se deduce todo esto, con columnas exactas, es
 el siguiente:

 \[ \begin{tikzcd}
	& 0 \dar              & 0 \dar            & 0 \dar           &   \\
 0 \rar & ker(\lambda) \rar \dar  & ker(\mu) \rar \dar    & ker(\eta) \dar \ar[out=355, in=175,looseness=1, overlay, swap]{dddll}{\delta}       &   \\
 0 \rar & L_1 \rar{\alpha_1} \dar{\lambda}  & M_1 \rar{\beta_1} \dar{\mu} & N_1 \rar \dar{\eta}        & 0 \\
 0 \rar & L_0 \rar{\alpha_0} \dar & M_0 \rar{\beta_0} \dar & N_0 \rar \dar        & 0 \\
	& coker(\lambda) \rar \dar & coker(\mu) \rar \dar  & coker(\eta) \rar \dar & 0 \\
	& 0                   & 0                 & 0                &
 \end{tikzcd} \]

** IV. Álgebra lineal
*** 4. Presentaciones y resoluciones
**** 4.1. Torsión
 #+begin_definition
 *Torsión*. Un elemento $m \in M$ módulo de $R$ es de *torsión* si $\{m\}$ es linealmente
 dependiente. Es decir,

   \[ \exists r \in R,\ r \neq 0\ :\ rm = 0 \]

 El conjunto de elementos de torsión se llama $Tor(M)$. Un módulo es *libre de torsión*
 si $Tor(M) = 0$ y *de torsión* si $Tor(M)=M$.
 #+end_definition

 Un anillo conmutativo es libre de torsión sobre sí mismo si y sólo si es dominio de
 integridad. Cuando esto ocurre, $Tor(M)$ es siempre submódulo de $M$. Submódulos o
 sumas de módulos libres de tensión serán libres de torsión, y por todo esto, los módulos
 libres sobre dominios de integridad serán libres de torsión.

 #+begin_definition
 *Cíclico*. Un módulo es *cíclico* cuando es generado por un elemento. Es decir,
 cuando $M \cong R/I$ para algún ideal.
 #+end_definition

 Cuando en un dominio de integridad todos sus
 módulos cíclicos son libres de torsión, es un cuerpo. Otra forma de pensar sobre un módulo
 cíclico es como aquel que admite un epimorfismo:

 \[ R \longrightarrow M \longrightarrow 0 \]

**** 4.2. Módulos finitamente presentados y resoluciones libres
 #+begin_definition
 *Anulador.* El anulador de un módulo $M$ es:

 \[Ann_R(M) = \{ r \in R\ |\ \forall m \in M, rm = 0 \}\]
 #+end_definition

 Es un ideal de $R$. Cuando $M$ es finitamente generado y $R$ es dominio de integridad,
 $M$ es de torsión si y sólo si $Ann(M) \neq 0$.

 #+begin_definition
 *Módulos finitamente generados y presentados*. Sabemos que todos los módulos admiten un
 epimorfismo de la forma:

 \[ R^{\oplus A} \longrightarrow M \longrightarrow 0\]

 Cuando lo admiten con $A$ finito, se tiene $M$ *finitamente generado*. Un módulo se dice
 *finitamente presentado* si hay una secuencia exacta de la forma:

 \[R^n \overset{\phi}\longrightarrow R^m \longrightarrow M \longrightarrow 0\]

 .
 #+end_definition

 Si $R$ es Noetheriano, todo módulo finitamente generado es finitamente presentado.

 #+begin_definition
 *Resolución*. Una resolución de $M$ mediante módulos libres finitamente generados es
 un complejo exacto:

 \[ \dots \rightarrow R^{m_3} \rightarrow R^{m_2} \rightarrow R^{m_1} \rightarrow R^{m_0} \rightarrow M \rightarrow 0 \]
 #+end_definition

 Aquí podemos entender que $R^{m_0}$ contiene los generadores, $R^{m_1}$ las relaciones
 entre los generadores, $R^{m_2}$ las relaciones entre relaciones, y así sucesivamente.

 Un dominio de integridad es *cuerpo si y sólo si todos sus módulos son finitamente generados*,
 esto es equivalente a tener:

 \[ 0 \longrightarrow R^m \longrightarrow M \longrightarrow 0 \]

 para cualquier módulo.

 Un dominio de integridad es *PID si todas las resoluciones como finitamente generado 
 extienden a finitamente presentado*, de la forma:

 \[0 \longrightarrow R^{m_1} \longrightarrow R^{m_0} \overset{\pi}\longrightarrow M \longrightarrow 0\]

 esto equivale a pedir que $\ker(\pi)$ sea libre.

**** 4.3. Leyendo una presentación
 Hemos visto que podemos estudiar un módulo finitamente presentado por un
 morfismo $\phi: R^n \longrightarrow R^m$, donde $M = coker(\phi)$. Esto quiere decir que 
 podemos asignarle una matriz explícita.

 #+begin_theorem
 *Producto de módulos en matrices*. Sean $M,N$ módulos con matrices $A,B$.
 Tenemos $M \oplus N$ con matriz:

 \[\left(\begin{array}{c|c}
 A & 0 \\ \hline 0 & B 
 \end{array}\right)\]
 #+end_theorem

 Además nótese que las *matrices equivalentes* representan el mismo 
 homeomorfismo, y por tanto el mismo módulo.

 #+begin_theorem
 *Transformaciones de matrices de módulos*. Una matriz representa el mismo módulo
 tras las transformaciones de:
  - Permutar filas o columnas
  - Añadir filas o columnas linealmente dependientes
  - Multiplicar filas o columnas por una unidad
  - Quitar una fila y columna en la que sólo queda una unidad
 #+end_theorem

 Las primeras son consecuencia de la equivalencia. La última puede colocarse como
 una parte de identidad en una matriz de la forma:

 \[A = \left(\begin{array}{c|c}
 u & 0 \\ \hline 0 & A' 
 \end{array}\right)\]

 Que no afecta al cokernel.

** VII. Cuerpos
*** 1. Extensiones de cuerpos I
**** 1.1. Definiciones básicas
***** Categoría de los cuerpos
Los cuerpos forman la *categoría $\mathtt{Fld}$* con los homomorfismos de 
anillos entre ellos. Todo homomorfismo de anillos entre cuerpos
es inyectivo y todo morfismo en esta categoría es monomorfismo.

Así, todo morfismo entre cuerpos en $Hom(k,K)$ es una extensión $K/k$.

***** Característica de un cuerpo
      La *característica* de $K$ es el generador de $ker(i)$ para 
      $i : \mathbb{Z} \longrightarrow K$. Las extensiones preservan la 
      característica, así que podemos particionar la categoría en categorías 
      $\mathtt{Fld}_p$.

***** Cuerpos primos
      El inicial de $\mathtt{Fld}_0$ es $\mathbb{Q}$, y el de $\mathtt{Fld}_p$ es $\mathbb{F}_p = \mathbb{Z}/p\mathbb{Z}$. Todos los
      cuerpos son extensiones de uno de estos llamados *cuerpos primos*.

***** Grado de una extensión
El *grado*, $[F : K]$, de una extensión es su dimensión como espacio
vectorial sobre la base. Es *finita* o *infinita* si lo es su grado.

**** 1.2. Extensiones simples
***** Extensión simple
Una extensión es *simple* si es de la forma $K(\alpha)$ donde 
$K(\alpha)$ es la intersección de todos los subcuerpos de algún
$F$ conteniendo al cuerpo $K$ y el elemento $\alpha$.

***** Polinomio irreducible mínimo
Dada una extensión simple $K(\alpha)$, consideramos la evaluación
$\epsilon : K[X] \longrightarrow K(\alpha)$ por casos:

 - Es *inyectiva* ssi es una *extensión infinita*. En este
   caso $K(\alpha) \cong K(X)$ es el cuerpo de funciones racionales.
 - No es *inyectiva*. Existe un único polinomio mónico
   irreducible $p$ que genera el núcleo,

   \[ K(\alpha) \cong \frac{K[t]}{(p(t))}\]

   Se le llama *polinomio mínimo*.

***** TODO Extensión de isomorfismos a extensiones simples
Proposition 1.5
***** Automorfismos de una extensión
El *grupo de automorfismos* de una extensión $Aut_K(F)$, es el
grupo de los automorfismos de cuerpos que dejan fijo $K$.
***** Automorfismos y raíces
Sea $K(\alpha)$ con $p$ polinomio mínimo. Entonces $p$ tiene $|Aut_K(K(\alpha))|$ raíces
distintas en $K(\alpha)$. En particular,

\[ |Aut_K(K(\alpha))| \leq [K(\alpha):K] \]

y el caso de igualdad se tiene con $p$ factorizando en factores 
lineales sobre $F$.
**** 1.3. Extensiones finitas y algebraicas
***** Elementos algebraicos y trascendentes
Sea $F/K$ una extensión con $\alpha \in F$, entonces $\alpha$ es *algebraico*
cuando $K(\alpha)/K$ es finita, y *trascendente* si no. Una extensión
es *algebraica* si todos sus elementos lo son.

*** 6. Un poco de teoría de Galois
**** 6.1. Correspondencia de Galois y extensiones de Galois
***** Cuerpo fijo
Sea $F/k$ extensión y $G \subseteq Aut_k(F)$. Llamamos *cuerpo fijo* de $G$ a:

\[ F^G = \{ \alpha\in F \mid \forall g \in G, g\alpha=\alpha\}\]

***** Correspondencia de Galois
Hay correspondencia entre los cuerpos intermedios de la extensión
y los subgrupos del grupo de automorfismos.

Dado $E$ cuerpo intermedio, lo enviamos a $Aut_E(F)$. Dado $G$ lo enviamos
a $F^G$.

***** Inclusión y correspondencia
Para cualesquiera subgrupo $G$ y cuerpo intermedio $E$:

 - $E \subseteq F^{Aut_E(F)}$
 - $G \subseteq Aut_{F^G}(F)$

Si llamamos $E_1E_2$ al menor subcuerpo de $F$ conteniendo $E_1,E_2$ y llamamos
$<G_1,G_2>$ al menor subgrupo de los automorfismos conteniendo $G_1,G_2$:

 - $Aut_{E_1E_2}(F) = Aut_{E_1}(F) \cap Aut_{E_2}(F)$
 - $F^{<G_1,G_2>} = F^{G_1} \cap F^{G_2}$

***** Extensiones de Galois
Sea $F/k$ extensión, equivalen:

 - $F$ es cuerpo de descomposición de algún $f \in k[t]$.
 - $F/k$ es normal y separable.
 - $|Aut_k(F)| = [F : k]$.
 - La correspondencia de Galois es biyección.
 - $F/k$ separable y, si $E/F$ es algebraica con $\sigma \in Aut_k(E)$, $\sigma(F)=F$.

Llamamos a esto una *extensión de Galois*.
** VIII. Vuelta al álgebra lineal
*** 1. Preliminares
**** 1.1. Funtores
 #+begin_definition
 *Funtor*. Un funtor covariante:

 \[{\cal F} : C \longrightarrow D\]

 Asigna a cada $A \in C$ un ${\cal F}(A) \in D$ y mapea los morfismos entre cada par de objetos:

 \[Hom_C(A,B) \rightarrow Hom_D({\cal F}(A),{\cal F}(B))\]

 Respetando la identidad y la composición de morfismos. 

 Un *funtor contravariante* es un funtor desde la categoría opuesta:

 \[{\cal F} : C^{op} \longrightarrow D\]
 #+end_definition

 Los funtores preservan los diagramas conmutativos. Llamamos *prehaz* a un funtor
 contravariante $C \longrightarrow \mathtt{Set}$.

 #+begin_definition
 *Funtor aditivo*. Llamamos a un funtor 
 ${\cal F}: R-\mathtt{Mod} \longrightarrow S-\mathtt{Mod}$ *aditivo* cuando
 la función $Hom_{R}(A,B) \rightarrow Hom_{S}({\cal F}(A),{\cal F}(B))$ es homomorfismo de grupos.
 #+end_definition

**** 1.3. Equivalencia de categorías
 #+begin_definition
 *Funtores plenamente fieles*. Dada la función inducida:
 \[Hom_C(A,B) \rightarrow Hom_D({\cal F}(A),{\cal F}(B))\]
 Un funtor es *fiel* si es inyectiva, *pleno* si es sobreyectiva y *plenamente fiel*
 si es biyectiva.
 #+end_definition

 #+begin_definition
 *Equivalencia de categorías*. Un funtor es una equivalencia de categorías si 
 es plenamente fiel y esencialmente sobreyectivo, es decir, para cada $Y \in D$,
 existe un $X \in C$ tal que $F(X) \cong Y$.
 #+end_definition

**** 1.4. Límites y colímites

 #+begin_definition
 *Límite*. Para un funtor ${\cal F}: {\cal I} \longrightarrow C$, su límite es
 un objeto $L \in C$ con morfismos $\lambda_I: L \longrightarrow {\cal F}(I)$ tales que

 - Conmuta el siguiente diagrama para cualquier $\alpha : I \longrightarrow J$:

 \[ \begin{tikzcd}[column sep=1.5em]
  & L \arrow{dr}{\lambda_J} \arrow{dl}[swap]{\lambda_I} \\
 {\cal F}(I) \arrow{rr}{{\cal F}(\alpha)} && {\cal F}(J)
 \end{tikzcd} \]

 - $L$ es final en este diagrama.
 #+end_definition

 Será esencialmente único y puede notarse por $\varprojlim {\cal F}$.

 #+begin_theorem
 *Límites sobre cadenas en R-Mod*. En R-Mod siempre existe un límite llamado \(\varprojlim {\cal A}_i\) sobre una
 cadena de la forma:

 \[ \begin{tikzcd}
 & & A 
 \arrow{lld}[swap]{\phi_5}
 \arrow{ld}{\phi_4}
 \arrow{d}{\phi_3}
 \arrow{rd}[swap]{\phi_2}
 \arrow{rrd}{\phi_1} 
 & & \\
 \dots \arrow{r}[swap]{\phi_{45}}  &
 A_4 \arrow{r}[swap]{\phi_{34}} &
 A_3 \arrow{r}[swap]{\phi_{23}} &
 A_2 \arrow{r}[swap]{\phi_{12}} &
 A_1
 \end{tikzcd} \]
 #+end_theorem

 Este límite es el submódulo de las /secuencias coherentes/ en $\prod_i A_i$, es decir, de
 aquellas tales que $a_i = \phi_{i,i+1}(a_{i+1})$; teniendo como morfismos $\phi_i$ las proyecciones
 canónicas


 #+begin_definition
 *Colímite*. La noción dual de límite es el *colímite*, es decir, para
 un funtor ${\cal F} : I \longrightarrow C$, su colímite es un objeto $L \in C$ con morfismos $\gamma_i : {\cal F}(I) \longrightarrow L$
 tales que

 - Conmuta el siguiente diagrama para cualquier $\alpha : I \longrightarrow J$:

 \[ \begin{tikzcd}[column sep=1.5em]
  & L  \\
 {\cal F}(I) \arrow{ur}{\gamma_I} \arrow{rr}{{\cal F}(\alpha)} && {\cal F}(J) \arrow{ul}[swap]{\gamma_J}
 \end{tikzcd} \]

 - $L$ es inicial en este diagrama.
 #+end_definition

**** 1.5. Comparando funtores
 #+begin_definition
 *Transformación natural*. Una transformación natural entre dos funtores ${\cal F} \Longrightarrow {\cal G}$ 
 consiste en morfismos $\upsilon_X : {\cal F}(X) \longrightarrow {\cal G}(X)$ tales que conmuta el diagrama:

 \[ \begin{tikzcd}
 {\cal F}(X) \arrow{r}{{\cal F}(\alpha)} \arrow{d}{\upsilon_X} & {\cal F}(Y) \arrow{d}{\upsilon_Y} \\
 {\cal G}(X) \arrow{r}{{\cal G}(\alpha)} & {\cal G}(Y)
 \end{tikzcd}
 \]

 para cualquier morfismo $\alpha$.

 Llamamos *isomorfismo natural* a una transformación natural donde cada $\upsilon$
 es un isomorfismo.
 #+end_definition

 #+begin_definition
 *Funtor adjunto*. Llamamos ${F}$ y ${G}$ adjuntos si tenemos:

 \[ Hom_C(X,GY) \cong Hom_D(FX,Y) \]

 Isomorfismos naturales.
 #+end_definition

 Lo que nos da realmente un isormorfismo natural de $Hom_C(F-,-)$ con $Hom_D(-,G-)$,
 entendidos como funtores. Llamamos aquí adjunto izquierdo a $F$ y adjunto derecho a $G$.
 Tenemos más sobre funtores adjuntos en la lista de reproducción de [[https://www.youtube.com/playlist?list=PL54B49729E5102248][The Catsters]].

 #+begin_theorem
 *Continuidad de adjuntos*. Los funtores adjuntos derechos son continuos, los adjuntos
 izquierdos son cocontinuos. Es decir, para $I : {\cal I}\longrightarrow D$, $J : {\cal J}\longrightarrow C$

 \[G(\varprojlim I) = \varprojlim (G \circ I)\]
 \[F(\varinjlim J) = \varinjlim (F \circ J)\]
 #+end_theorem

 Siempre que existan los límites. La demostración de esto se puede hacer aplicando los
 funtores en los diagramas conmutativos y usando las propiedades universales de los límites.

 #+begin_definition
 *Funtor exacto*. Un funtor exacto respeta la exactitud de las secuencias. Es decir,
 siendo la siguiente secuencia exacta:

 \[ 0 \longrightarrow A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C \longrightarrow 0\]

 La siguiente secuencia será exacta:

 \[ 0 \longrightarrow FA \overset{F\phi}\longrightarrow FB \overset{F\psi}\longrightarrow FC \longrightarrow 0\]
 #+end_definition

 En particular, lo llamamos /exacto a la izquierda/ si preserva la exactitud de:

 \[ 0 \longrightarrow A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C\]

 Y /exacto a la derecha/ si preserva la exactitud de:

 \[ A \overset{\phi}\longrightarrow B \overset{\psi}\longrightarrow C \longrightarrow 0\]

*** 2. Producto tensor y el funtor Tor
**** 2.1. Aplicaciones bilineales
 #+begin_definition
 *Aplicación bilineal*. Una aplicación $\phi:M\times N \longrightarrow P$ es bilineal si
 son lineales $\phi(\_,n)$ y $\phi(m,\_)$ para cualesquiera $m,n$.
 #+end_definition

 #+begin_definition
 *Producto tensor*. $M \otimes_R N$ es el producto tensor de $M$ y $N$ como módulos de $R$
 si cualquier aplicación bilineal factoriza de forma única a través de él:

 \[ \begin{tikzcd}
 M \times N \arrow{r}{\phi} \arrow{d}{\otimes} & P \\
 M \otimes N \arrow{ru}[swap]{\exists! \overline\phi} &
 \end{tikzcd} \]
 #+end_definition

 Usando universalidad podemos ver que $R \otimes N \cong N$ y que $M\otimes N \cong N\otimes M$. La construcción
 explícita del producto tensor se hace sobre el módulo libre sobre $M \times N$ provocando un
 cociente sobre los submódulos generados por:

 \[(m,r_1n_1+r_2n_2) - r_1(m,n_1) - r_2(m,n_2)\]
 \[(r_1m_1+r_2m_2,n) - r_1(m_1,n) - r_2(m_2,n)\]

 Lo que nos permite actuar con ellos de forma bilineal. La demostración se basa en usar
 la propiedad universal de la proyección sobre ese cociente.

**** 2.2. Adjunción con Hom
 Dado un módulo $N$ de $R$, tenemos un funtor covariante $\otimes_R N$, que será *adjunto izquierdo*
 a $Hom_{R-mod}(N,-)$. Podemos observar simplemente que una aplicación bilineal, al currificarse,
 determina una función que va de $M$ a $Hom(N,P)$, y que es lineal. Sabiendo esto, es trivial
 que:

 \[ Hom_R(M, Hom_R(N,P)) \cong Hom_R(M \otimes N, P)\]

 La naturalidad y el hecho de que es un isomorfismo se comprueban fácilmente. El hecho de
 que exista una adjunción nos dice además que $\otimes_R N$, o $N\otimes_R$ por la isomorfía anterior,
 son cocontinuos.

 #+begin_fact
 Para cualesquiera \(R\)-módulos, se tiene:

 \[(M_1 \oplus M_2) \otimes N \cong (M_1 \otimes N) \oplus (M_2 \otimes N)\]

 \[N \otimes (M_1 \oplus M_2) \cong (N \otimes M_1) \oplus (N \otimes M_2)\]

 \[(\oplus_\alpha M_\alpha) \otimes N \cong \oplus_\alpha (M_\alpha \otimes N)\]
 #+end_fact

 Por cocontinuidad.

 #+begin_fact
 Para cualesquiera dos conjuntos $A,B$, se tiene:

 \[R^{\oplus A} \otimes R^{\oplus B} \cong R^{\oplus A \times B}\]
 #+end_fact

 Teniendo \(R^{\oplus n} \otimes R^{\oplus m} \cong R^{\oplus nm}\). De hecho, la base del espacio producto
 tensor la forman los vectores puros que emparejan elementos de las 
 bases de cada uno de los espacios.

 #+begin_theorem
 *Producto tensor de cocientes*. Dado un $N$ módulo de $R$, e $I$ ideal,
 tenemos:

 \[\frac{R}{I}\otimes N \cong \frac{N}{IN}\]

 Y desde ahí, aplicando además el tercer teorema de isomorfía, tenemos:

 \[\frac{R}{I} \otimes \frac{R}{J} \cong \frac{R}{I+J}\]
 #+end_theorem

 Esto se deduce de aplicar el funtor $\_ \otimes N$ a la secuencia exacta del 
 ideal:

 \[I \longrightarrow R \longrightarrow \frac{R}{I} \longrightarrow 0\]
 
 \[I \otimes N \longrightarrow N \longrightarrow \frac{R}{I} \otimes N \longrightarrow 0\]

 Desde donde se obtiene $IN$ como inclusión de $I\otimes N$ en $N$.

**** 2.3. Exactitud y planitud
 #+begin_definition
 *Módulo plano*. El módulo $N$ es *plano* si el funtor $\_ \otimes N$ es un
 funtor exacto.
 #+end_definition

 Un *módulo libre* será siempre plano.

**** 2.4. Los funtores Tor
 #+begin_definition
 *El funtor Tor*. Lo que se aleja de la exactitud el funtor $\_ \otimes N$
 es medido por el funtor $Tor_1(\_,N)$. De hecho, si tenemos una secuencia
 exacta:

 \[0\longrightarrow A \longrightarrow B \longrightarrow C \longrightarrow 0\]

 Obtenemos aplicando el funtor $\otimes N$ esta otra secuencia:

 \[Tor_1(C,N) \longrightarrow A \otimes N \longrightarrow B \otimes N \longrightarrow C \otimes N \longrightarrow 0\]

 Y de hecho, esta secuencia podrá extenderse aún más con /funtores derivados/,
 que se definen como:

 \[Tor_i^R(M,N) = H_i(M_{\bullet} \otimes N)\]
 #+end_definition

 Aquí entendemos $M_\bullet \otimes N$ como el complejo que se obtiene tomando una resolución
 libre de $M$:

 \[\dots \longrightarrow R^{\otimes S_2} \longrightarrow R^{\otimes S_1} 
 \longrightarrow R^{\otimes S_0} \longrightarrow M \longrightarrow 0}\]

 Y retirando $M$ y tensando sobre $N$, para tener:

 \[\dots \longrightarrow N^{\otimes S_2} \longrightarrow N^{\otimes S_1} 
 \longrightarrow N^{\otimes S_0} \longrightarrow 0}\]

 Todo esto se obtendrá de manera natural aplicando el lema de la serpiente a una secuencia
 de resoluciones compatibles, algo que, si los módulos fueran PID y tuvieran una resolución
 de grado 2, sería de la forma:

 \[ \begin{tikzcd}
    & 0 \dar & 0 \dar & 0 \dar &   \\
 0 \rar & R^{\oplus a_1}\rar\dar & R^{\oplus b_1} \rar\dar & R^{\oplus c_1} \rar\dar & 0 \\
 0 \rar & R^{\oplus a_0}\rar\dar & R^{\oplus b_0} \rar\dar & R^{\oplus c_0} \rar\dar & 0 \\
 0 \rar & A\rar\dar & B \rar\dar & C \rar\dar & 0 \\
  & 0 & 0 & 0 & 
 \end{tikzcd} \]

 Tensando las dos filas superiores, que son libres, nos quedarían dos filas sobre las que aplicar
 el lema de la serpiente y obtener los funtores derivados tal y como los hemos definido.

*** 5. Funtor Hom y dualidad 
**** 5.1. Adjunciones, de nuevo
 Ya sabemos que el funtor $Hom(N,\_)$ es adjunto derecho a $\_\otimes N$, ahora
 estudiamos el funtor $Hom(\_,N)$.

 #+begin_theorem
 *Adjunción de Hom contravariante*. El funtor $Hom(\_,N)$ es adjunto derecho
 de su funtor opuesto, $Hom^{op}(\_,N)$.
 #+end_theorem

 Aplicando currificación tenemos trivialmente:

 \[Hom(L,Hom(M,N)) \cong Hom(M,Hom(L,N))\]

 Que, teniendo en cuenta que estamos usando la categoría opuesta, prueba la
 adjunción.

 #+begin_proposition
 *Exactitud de Hom*. Ambos funtores $Hom$ son adjuntos derechos y por tanto,
 exactos por la izquierda. Teniendo en cuenta que uno es contravariante, quiere
 decir que:

 \[ A \overset{}\longrightarrow B \overset{}\longrightarrow C \overset{}\longrightarrow 0\]

 Lleva a:

 \[ 0 \overset{}\longrightarrow Hom(C,N) \overset{}\longrightarrow 
 Hom(B,N) \overset{}\longrightarrow Hom(A,N)\]
 #+end_proposition

**** 5.2. Módulos duales.
 #+begin_definition
 *Módulo dual*. El dual de un R-módulo $M$ es el módulo $M^{\vee} = Hom_R(M,R)$.
 #+end_definition

 Tenemos que $Hom(M,R^n) \cong M^{\vee} \otimes R^n$.

*** 6. Módulos proyectivos e inyectivos, y el funtor Ext
**** 6.1. Proyectividad e inyectividad
 #+begin_definition
 *Módulos proyectivos e inyectivos*. Un R-módulo es /proyectivo/ si $Hom(P,\_)$
 es exacto; e /inyectivo/ si $Hom(\_,P)$ es exacto.
 #+end_definition

 Esto es equivalente a decir que cada epimorfismo $M \longrightarrow N$ lleva un
 morfismo $P \longrightarrow N$ a $P \longrightarrow M$, en el caso de /proyectividad/:

 \[ \begin{tikzcd}
  & P \dlar[swap,dashed]{\exists p'} \dar[swap]{p} \drar{0} & \\
 M \rar & N \rar & 0
 \end{tikzcd} \]

 O que cada monomorfismo $L \longrightarrow M$ lleva un morfismo $L \longrightarrow Q$ a
 un monomorfismo $M \longrightarrow Q$, en el de la /inyectividad/:

 \[ \begin{tikzcd}
  & Q & \\
 0 \urar{0} \rar & N \rar \uar[swap]{q} & M \ular[dashed,swap]{\exists q'}
 \end{tikzcd} \]

 Además, esto es equivalente a decir que un módulo $P$ es /proyectivo/ si toda secuencia

 \[ 0 \overset{}\longrightarrow L \overset{}\longrightarrow M \overset{}\longrightarrow P \overset{}\longrightarrow 0 \]

 es escindida, y $Q$ es /inyectivo/ si toda secuencia:

 \[ 0 \overset{}\longrightarrow Q \overset{}\longrightarrow M \overset{}\longrightarrow N \overset{}\longrightarrow 0 \]

 es escindida.

**** 6.2. Módulos proyectivos
 #+begin_theorem
 *Caracterización de proyectividad*. Un módulo es proyectivo ssi es el sumando
 directo de un módulo libre.
 #+end_theorem

 Así, la suma directa de dos módulos proyectivos es proyectiva; el producto tensor
 de dos módulos proyectivos es proyectivo, y todo módulo proyectivo es plano.

**** 6.3. Módulos inyectivos
 #+begin_theorem
 *Caracterización de inyectividad*. Un módulo es *inyectivo* ssi toda aplicación
 $f : I \longrightarrow Q$ extiende a una aplicación $\hat f : R \longrightarrow Q$, donde I es ideal de R.
 #+end_theorem

**** 6.4. El funtor Ext
 Existirían dos formas naturales de definir *Ext*, que coinciden no trivialmente:

 #+begin_definition
 *Funtor Ext*. Dado $M$ con una resolución proyectiva:

 \[ \dots \overset{}\longrightarrow P_1 \overset{}\longrightarrow P_0 \overset{}\longrightarrow M \overset{}\longrightarrow 0 \]

 aplicamos el funtor contravariante $Hom(\_,N)$ eliminando $M$ para obtener:

 \[ 0 \overset{}\longrightarrow Hom(P_0,N) \overset{}\longrightarrow Hom(P_1,N) \overset{}\longrightarrow Hom(P_2,N) \overset{}\longrightarrow \dots \]

 Y tomamos la cohomología de este complejo $Hom(M_\bullet,N)$, dejando como definición:

 \[Ext^i_R(M,N) = H^i(Hom(M_\bullet,N))\]
 #+end_definition

 #+begin_definition
 *Funtor Ext*. Dado $N$ con una resolución inyectiva:

 \[ 0 \overset{}\longrightarrow N \overset{}\longrightarrow Q_0 \overset{}\longrightarrow Q_1 \overset{}\longrightarrow \dots \]

 aplicamos el funtor covariante $Hom(M,\_)$ eliminando $N$ para obtener:

 \[ 0 \overset{}\longrightarrow 
 Hom(M,Q_0) \overset{}\longrightarrow 
 Hom(M,Q_1) \overset{}\longrightarrow 
 Hom(M,Q_2) \overset{}\longrightarrow \dots \]

 Y tomamos la cohomología de este complejo $Hom(M,N_\bullet)$, dejando como definición:

 \[Ext^i_R(M,N) = H^i(Hom(M,N_\bullet))\]
 #+end_definition

** IX. Álgebra homológica
*** Complejos y homología, de nuevo
**** 3.1. Recordatorio de definiciones básicas
 #+begin_definition
 *Resolución*. La /resolución/ de un objeto $A$ es un complejo
 exacto excepto en un punto, donde es isomorfa a $A$.
 #+end_definition

 Esto es equivalente a tener un complejo exacto de la forma:

 \[ \dots \overset{}\longrightarrow 
 M_2 \overset{}\longrightarrow 
 M_1 \overset{}\longrightarrow 
 M_0 \overset{}\longrightarrow 
 A \longrightarrow
 0\]

**** 3.2. La categoría de los complejos
 #+begin_definition
 *Categoría de complejos de cocadenas*. La categoría $C(A)$ tiene como objetos
 los complejos de cocadenas en una categoría $A$; y como morfismos entre dos 
 cocadenas,   $Hom(M^\bullet,N^\bullet)$, los diagramas conmutativos entre ellas. Por ejemplo:

 \[ \begin{tikzcd}
 \dots \rar & M^{i-1} \rar\dar{\alpha^{i-1}} & M^{i} \rar\dar{\alpha^{i}} &  M^{i+1} \rar\dar{\alpha^{i+1}} & \dots \\
 \dots \rar & N^{i-1} \rar & N^{i} \rar & N^{i+1} \rar & \dots
 \end{tikzcd} \]

 representa el morfismo $\alpha_\bullet$.
 #+end_definition

 Esta es una categoría abeliana. De ella definiremos además dos variantes:

 - $C^+(A)$, subcategoría plena de los complejos acotados por debajo.
 - $C^-(A)$, subcategoría plena de los complejos acotados por arriba.
* The Catsters
** Adjunctions
Serie de [[https://www.youtube.com/playlist?list=PL54B49729E5102248][vídeos]] sobre funtores adjuntos.

*** Adjuntions 1
Tenemos varias nociones de igualdad entre categorías.

#+begin_definition
*Isomorfismo de categorías*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que $1_C = GF$ y $FG = 1_D$.
#+end_definition

#+begin_definition
*Equivalencia de categorías*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que $1_C \cong GF$ y $FG \cong 1_D$. Entendiendo la isomorfía en la 
categoría de funtores, es decir, una [[https://ncatlab.org/nlab/show/natural+isomorphism][isomorfía natural]].
#+end_definition

#+begin_definition
*Adjunción*. Ocurre con dos functores:

\[ \begin{tikzcd}
{\cal C} \arrow[bend left]{r}{F} & {\cal D} \arrow[bend left]{l}{G}
\end{tikzcd}
\]

Tales que tenemos transformaciones naturales $1_C \overset{\eta}\Longrightarrow GF$ y 
$FG \overset{\epsilon}\Longrightarrow 1_D$ que cumplen las dos identidades triangulares siguientes:
 
\[ \begin{tikzcd}
F \arrow{r}{\eta} \arrow{dr}{id} & FGF \arrow{d}{\epsilon} \\
 & F
\end{tikzcd}   
\]     \[ \begin{tikzcd}
G \arrow{r}{\eta} \arrow{dr}{id} & GFG \arrow{d}{\epsilon} \\
 & G
\end{tikzcd}
\]
#+end_definition

En este caso escribimos $F \dashv G$, y $F$ es funtor adjunto de $G$.

*** Adjuntions 2
Damos una definición equivalente de funtores adjuntos.

#+begin_definition
*Adjunción*. Una adjunción es un isomorfismo natural:

\[Hom_D(FX,Y) \cong Hom_C(X,GY)\]

Natural sobre $X$ fijado cualquier $Y$ y natural sobre $Y$ fijado 
cualquier $X$. Entendiendo que usamos los funtores contravariantes $Hom(F-,Y)$,
$Hom(-,GY)$ por un lado y los funtores covariantes $Hom(FX,-)$ y $Hom(X,G-)$;
que nos dan los siguientes cuadrados de naturalidad:

\[ \begin{tikzcd}
Hom_D(FX',Y) \arrow{d}[swap]{Hom_D(Ff,Y)} \arrow{r}{\alpha_{X'}} & Hom_C(X',GY) \arrow{d}{Hom_C(f,GY)}\\
Hom_D(FX, Y) \arrow{r}{\alpha_{X}}& Hom_C(X,GY)
\end{tikzcd}
\] 

\[ \begin{tikzcd}
Hom_D(FX,Y) \arrow{d}[swap]{Hom_D(FX,g)} \arrow{r}{\beta_{Y}} & Hom_C(X,GY) \arrow{d}{Hom_C(X,Gf)}\\
Hom_D(FX,Y') \arrow{r}{\beta_{Y'}}& Hom_C(X,GY')
\end{tikzcd}
\] 
#+end_definition

Esta definición es equivalente intuitivamente a la anterior porque podemos crear $\eta$ y $\epsilon$
desde las identidades usando las siguientes transformaciones naturales: 

\[Hom_D(FX,FX) \cong Hom_C(X,GFX)\]

\[Hom_D(FGY,Y) \cong Hom_C(GY,GY)\]

*** Adjuntions 3

Podemos presentar ejemplos de adjunciones.
Los *funtores libres y de olvido* suelen ser adjuntos. Entre $Set$ y $Monoid$ tenemos:

\[ \begin{tikzcd}
{Set} \arrow[bend left]{r}{Free} & {Monoid} \arrow[bend left]{l}{Forget}
\end{tikzcd}
\]

Con la adjunción $Free \dashv Forget$. 

#+begin_theorem
*Mónada de una adjunción*. Cada adjunción da lugar a una mónada.
#+end_theorem

Tenemos un funtor $T = GF : {\cal C}  \longrightarrow {\cal C}$. Podemos definir la unidad de
la mónada como la unidad de la adjunción $\eta : 1_C \Longrightarrow T$ y la
multiplicación podemos definirla usando $id \ast \epsilon \ast id : GFGF \Longrightarrow GF$.

Ahora debemos comprobar que cumple los axiomas de mónada. El primero
se obtiene directamente desde los triángulos de la adjunción:

\[ \begin{tikzcd}
T \arrow{r}{T\eta} \arrow{dr}{id} & T^2 \arrow{d}{\mu} \\
 & T
\end{tikzcd}   
\]   \[ \begin{tikzcd}
GF \arrow{r}{GF\eta} \arrow{dr}{id} & GFGF \arrow{d}{G \epsilon F} \\
 & GF
\end{tikzcd}   
\]

Donde el segundo es resultado de aplicar el funtor $G$ a uno de los triángulos conmutativos
de la adjunción. Comprobamos el segundo axioma:

\[ \begin{tikzcd}
T^2 \arrow{d}{\mu} & T \arrow{dl}{id} \arrow{l}[swap]{\eta T} \\
T
\end{tikzcd}   
\]   \[ \begin{tikzcd}
GFGF \arrow{d}{G \epsilon F} & GF \arrow{dl}{id} \arrow{l}[swap]{\eta GF} \\
GF
\end{tikzcd}   
\]

Donde tenemos el resultado de aplicar $F$ por la derecha al otro triángulo conmutativo.

Y finalmente el axioma de conmutatividad de la mónada se comprueba como:

\[ \begin{tikzcd}
T^3 \arrow{d}{T \mu} \arrow{r}{\mu T} & T^2 \arrow{d}{\mu} \\
T^2 \arrow{r}{\mu} & T
\end{tikzcd} \]  \[ \begin{tikzcd}
GFGFGF \arrow{d}{GFG \epsilon F} \arrow{r}{G \epsilon FGF} & GFGF \arrow{d}{G\epsilon F} \\
GFGF \arrow{r}{G \epsilon F} & GF
\end{tikzcd} \] 

Donde el segundo diagrama se obtiene desde la naturalidad de $\epsilon$ aplicando funtores.

*** Adjuntions 4
Vamos a probar la igualdad entre las dos definiciones de adjunción.
Supongamos primero que tenemos el isomorfismo natural entre los dos 
conjuntos de morfismos, es decir, tenemos:

\[ (-) : Hom_D(FX,Y) \cong Hom_C(X,GY) \]

Si tomamos ahora los dos cuadrados naturales que teníamos por este 
isomorfismo y tomamos en ellos los casos particulares $Y = FX$ primero,
y $X = GY$ después:


\[ \begin{tikzcd}
Hom_D(FX,FX) \arrow{d}[swap]{\_ \circ Ff} \arrow{r}{(-)} & Hom_C(X,GFX) \arrow{d}{\_\circ f}\\
Hom_D(FX', FX) \arrow{r}{(-)}& Hom_C(X',GFX)
\end{tikzcd}
\]

Si tomamos la identidad $1_{FX}$ y llamamos $\eta_X = \overline{1_{FX}}$, tenemos que
\(\eta \circ f = \overline{Ff}\). Ahora, si damos la vuelta al isomorfismo $(-)$ en este 
diagrama a la vez que hacemos $X = GY$:

\[ \begin{tikzcd}
Hom_D(FGY,Y) \arrow{d}[swap]{\_ \circ Ff}  & Hom_C(GY,GY) \arrow{l}[swap]{(-)} \arrow{d}{\_\circ f}\\
Hom_D(FGY',Y) & Hom_C(GY',GY) \arrow{l}[swap]{(-)}
\end{tikzcd}
\]

Volviendo a tomar la identidad $1_{GY}$ y llamando $\epsilon_Y = \overline{1_{GY}}$, tenemos
$\epsilon \circ Ff = \overline{f}$.

Ahora tomamos el segundo cuadrado natural, y repetimos el mismo
proceso.

\[ \begin{tikzcd}
Hom_D(FX,FX) \arrow{d}[swap]{g \circ \_} \arrow{r}{(-)} & Hom_C(X,GFX) \arrow{d}{Gg\circ \_}\\
Hom_D(FX,FX') \arrow{r}{(-)}& Hom_C(X,GFX')
\end{tikzcd}
\] 

Obteniendo desde la identidad en $FX$ la ecuación $\overline{g} = Gg \circ \eta$. Y volviendo
a dar la vuelta a los isomorfimos llegamos a:

\[ \begin{tikzcd}
Hom_D(FGY,Y) \arrow{d}[swap]{g \circ \_}  & Hom_C(GY,GY) \arrow{l}[swap]{(-)} \arrow{d}{Gg \circ \_}\\
Hom_D(FGY,Y') & \arrow{l}[swap]{(-)} Hom_C(GY,GY')
\end{tikzcd}
\]

Obteniendo finalmente $\overline{Gg} = g \circ \epsilon$. De este proceso hemos obtenido finalmente
las siguientes ecuaciones:

\[ \begin{aligned}
\eta \circ f &= \overline{Ff} \\
\epsilon \circ Ff &= \overline{f} \\
Gg \circ \eta &= \overline{g} \\
g \circ  \epsilon &= \overline{Gg} 
\end{aligned} \]

Con ellas podemos probar la naturalidad de $\eta$ y la naturalidad de
$\epsilon$:

\[ \begin{tikzcd}
GFX  \arrow{r}{GFf} & GFY \\
X \arrow{u}[swap]{\eta_X} \arrow{r}[swap]{f} & Y \arrow{u}{\eta_Y}
\end{tikzcd}
\]   \[ \begin{tikzcd}
FGX \arrow{d}[swap]{\epsilon_X} \arrow{r}{FGg} & FGY \arrow{d}{\epsilon_Y}\\
X \arrow{r}[swap]{g} & Y
\end{tikzcd}
\]

Ya que $\eta \circ f = \overline{Ff} = GFf \circ \eta$ y $f \circ \epsilon = \overline{Gf} = \epsilon \circ FGf$. Y además podemos probar
los dos triángulos de naturalidad.

\[ \begin{tikzcd}
F \arrow{r}{F \eta_X} \arrow{dr}{id} & FGF \arrow{d}{\epsilon_{FX}} \\
 & F
\end{tikzcd}   
\]     \[ \begin{tikzcd}
G \arrow{r}{\eta_{GX}} \arrow{dr}{id} & GFG \arrow{d}{G\epsilon_X} \\
 & G
\end{tikzcd}
\]

Teniendo finalmente que:


\[ \begin{aligned}
\epsilon \circ F\eta &= \overline{\eta} = 1 \\
G\epsilon \circ \eta &= \overline{\epsilon} = 1
\end{aligned} \]

El otro sentido de la demostración se tiene llegando primero a las cuatro ecuaciones,
y usándolas para definir el isomorfismo $(-)$. Falta entonces demostrar su naturalidad.
* Harpreet Bedi's channel
** Sheaves and coho
*** Preseaves and sheaves
**** Preseaf definition
#+begin_definition
*Preseaf*. A preseaf ${\cal F}$ of abelian groups on a topological space $X$ consists of:

- For each open set $U$, an abelian group ${\cal F}(U)$, whose elements are called *sections*.
- For each inclusion $V \subseteq U$, a *restriction map*, homomorphism of the form:
  
 
\[p_{U,V} : {\cal F}(U) \longrightarrow {\cal F}(V)\]

such that $p_{U,W} = p_{V,W} \circ p_{U,V}$.
#+end_definition

We can write the restriction of an element $u \in U$ to a set $V \subseteq U$ as
$u|_V = p_{U,V}(u)$.

**** Sheaf definition
 #+begin_definition
 *Gluability axiom*. Given $U = \bigcup U_i$ with sections $s_i \in {\cal F}(U_i)$, if we have:

 \[ s_\alpha|_{U_\alpha \cap U_\beta} = s_\beta|_{U_\alpha \cap U_\beta} \]

 then there exists $s \in {\cal F}(U)$ such that $s|_U_\alpha = s_\alpha$.
 #+end_definition
 #+begin_definition
 *Uniqueness axiom*. Given $U = \bigcup U_i$ with sections $s,t \in {\cal F}(U)$ such that:

 \[\forall U_\alpha:\ s|_U_\alpha = t|_U_\alpha\]

 then $s=t$.
 #+end_definition
 #+begin_definition
 *Sheaves*. A presheaf satisfiying gluability and uniqueness.
 #+end_definition
** Homological Algebra
*** 2. Chain Complex and Homology
*** 4. Homology Theorem
**** Setting
Given a SES of chain complexes $0 \longrightarrow {\cal A}
\longrightarrow{\cal B}
\longrightarrow{\cal C}
\longrightarrow 0$, we have a long exact
sequence like:

\[ \begin{tikzcd}
 & \dots\rar & H_{n+1}({\cal C}) \arrow[out=355,in=175,swap]{dll}{\delta_{n+1}} \\
H_{n}({\cal A})\rar & H_{n}({\cal B}) \rar & H_{n}({\cal C}) \arrow[out=355,in=175,swap]{dll}{\delta_n}\\
H_{n-1}({\cal A})\rar & \dots & 
\end{tikzcd} \]

**** Naturality
When we have two SES of chain complexes:

\[ \begin{tikzcd}
0 \rar & {\cal A}\rar\dar & {\cal B}\rar\dar & {\cal C}\rar\dar & 0 \\
0 \rar & {\cal A}'\rar & {\cal B}'\rar & {\cal C}'\rar & 0 \\
\end{tikzcd} \]

where it hols for every $n$ that:

\[ \begin{tikzcd}
H_n({\cal C}) \rar\dar & H_{n-1}({\cal A})\dar \\
H_n({\cal C}') \rar & H_{n-1}({\cal A}')
\end{tikzcd} \]

*** 8. Proj, inj and flat modules
**** Definitions
An $R$-module $D$ is:

 1. *Projective* if $Hom(D, -)$ is exact.
 2. *Injective* if $Hom(-,D)$ is exact.
 3. *Flat* if $D \otimes -$ is exact.

**** Considerations
We know that $Hom(D,-)$ and $Hom(-,D)$ are left-exact and that
$D\otimes -$ is right-exact; so for them to be exact, we only need:

 - A module $D$ is *projective* when $B \longrightarrow C$ surjective induces
   $Hom(D,B) \longrightarrow Hom(D,C)$ surjective.

   \[ \begin{tikzcd}
               & B \dar[two heads] \\
   D \rar\urar[dashed]{\exists} & C
   \end{tikzcd} \]

 - A module $D$ is *injective* when $A \longrightarrow B$ surjective induces
   $Hom(B,D) \longrightarrow Hom(A,D)$ surjective.

   \[ \begin{tikzcd}
     & A \dar[two heads]\dlar \\
   D & B \lar[dashed]{\exists}
   \end{tikzcd} \]

 - A module $D$ is *flat* when $A \longrightarrow B$ injective induces 
   $D\otimes A \longrightarrow D \otimes B$ injective.

*** 9. Resolutions: projective, injective and flat
**** Definitions
***** Resolutions
Resolutions are *exact sequences*.

***** Projective resolution
A resolution, with $d_i$ maps:

\[\dots\longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

where $P_i$ is projective.

***** Injective resolution
A resolution:

\[0 \longrightarrow M \longrightarrow E_0\longrightarrow E_1
\longrightarrow E_2 \longrightarrow \dots\]

where $E_i$ is injective.

***** Flat resolution
A resolution:

\[\dots\longrightarrow F_2\longrightarrow F_1\longrightarrow F_0
\longrightarrow M \longrightarrow 0\]

where $F_i$ is flat.

**** How to form a resolution
It is important to notice that, given a module $M$, you can always find a surjection
from a proyective module (we have /enough projectives/). So we can construct a
projective resolution as follows:

\[ \begin{tikzcd}
&\ker f_2 \drar&&&&\ker \pi\drar &&& \\
\dots&&P_2 \drar[two heads]{f_2}&&P_1 \urar[two heads]{f_1} && P_0 \rar[two heads]{\pi} & M \rar & 0\\
&&&\ker f_1 \urar&&&&
\end{tikzcd} \]

We can reverse the arrows to obtain an injective resolution.

*** TODO 10. Homotopic projective resolutions
**** Extending a morphism
Given two projective resolutions of two $R$ modules, $A$ and $A'$, and a morphism
between them, $f$. We can extend it to $f_n \in Hom(P_n,P_n')$.

\[ \begin{tikzcd}
\dots\rar & P_{n+1}\rar & P_n\rar& \dots
 \rar & P_1\rar{d_1} & P_0\rar{d_0}& A \dar{f} \rar& 0 \\
\dots\rar & P_{n+1}'\rar & P_n'\rar&\dots
 \rar & P_1'\rar{d_1¡} & P_0'\rar{d_0'}& A' \rar& 0 \\
\end{tikzcd} \]

**** Extending the morphism, base case
We use that $P_0$ is projective to construct:

\[ \begin{tikzcd}
     & P_0 \arrow[ddl,"f_0",dashed,swap] \dar\\
     & A \dar{f} \\
P_0' \rar[two heads] & A'
\end{tikzcd} \]

**** Extending the morphism, inductive case
We are going to show that $f_n(\im d_{n+1}) \subset \im d_{n+1}' = \ker d_n'$. That is, 
$d_n' \circ f_n \circ d_{n+1} = 0$. And that follows from diagram chasing. We use
again the projectivity of $P_{n+1}$.

\[ \begin{tikzcd}
     & P_{n_+1} \arrow[ddl,"f_{n+1}",dashed,swap] \dar\\
     & \im d_{n+1} \dar{f_n} \\
P_{n+1}' \rar[two heads] & \im d_{n+1}'
\end{tikzcd} \]


**** TODO Homotopic resolutions
*** 11. Derived functors Ext and Tor
**** Right derived functors
Let $F$ be additive, covariant and left-exact. Let 
$0 \longrightarrow M \longrightarrow E^\bullet$ be an injective resolution with $M$ deleted; then $F(E^\bullet)$ is a complex,
and we define:

\[R^i F(M) = H^i(F(E^\bullet)) = 
\frac{\ker \{F(E_i) \longrightarrow F(E_{i+1})\}}
{\im\{ F(E_{i-1}) \longrightarrow F(E_i)\}}\]

That is, if we take the injective resolution:

\[ 0 \longrightarrow M \longrightarrow E_0 \longrightarrow E_1 
\longrightarrow \dots\]

Delete $M$ and apply $F$ to get a (non neccesarily exact) complex where we can compute 
the homology:

\[ 0 \longrightarrow F(E_0) \longrightarrow F(E_1)
\longrightarrow F(E_2) \longrightarrow \dots\]

**** Left derived functors
Let $F$ be additive, contravariant and left-exact. Let 
$P^\bullet \longrightarrow M \longrightarrow 0$ be a projective resolution with $M$ deleted; then $F(E^\bullet)$ is a complex,
and we define:

\[R^i F(M) = H^i(F(P^\bullet)) = 
\frac{\ker \{F(P_i) \longrightarrow F(P_{i+1})\}}
{\im\{ F(P_{i-1}) \longrightarrow F(P_i)\}}\]

That is, if we take the injective resolution:

\[\dots \longrightarrow P_2\longrightarrow P_1\longrightarrow P_0
\longrightarrow M \longrightarrow 0\]

Delete $M$ and apply $F$ to get a (non neccesarily exact) complex where we can compute 
the homology:

\[ 0 \longrightarrow F(P_0) \longrightarrow F(P_1)
\longrightarrow F(P_2) \longrightarrow \dots\]
* Jara - Apuntes
** XIII. Homología de Hochschild
*** 3. Cohomología de Hochschild
**** Resolución proyectiva
***** R;R módulo
      Sea $R$ una $K\text{-álgebra}$; un $(R;R)$ *módulo* es un $R$ módulo a izquierda y derecha 
      verificando la *relación de compatibilidad*:

      \[r_1(mr_2) = (r_1m)r_2\]

      En particular, se tiene,

      \[km = mk \quad \forall k \in K\]

      Un *homomorfismo de R;R-módulos* es un homomorfismo de R-módulos a izquierda y
      *R-módulos* a derecha. Forman la categoría $(R;R)\mathtt{-Mod}$.

***** Álgebra envolvente
Sea $R$ una $K\text{-álgebra}$, llamamos *álgebra envolvente* a $R^e = R \otimes R^{op}$. Con el producto:

\[ (r_1 \otimes s_1)(r_2 \otimes s_2) = (r_1r_2) \otimes (s_2s_1)\]

***** Caracterización de R;R-módulos
Para cada $K\text{-álgebra}$, $R$, las categorías siguientes son isomorfas:

  - $(R;R)\text{-Mod}$
  - $R^e\text{-Mod}$
  - $\text{Mod-}R^e$

**** Cohomología de Hochschild
***** Definición 
Sea $R$ una $K\text{-álgebra}$ y $M$ un $(R;R)\text{-módulo}$, llamamos:

  - *cohomología de Hochschild* de $R$ en $M$ a 
    $HH^{\bullet}(R,M) = \operatorname{Ext}^\bullet_{R^e}(R,M)$.
  - *homología de Hochschild* de $R$ en $M$ a 
    $HH_{\bullet}(R,M) = \operatorname{Tor}_\bullet^{R^e}(R,M)$.
* Pierce - Associative Algebras
** 10. Separable Algebras
*** 10.1. Bimodules
**** Opposite algebra
If $A$ is an R-algebra, the *opposite algebra* of $A$ is $A^\ast$; where
multiplication is defined as $x \circ y = yx$.

**** Enveloping algebra
The *enveloping algebra* of an R-algebra is:

\[ A^e = A^\ast \otimes A\]
* Rotman - An introduction to homological algebra
** 1. Introduction
*** 1.1. Simplicial Homology
**** Motivation: Green's Theorem
***** Original statement
Let $C$ be a positively oriented, smooth and simple closed curve in
a plane; being $D$ the region bounded by $C$. If $L,M$ have continuous
partial derivatives in $D$, then:

\[ \oint_C (L dx + M dy) = 
\iint_D \left(
  \frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}
\right) dx dy\]

***** A rewrite
If we have some "bad points" that we want to delete from $C$.
We can define multiple $\gamma_i$ around them and have our integral to be:

\[ \oint_C (L dx + M dy) +
\sum^n_{i=1} \left( \int_{\gamma_i} L dx + Q dy \right) 
= \iint_D \left(
  \frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}
\right) dx dy\]

As the diagram is:

[[./images/greentheorem.png]]

In this setting, the notion of $\mathbb{Z}$ linear combinations of paths
makes sense. We can take the free abelian group $G[Y]$ with $Y$ being
the set of paths $\gamma : [0,1] \longrightarrow X$.

***** An equivalence relation
For functions satisfying $\frac{\partial Q}{\partial x} = \frac{\partial P}{\partial y}$, the double integral dissapears,
and we have:

\[ \int_{m\gamma + \sum_i m_i\gamma_i} P dx + Q dy = 0\]

Here we can define an equivalence relation between pairs of paths,
where $\beta \sim \beta'$ if:

\[ \int_\beta P dx + Q dy = \int_{\beta'} P dx + Q dy \]

The equivalence class of $\beta$ is called its *homology class*.

**** Boundaries
If we take the simplices to form abelian groups, the boundaries
are homomorphisms.

[[./images/rectangle.png]]

For instance, if we can take this rectangle and compute its boundary.
We use free abelian groups of $n\text{-simplexes}$, called $C_n(X)$.

***** Boundary of a triangle
We use the minus sign to denote the inverse path, and we have:

\[ \delta([a,b,c]) = [a,b] + [b,c] - [a,c]\]

***** Boundary of the boundary of a triangle
As the double boundary is the boundary of a sphere, it is 
automatically null:

\[
\delta(\delta([a,b,c])) = (a - b) + (b - c) - (a - c) = 0
\]

***** Boundary of the rectangle
Now, we can compute the boundary of the rectangle; assuming that
the boundary function is a homomorphism preserving the union:

\[\begin{aligned}
\delta(\square) &=  \delta[a,b,c] + \delta[a,c,d] \\ 
&= [a,b]+[b,c]-[a,c]+[a,c]+[c,d]-[a,d] \\
&= [a,b]+[b,c]+[c,d]-[a,d]
\end{aligned}\]

**** Simplicial boundary maps
Let $X$ be a finite simplicial complex. We define:

\[ \delta_n [v_0,\dots,v_n] 
= \sum^n_{i=0} (-1)^i [v_0,\dots,\hat{v_i},\dots,v_n]\]

being a map from $C_n(X)$ to $C_{n-1}(X)$. We define $\delta_0 = 0$ as a convention.

**** Boundary maps are exact
For all $n > 0$, 

\[\delta_{n-1}\delta_n = 0\]

***** Proof
We can see that, for every pair of indexes, we have the same term 
twice, depending on whether we take the two indexes ordered or using
an inverse order:

\[ 
[x_0,\dots,\hat{x_i},\dots,\hat{x_j},\dots,x_n](-1)^{i+(j-1)} +
[x_0,\dots,\hat{x_i},\dots,\hat{x_j},\dots,x_n](-1)^{j+i} = 0
\]

**** Simplicial cycles and boundaries
The elements in $Z_n(X) = \ker \delta_n \subset C_n(X)$ are called *simplicial cycles*.
The elements in $B_n(X) = \im \delta_{n+1} \subset C_n(X)$ are called *simplicial 
boundaries*.

**** Exactness for cycles and boundaries
For all $n$,

\[ B_n(X) \subseteq Z_n(X)\]

***** Proof
It is trivial knowing that boundary maps are [[*Boundary maps are exact][exact]].

**** Simplicial homology group
The nth simplicial homology group of a finite simplicial complex is:

\[ H_n(X) = Z_n(X) / B_n(X) \]

What survives in this group are the cycles that are not boundaries;
that is, the boundaries of punctured sections.

**** Two modifications
We can consider *homology* with coefficients in $G$ by tensoring the
sequence of chain groups by $G$ and taking homology groups. We can
consider the *cohomology* with coefficients in $G$ applying $Hom(-,G)$
to the chain of groups and then taking homology groups.

*** 1.2. Categories and Functors
**** 1.2.1. Russell's paradox
The Russell paradox is solved in with the Zermelo-Fraenkel axioms,
specifically, the *axiom of comprehension*. It says that any definable
subclass of a set is a set; restricting the comprehension to only
already defined sets.

**** 1.2.2. Classes and sets
A class in ZFC is called *small* if it has a cardinal number. A *set*
is only a small class. In this book, we only worry about classes and
sets that are not a member of themselves.

# A cardinal number?
# More details in Mac Lane, Categories for the working mathematician.

**** 1.2.3. Categories
A category ${\cal C}$ consists of:

 - $obj({\cal C})$, a class of objects.
 - $Hom(A,B)$, a set of morphisms for every ordered pair $(A,B)$.
 - $\circ : Hom(A,B) \times Hom(B,C) \longrightarrow Hom(A,C)$, composition of functions.

**** 1.2.4. Axioms of categories
A category has disjoint $Hom$ sets, and there must be an identity element
$1_A \in Hom(A,A)$ for every morphism, following these rules:

 - The identity is a neutral element: $f \circ 1_A = f$ and $1_B \circ f = f$.
 - Composition is associative: $f \circ (g \circ h) = (f \circ g) \circ h$

**** 1.2.5. Examples of categories
***** Sets
***** Groups
***** Partially ordered sets
***** Inclusion of open sets
***** Topological spaces
***** Abstract simplicial complexes
****** Abstract simplicial complexes
We denote =Abs= the category of abstract simplicial complexes.
An abstract simplicial complex $K$ is a set of *vertices* $Vert(K)$ and
a family of nonempty finite subsets called *simplexes* 
$\sigma \subseteq Vert(K)$ such that:

 1. $\{v\}$ is a simplex for every $v \in Vert(K)$.
 2. Every subset of a simplex is a simplex.

****** Simplicial maps
A *simplicial map* is a function $\phi : Vert(K) \longrightarrow Vert(L)$ 
such that, if $\sigma$ is a simplex in $K$, then $\phi(\sigma)$ is a simplex
in $L$.

****** Dimension
A simplex with $|\sigma| = n+1$ is called a *n-simplex*. Simplicial
maps don't have to preserve dimension.

***** Nerves
If ${\cal U} = \{U\}_{i\in I}$ is the cover of a topological space, we define an 
abstract simplicial complex ${\cal N}({\cal U})$ having vertices $Vert({\cal N}({\cal U})) = {\cal U}$
and simplexes $\{U_0,\dots,U_n\} \subseteq {\cal U}$ such that:

\[ \bigcap_{k=0}^n U_k = \varnothing\]

***** Monoids
***** Homotopy category
**** 1.2.6. Algebraic examples of categories
***** Abelian groups
***** Rings (unital)
***** Commutative rings
**** 1.2.7. Modules
A left R-module, where $R$ is a ring, is an additive abelian group $M$
with a scalar multiplication $R \times M \longrightarrow M$, such that:

 1. $r(m+m') = rm+rm'$
 2. $(r+r')m = rm + r'm$
 3. $(rr')m = r(r'm)$
 4. $1m = m$

A right module is defined anagously.

**** 1.2.8. Examples of modules
***** Vector spaces over a field
***** Abelian groups over Z
***** Every ring over itself
***** Every ring over its center

**** 1.2.9. Homomorphisms of R-modules
A function $f : M \longrightarrow N$ such that:

 1. $f(m+m') = f(m)+f(m')$
 2. $f(rm) = rf(m)$

In the case of right modules, we can define them anagously.

***** The composite and inverse of homomorphisms is an homomorphism
Trivial.

**** 1.2.10. Examples of homomorphisms
***** Linear transformations in vector spaces
***** Homomorphisms of abelian groups for Z-modules
***** Homothety
Let $M$ be an R-module, and $r \in Z(R)$; multiplication by $r$, $\mu_r$, is
an homomorphism because:

\[ \mu_r(am) = r(am) = a(rm) = a\mu_r(m)\]

**** 1.2.11. Opposite rings
If $R$ is a ring, its opposite ring $R^{op}$ is the same ring with the
opposite multiplication, defined by:

\[ \mu^o(r,t) = \mu(t,r)\]

**** 1.2.12. Categories of modules
We call $_RMod$ the category of *left* R-modules, and $Mod_R$ to the 
category of *right* R-modules.

**** 1.2.13. Subcategories
A category ${\cal S}$ is a subcategory of ${\cal C}$ when:

 1. $obj({\cal S}) \subseteq obj({\cal C})$.
 2. $Hom_S(A,B) \subseteq Hom_C(A,B)$.
 3. Identities and compositions are the same.

**** 1.2.14. Full subcategories
A full subcategory has $Hom_S(A,B) = Hom_C(A,B)$ for every $A,B \in obj({\cal S})$.

**** 1.2.15. Functors
A functor $T : {\cal C} \longrightarrow {\cal D}$ is a function such that:

 1. $T : obj({\cal C}) \longrightarrow obj({\cal D})$.
 2. $T : Hom(A,B) \longrightarrow Hom(TA,TB)$.
 3. Preserves composition: $T(f \circ g) = Tf \circ Tg$.
 4. Preserves identities: $T(1_A) = 1_{T(A)}$.

**** 1.2.16. Examples of functors
***** Subcategories as inclusion functors
***** Identity functor
***** Hom(A,-) functor
***** Chains as functors from the partially ordered integers
***** Forgetful functors

**** 1.2.27. Diagrams
A diagram is a functor whose domain is a *small category*; that
is $T : {\cal D} \longrightarrow {\cal C}$, where $obj({\cal D})$ is a set.

**** 1.2.28. Paths
A path is a functor $P : n+1 \longrightarrow {\cal C}$, where the domain is the partial 
ordering of integers $0,\dots,n+1$. A path is *simple* if the functor is
injective.

**** 1.2.29. Commutativity of diagrams
A diagram commutes if the composites of the labels on any two simple path
are equal.

**** TODO 1.2.30. Contravariant functors
**** TODO 1.2.31. Examples of contravariant functors
***** Hom(-,B) functor
***** The dual space functor
\[( )^\ast = Hom_k(-,k) : \sideset{_k}{}{Mod} \longrightarrow \sideset{_k}{}{Mod}\]
***** Order-reversing functions on partially ordered sets

***** Presheaves
If ${\cal U}$ is a topology with the inclusion, a contravariant functor 
${\cal P} : {\cal U} \longrightarrow {\cal C}$ is a presheaf.

**** 1.2.32. Faithful functors
A functor is faithful if all the functions 
$Hom(A,B) \longrightarrow Hom(TA,TB)$ are injective.
**** 1.2.33. Concrete categories
A category is concrete if there is a faithful functor ${\cal C} \longrightarrow \mathtt{Set}$.

**** 1.2.33. Opposite category
We define ${\cal C}^{op}$ to be the category with:

 - $obj({\cal C}^{op}) = obj({\cal C})$
 - $Hom_{{\cal C}^{op}}(A,B) = Hom_{\cal C}(B,A)$
 - $g \circ_{op} f = f \circ g$

**** 1.2.34. Isomorphisms
A morphism $f : A \longrightarrow B$ such that exists $g : B \longrightarrow A$ with
$f \circ g = 1$ and $g \circ f = 1$.

**** 1.2.35. Functors preserve isomorphisms
Let $T$ be a functor, if $f$ is an isomorphism, then $T(f)$ is an isomorphism.

***** Proof
If $g$ is its inverse, then:

\[ T(f)T(g) = T(fg) = 1\]
\[ T(g)T(f) = T(gf) = 1\]

If $T$ is a contravariant functor, the proof remains the same.

**** 1.2.36. Natural transformations
Let $F,G : {\cal A} \longrightarrow {\cal B}$ be covariant functors. A natural transformation
$\tau : F \Longrightarrow G$ is a family of morphisms $\tau_A : S A \longrightarrow T A$, making the following
diagram commute for every $f \in Hom(A,B)$:

\[ \begin{tikzcd}
FA \rar{\tau_A} \dar{Ff} & GA \dar{Gf} \\
FB \rar{\tau_B} & GB
\end{tikzcd} \]

**** 1.2.37. Natural isomorphisms
A natural transformation $\tau$ for which each $\tau_A$ is an isomorphism.

**** 1.2.38. Composition of natural transformations
If $\tau : F \Longrightarrow G$ and $\sigma : G \Longrightarrow H$ are natural transformations, then the
composition is a natural transformation.

***** Proof
Composing the two commutative diagrams gives us the proof.

**** 1.2.39. Identity natural transformation
For any functor $F : {\cal A} \longrightarrow {\cal B}$, we can describe an identity natural 
transformation using the identity morphisms.

**** TODO 1.2.40. Examples of natural transformations
**** TODO 1.2.41. Natural transformations are proper classes
**** TODO 1.2.42. Yoneda Lemma
**** TODO 1.2.43. Representable functors
**** TODO Examples
**** TODO Yoneda Imbedding
*** 1.3. Singular Homology
**** 1.3.1. Hilbert spaces and euclidean spaces
A *Hilbert space* is the set ${\cal H}$ of all sequences $(x_i) \in \mathbb{R}$ such that
$\sum x_i^2 < \infty$. A *Euclidean space*, $\mathbb{R}^n$ is a subset of ${\cal H}$ consisting of
all sequences of the form $(x_0,x_1,\dots,x_{n-1},0,\dots)$.

**** 1.3.2. Standard n-simplex
The standard n-simplex is the set of all convex combinations:

\[\Delta^n = [e_0,e_1,\dots,e_n]\]

Where $e_i$ form an orthogonal basis.

**** 1.3.3. Singular n-simplex
Given a topological space $X$, a singular n-simplex is a continuous map
$\sigma : \Delta^n \longrightarrow X$.

**** 1.3.4. Singular n-chains
We define $S_n(X)$ as the free group with singular n-simplexes as basis.
By convention, $S_{-1}(X) = \{0\}$. The elements on this group are called
singular n-chains.

**** TODO 1.3.5. Face maps
The ith face map $\epsilon^n_i : \Delta^{n-1} \longrightarrow \Delta^n$ is defined by:



** 5. Setting the stage
*** 5.4. Sheaves
**** Protosheaves
 #+begin_definition
 *Local homeomorphism*. Continuous map $p : E \longrightarrow X$ such that for each $e \in E$ there is
 an open neighboorhood $S$ of $e$ such that $p|_S$ is an isomorphism.
 #+end_definition
 #+begin_definition
 *Protosheaf*. Surjective local homeomorphism.
 #+end_definition

**** Etale-sheaves
 #+begin_definition
 *Etale-sheaf of abelian groups*. A *protosheaf* such that:

 - The stalk $E_x$ is an abelian group.
 - Inversion and adition are continuous.
 #+end_definition

 #+begin_definition
 *Etale-map*. Given two etale-sheaves $E$ and $E'$, a map $\phi : E \longrightarrow E'$ such
 that $p'\phi = p$, and each $\phi|_{E_x}$ is a homomorphism.
 #+end_definition

 Here, etale-sheaves of abelian groups over a topological space X form an
 abelian category $\mathtt{Sh}_{et}(X,\mathtt{Ab})$.

*** 5.5. Abelian categories
**** Additive category
 #+begin_definition
 *Additive category*. ${\cal C}$ is additive if:

 - $Hom(A,B)$ is an *abelian group*.
 - *Distributivity* holds: $b \circ (f+g) = b\circ f + b \circ g$ and $(f+g)\circ a = f\circ a + g\circ a$.
 - Has a *zero object*.
 - Has finite *products* and *coproducts*.

 A functor $T$ between two additive categories is additive if $T(f+g) = Tf+Tg$.
 #+end_definition

 #+begin_theorem
 *Sums and products are the same*. Products and coproducts are isomorphic:

 \[A \mathbin{\Pi} B \cong A \amalg B\]

 So we call them *direct sums*, $A \oplus B$. And there are canonical morphisms:

 \[ \begin{tikzcd}
 & A \oplus B \dlar[bend right,swap]{\pi_A} \drar[bend left]{\pi_B} $ \\
 A \urar[bend right,swap]{i_A} & & B \ular[bend left]{i_B}
 \end{tikzcd} \]

 Such that: \(i_A \circ \pi_A + i_b \circ \pi_B = id\) and \(\pi_B \circ i_A = \pi_A \circ i_B = 0\).
 #+end_theorem

**** Monomorphisms and epimorphisms
 #+begin_definition
 *Monomorphism*. A morphism $u$ such that:
 \[u \circ f = u \circ g \quad \Rightarrow \quad f = g\]
 #+end_definition
 #+begin_definition
 *Epimorphism*. A morphism $u$ such that:
 \[f \circ u = g \circ u \quad \Rightarrow \quad f = g\]
 #+end_definition

 We have that $u : B \longrightarrow C$ is *monomorphism* iff the induced 
 $u^\ast : Hom(A,B) \longrightarrow Hom(A,C)$ is injective. And $v : B \longrightarrow C$ is *epimorphism* 
 iff the induced $v^* : Hom(B,D) \longrightarrow Hom(C,D)$ is surjective.

**** Kernels and cokernels
 #+begin_definition
 *Kernel*. The kernel of $u$ is the equalizer of $u$ and $0$. In a diagram:

 \[ \begin{tikzcd}
 & C \dar[dashed] \arrow[ddr, bend left] \arrow[ddl,bend right] &\\
 & \ker(u) \dlar[swap]{i} \drar{0} & \\
 A \arrow[shift left]{rr}{u} \arrow[shift right]{rr}[swap]{0} & & B
 \end{tikzcd} \]
 #+end_definition
 #+begin_definition
 *Cokernel*. The cokernel of $u$ is the coequalizer of $u$ ans $0$. In a diagram

 \[ \begin{tikzcd}
 & C &\\
 & \ker(u) \uar[dashed]   & \\
 A \urar{0} \arrow[uur, bend left]
 \arrow[shift left]{rr}{u} \arrow[shift right]{rr}[swap]{0} & & 
 B \ular[swap]{\pi} \arrow[uul,bend right]
 \end{tikzcd} \]
 #+end_definition

 #+begin_theorem
 *Monomorphisms and kernels*.
 - If $\ker(u)$ exists, $u$ is monomorphism iff $ker(u) = 0$.
 - If $coker(v)$ exists, $v$ is epimorphism iff $coker(v) = 0$.
 #+end_theorem
**** Abelian category
 #+begin_definition
 *Abelian category*. ${\cal C}$ is abelian if

 - Every morphism has *kernel* and *cokernel*.
 - Every monomorphism is a *kernel*.
 - Every epimorphism is a *cokernel*.
 #+end_definition

 Abelian categories are /self-dual/, if ${\cal A}$ is an abelian category, then
 ${\cal A}^{op}$ is an abelian category.

 #+begin_definition
 *Image*. Given $f : A \longrightarrow B$ in an abelian category, its image is:

 \[img(f) = ker(coker(f))\]
 #+end_definition

* Carlos Ivorra - Álgebra conmutativa
** I. Funtores Derivados
*** 1.1. Haces
**** Prehaces
Un *prehaz* sobre un espacio topológico $X$ es un par $({\cal F},\rho)$, donde cada abierto $U$
tiene un grupo asociado ${\cal F}(U)$ y cada inclusión $U \subset V$ tiene asociado un homomorfismo
llamado *restricción*, $\rho_U^V : {\cal F}(V) \longrightarrow {\cal F}(U)$ cumpliendo:

  - ${\cal F}(\varnothing) = 0$
  - $\rho_U^U$ es la identidad
  - Si $U\subset V\subset W$, entonces $\rho_V^W \circ \rho_U^V = \rho_U^W$

Cuando los grupos ${\cal F}(U)$ son anillos o módulos tenemos un *prehaz de anillos* o un
*prehax de módulos*.

# Categóricamente, un funtor contravariante desde los conjuntos del espacio
# topológico con la inclusión a los grupos, o módulos, o álgebras...

**** Notación de restricción
Normalmente escribiremos $f|_{U}$ para llamar a la restricción de $f$ a $U$, esto 
es $\rho_U^V(f)$.

**** Haces
Un *haz* es un prehaz tal que si $U = \bigcup U_i$ es el recubrimiento de un abierto:

  - Si $f|_{U_i} = 0$ para todos los $i$, entonces $f = 0$.
  - Para una familia de elementos $f_i \in {\cal F}(U_i)$ cumpliendo que 
    $f_i|_{U_i \cap U_j} = f_j|_{U_i \cap U_j}$, se tiene que hay un $f \in {\cal F}(U)$ tal que $f|_{U_i} = f_i$.

**** Grupo de gérmenes o grupo local
Dado un prehaz ${\cal F}$ sobre $X$, con $P \in X$, llamamos *grupo de gérmenes* en $P$ al grupo
${\cal F}_P$, formado por las clases de equivalencia de pares $(U,f)$ con $P\in U$, $f \in {\cal F}(U)$;
respecto de la relación dada por $(U,f) \sim (V,g)$ ssi hay un abierto $W \subset U \cap V$
tal que $P \in W$ y además $f|_W = g|_W$. Teniendo como operación de grupo a:

\[ [(U,f)]+[(V,g)] = [(U\cap V, f|_{U\cap V} + g|_{U\cap V})] \]

**** Homomorfismo de prehaces
Un *homomorfismo de prehaces* $\alpha : {\cal F} \longrightarrow {\cal G}$, asigna a cada abierto $U$ un homomorfismo
de grupos $\alpha_U : {\cal F}(U) \longrightarrow {\cal G}(U)$, tal que:

\[ \begin{tikzcd}
{\cal F}(V) \rar{\alpha_V} \dar[swap]{\rho_U^V} & {\cal G}(V) \dar{\rho_U^V} \\
{\cal F}(U) \rar{\alpha_U} & {\cal G}(U)
\end{tikzcd} \]

# Categóricamente son transformaciones naturales.
* Álgebra III
** 1. Polinomios simétricos
*** Motivación: La cúbica
**** Polinomios cúbicos
Toda ecuación cúbica polinómica puede escribirse en la forma
\(Y^3 + pY + q\), tomando un cambio de variable desde la original
\(X \mapsto - \frac{1}{3} b\). Esto se llama una cúbica deprimida.

***** Método de Vieta
El método de Vieta toma \(t = w - \frac{p}{3w}\), y llega a la ecuación:

\[w^3 + q - \frac{p^3}{27w^3} = 0\]

Ahora podemos resolver esa cuadrática y resolver luego la ecuación
en $w^3$.

*** 1.1. Polinómios simétricos
**** Polinomios simétricos
Un *polinomio simétrico* es aquel invariante por $f_\sigma$ para cualquier $\sigma \in S_r$, 
donde $f_\sigma (X_i) = X_{\sigma i}$. Llamamos $Sim(A[X_1\dots X_n])$ al subanillo de polinomios 
simétricos.

**** Componentes homogéneas
Llamamos *componente homogénea* a cada sumando homogéneo maximal de un 
polinomio. Un polinomio es simétrico si y sólo si cada una de sus componentes 
lo es.

**** Polinomios simétricos elementales
Los polinomios simétricos elementales son aquellos de la forma:

\[e_i = \sum_{i_1 < \dots < i_i} X_{i1} X_{i2} \dots X_{ii}\]

**** Teorema fundamental de los polinomios simétricos
Los polinomios elementales generan cada polinomio $Sim(A[X_1\dots X_n])$ 
de forma única. En particular,

\[\omega : A[X_1,\dots,X_r] \longrightarrow Sim(A[X_1,\dots,X_r])\]

con $\omega(a) = a$ y $\omega(X_i) = e_i$ es un isomorfismo.

***** Demostración
Damos una relación de orden lexicográfica entre los monomios de un
polinomio simétrico homogéneo. Al mayor de ellos, llamado 
$X_1^{k_1} \dots X_r^{k_r}$ le restamos $e^{b1}_1 e^{b2}_2 \dots e^{br}_r$, donde
$b_i = k_i - k_{i+1}$. Nos quedará $0$ u otro polinomio simétrico de
igual grado pero menor en el orden lexicográfico. Este proceso debe
ser finito.

La unicidad se obtiene con $0 = h(e_1\dots e_r) - k(e_1\dots e_r) =
l(e_1 \dots e_r)$.

*** 1.2. Polinomios alternados
**** Polinomio alternado
Un polinomio $f$ es alternado cuando para toda permutación se tiene
$\sigma(f) = sign(\sigma) f$.

*** 1.3. La resultante
**** Resultante
Dados dos polinomios $f,g$ en un cuerpo $K$ en el que descomponen 
podemos escribirlos como:

\[ f = a_n(X-\alpha_1)\dots(X-\alpha_n) = a_n \prod^n_{i=1}(X-\alpha_i)\]
\[ g = b_n(X-\beta_1)\dots(X-\beta_n) = a_n \prod^n_{i=1}(X-\beta_i)\]

La *resultante* busca ser una expresión que se anula cuando tienen
raíz común, y se define como:

\[ R(f,g) = a_n^m b_m^n \prod^n_{i=1} \prod^m_{j=1} (\alpha_i - \beta_j)\]

**** TODO Propiedades de la resultante
La resultante de dos polinomios $f,g$ cumple:

 1. $R(f,g) = 0$ ssi tienen una raíz común.
 2. $R(g,f) = (-1)^{nm}R(f,g)$, siendo $nm$ el producto del número de raíces.
 3. $R(f,g) = a^m_n \prod^n_{i=1} g(\alpha_i)$
 4. $R(fg,h) = R(f,h)R(g,h)$, $R(f,gh) = R(f,g)R(f,h)$
 5. Si $m=0$, entonces $R(f,k) = k^n$
 6. $R(X^k,f) = a_0^k$; con $R(f,X^k) = (-1)^{nk}a_0^k$

*** 1.4. Discriminante
**** Raíces múltiples
Podemos usar la resultante para caracterizar los polinomios
con raíces múltiples, que son aquellos que comparten raíz con
su derivada.
# Duda: ¿Una raíz doble se comparte con la derivada siempre?

\[ R(f,f') = a_n^{n-1} \prod f'(\alpha_j)\]

**** El discriminante
El *discriminante* de un polinomio con raíces
$\alpha_1, \dots, \alpha_n$ en una clausura algebraica es:

\[\text{Discr}(p) = a^{2n-2} \prod_{i>j}(\alpha_i-\alpha_j)^2\]

**** Relación con la resultante
\[R(p,p') = (-1)^{\frac{n(n-1)}{2}}a_n \text{Discr}(p)\]

*** 1.5. Métodos de cálculo
**** Método modular
**** Por el algoritmo de Euclides
**** Resultante de Euler-Sylvester-Cayley
Definimos la resultante de Euler-Sylvester-Cayley:

\[
R(f,g) = \left| \begin{matrix}
a_n & a_{n-1} & \dots & a_0 & 0 & \dots &\\
0   & a_n & \dots & a_{1} & a_0 & 0 & \dots \\
0   &   0 & a_n & \dots & a_1 & a_0 & \dots \\
&      &     &\vdots & & & \\
b_m & b_{m-1} & \dots & b_0 & 0 & \dots &\\
0   & b_m & \dots & b_1 & b_0 & 0 & \dots \\
0   &   0 & b_m & \dots & b_1 & b_0 & \dots \\
\end{matrix} \right|
\]

***** Origen
La resultante se obtiene como la determinante de la
matriz del sistema de ecuaciones que dan:

\[ \begin{aligned}
X^{m-1} f &= 0 \\
X^{m-2} f &= 0 \\
& \vdots \\
1f &= 0 \\
X^{n-1}g &= 0 \\
X^{n-2}g &= 0 \\
& \vdots \\
1g &= 0 \\
\end{aligned}\]

Por Teorema de Rouché, este sistema tiene solución ssi 
el determinante de los coeficientes es cero.

**** Resultante como determinante
La *resultante* de dos polinomios $p,q$ es el determinante solución de 
$pq' - qp' = 0$ dados $p$ y $q$.

\[R(p,q) = \left| \begin{matrix}
a_0 & a_1 & \dots & a_n & 0 & \dots &\\
0   & a_0 & \dots & a_{n-1} & a_n & 0 & \dots \\
0   &   0 & a_0 & \dots & a_{n-1} & a_n & \dots \\
&     &     &\dots & & & \\
b_0 & b_1 & \dots & b_m & 0 & \dots &\\
0   & b_0 & \dots & b_{m-1} & b_m & 0 & \dots \\
0   &   0 & b_0 & \dots & b_{m-1} & b_m & \dots \\
\end{matrix} \right|
\]

Y llamamos *matriz resultante* a la matriz de la que es determinante.

** 2. Series de grupos y grupos solubles
*** 2.1. Series de composición
**** 2.1.1. Factor
Sea $G$ grupo, llamamos factor a cualquier $H/H'$ donde $G > H \trianglerighteq H'$.

**** 2.1.2. Proyección
Llamamos proyección de $H/H'$ sobre $K/K'$, ambos factores, a:

\[\frac
{K'(H\cap K)}
{K'(H'\cap K)}\]

**** 2.1.3. Serie
Llamamos serie a toda cadena finita:

\[ G > G_1 > G_2 > \dots > G_r = 1\]

Donde llamamos a $r$ la longitud del grupo.

**** 2.1.4. Refinamiento de una serie
Dadas dos series,

\[ G > G_1 > G_2 > \dots > G_r = 1\]
\[ G > G'_1 > G'_2 > \dots > G'_r = 1\]

llamamos a la segunda refinamiento si todo grupo suyo aparece en
la primera. Es refinamiento propio si además son distintas.

**** 2.1.5. Serie normal
Una serie es *normal* cuando se verfica $G_i \trianglerighteq G_{i+1}$. Llamamos a $G_{i-1}/G_i$
los *factores* de la serie.

**** 2.1.5. Serie propia
Una serie propia tiene sólo inclusiones propias $G_i \gneq G_{i+1}$.

**** 2.1.5. Isomorfismo de series
Dos series son isomorfas cuando existe una permutación que hace 
isomorfos sus factores:

\[\exists \sigma \in S_r : \quad 
G_{i-1}/G_i \cong H_{\sigma(i)-1}/H_{\sigma(i)}\]

**** 2.1.5. Serie de composición
Una *serie de composición* es una serie normal propia sin 
refinamientos normales. Llamamos *factores de composición* a sus
factores.

**** 2.1.6. Grupo simple
Un grupo simple es aquel que no admite subgrupos normales propios.

**** 2.1.7. Grupos abelianos finitos simples
Un grupo abeliano, finito y simple es isomorfo a $\mathbb{Z}_p$ para algún $p$ primo.

***** TODO Demostración

**** 2.1.8. Los factores de composición son simples
Los factores de cualquier serie de composición son simples.

***** TODO Demostración

**** 2.1.9. Existencia de la serie de composición
Todo grupo finito posee una serie de composición.

**** 2.1.10. Teorema de refinamiento de Schreier
Dos series normales de un grupo tienen refinamientos isomorfos.

**** 2.1.11. Teorema de Jordan-Holder
Si un grupo admite serie de composición, toda serie normal propia puede
refinarse a una serie de composición. Las series de composición son 
isomorfas.

*** 2.2. El programa de Holder
**** TODO 2.2.1. Teorema de clasificación de grupos simples finitos
**** 2.2.2. Teorema de Abel
El grupo $A_n$ es simple para $n \geq 5$.

***** TODO Demostración

**** 2.2.3. Teorema de Feit-Thompson
Si $G$ es simple de orden impar, entonces $G \cong \mathbb{Z}_p$ con $p$ primo.

***** TODO Demostración

*** 2.3. Grupos solubles
**** 2.3.1. Serie derivada
Dado $G$, definimos la serie derivada de $G$ como:

\[G = G^0 > G' > G'' > \dots \]

donde $G^{(i+1} = [G^{(i}, G^{(i}]$ es el grupo derivado. Nótese que no tiene por
qué ser finita.

**** 2.3.2. Caracterización de grupos solubles
Para $G$ grupo finito, equivalen:

 1. Los factores de composición son cíclicos de orden primo.
 2. $G$ tiene serie normal con factores cíclicos.
 3. $G$ tiene serie normal con factores abelianos.
 4. Se tiene $G^{(i} = 1$.

***** TODO Demostración

**** 2.3.3. Grupo soluble
Un grupo es soluble si tiene una serie normal con factores cíclicos.

**** 2.3.4. Subgrupos de solubles
Son solubles:

 1. Los subgrupos de un grupo soluble.
 2. Los cocientes de un grupo soluble.
 3. Si $N$ y $G/N$ son solubles, $G$ es soluble.

***** TODO Demostración

**** 2.3.5. Producto de solubles
Todo producto finito de grupos solubles es soluble.

***** TODO Demostración

**** 2.3.6. Teorema de Hall
Sea $G$ soluble de orden $mk$ cumpliendo $mcd(m,k) = 1$. Entonces:

 1. $G$ posee un grupo de orden $m$.
 2. Dos subgrupos cualesquiera de orden $m$ son conjugados.
 3. Todo subgrupo de orden $m' \mid m$ está contenido en uno de orden $m$.
 4. El número de subgrupos de orden $m$, $r_m$ es producto de factores
    congruentes a $1$ módulo algún factor primo de $m$. Es además potencia
    de primo y divide a alguno de los factores de $G$.

***** TODO Demostración

**** 2.3.7. Caracterización de grupo soluble
Dado $G$ grupo finito, es soluble ssi para cualquier descomposición $|G|=mk$
con $mcd(m,k) = 1$, existe un subgrupo de orden $m$.

** 3. Extensiones de cuerpos
*** 3.1. Generalidades
**** 3.1.1. Extensiones de cuerpos
Una *extensión de cuerpos* es un subcuerpo $K$ de $F$, se nota por $F/K$. 

**** 3.1.2. Grado de la extensión
Llamamos *grado* a la dimensión de $F$ como espacio vectorial.
Notamos por $[F : K]$.

**** 3.1.3. Cuerpo intermedio
Un cuerpo intermedio entre $F$ y $K$ es cualquier subcuerpo de $F$ 
conteniendo a $K$.

**** 3.1.4. Torre de cuerpos
Una torre es una sucesión de subcuerpos:

\[F_0 \subset F_1 \subset \dots \subset F_n\]

**** 3.1.5. Extensiones finitas
Una extensión es finita ssi $[F:K]$ es finito.

**** 3.1.6. Base de una torre de inclusiones
Sea $K \supset F \supset E$ una torre de inclusiones. Sean $\{u_i\}_{i\in I}$ una base de $E$
sobre $F$ y $\{v_j\}_{j\in J}$ una base de $F$ sobre $K$. Entonces $\{u_iv_j\}$ es una base
de $E$ sobre $K$.

***** Demostración
****** Es sistema de generadores
Si tenemos ambos sistemas de generadores, podemos escribir cada
elemento de $E$ como:

\[ e 
= \sum u_i f_i 
= \sum u_i \left(\sum v_j k_{ij}\right) 
= \sum u_iv_ik_{ij}\]

****** Son linealmente independientes
Aplicando la independencia lineal de cada una de las bases:

\[ \sum u_iv_jk_{ij} 
= \sum u_i \left( \sum v_jk_{ij}\right) = 0\]

Tenemos que $\sum v_jk_{ij} = 0$, luego $k_{ij} = 0$.

**** 3.1.7. Teorema del grado
Sean $K \subset F \subset E$, extensiones de cuerpos, se tiene que:

\[ [E:K] = [E:F][F:K] \]

***** Demostración
Teniendo una base de cada uno de ellos, calculamos la base
de la [[*3.1.6. Base de una torre de inclusiones][torre de inclusiones]], que nos da la dimensión.

**** 3.1.8. Corolario al Teorema del grado: finitud
Sean $K \subset F \subset E$, la extensión $E/K$ es finita ssi las extensiones
$E/F$ y $F/K$ lo son.

***** Demostración
Si ambas son finitas, podemos aplicar el [[*3.1.7. Teorema del grado][teorema del grado]]. Cuando
$E/K$ es finita, tenemos que $E/F$ tiene como sistema generador a la
base y $F/K$ es un subespacio de $E/K$.

**** 3.1.9. Corolario al Teorema del grado: torres de cuerpos
Sea $F_0 \subset F_1 \subset \dots \subset F_n$ torre de longitud $n$, entonces:

\[ [F_n : F_0] =
[F_n:F_{n-1}] \dots [F_2:F_1][F_1 : F_0]
\]

***** Demostración
Por inducción sobre la longitud de la torre y aplicando el teorema
del grado a cada paso.

**** 3.1.10. Corolario al Teorema del grado: extensiones primas
Sea $F/K$ una extensión tal que $[F:K] = p$ es primo. Entonces no
existe ningún cuerpo intermedio propio.

***** Demostración
Usando el teorema del grado, tenemos que debería tener grado $p$, en
cuyo caso sería un subespacio de la misma dimensión que $F$, y por
tanto $F$. O debería tener grado $1$, en cuyo caso sería $K$.

*** 3.2. Elementos algebraicos y extensiones algebraicas
**** 3.2.1. Homomorfismo unital
Para todo anillo $A$ existe un único homomorfismo de anillos
$1_\mathbb{Z} : \mathbb{Z} \longrightarrow A$, llamado *homorfismo unital*.

**** 3.2.2. Característica del anillo
La característica de $A$ es el entero no negativo que genera
al ideal $ker(1_\mathbb{Z})$.

**** 3.2.3. Característica en dominios de integridad
Si $A$ es dominio de integridad, $car(A)$ es primo o $0$.

***** Demostración
Trivialmente desde el homomorfismo unital. Si no fuera así,
tendríamos $ab = 0$ enteros.

**** 3.2.4. Caracterización de la característica
$car(A)=n$ ssi $n$ es el menor entero positivo tal que $na = 0$ para
todo $a \in A$.

***** Demostración
Si hubiera otro menor, debería pertenecer al núcleo del homomorfismo
unital, y no podría ser generado por $n$. Si cumple la condición
y es el menor, todo el resto de elementos del núcleo deben ser 
múltiplos, porque si no lo fueran, podríamos crear un menor con 
Bezout.

**** 3.2.5. Intersección de anillos
Sea $A$ un anillo y sea $\{B_i\}_{i\in I}$ una familia de subanillos. Entonces
$\bigcap B_i$ es subanillo. Análogo para cuerpos y subcuerpos.

***** Demostración
Si dos elementos pertenecen a todos los $B_i$, tenemos que su suma
y su producto pertenece a cada uno de ellos.

**** 3.2.6. Anillo primo
Llamamos subanillo primo de $A$ a la intersección de todos los 
subanillos de $A$.

**** 3.2.7. Clasificación de anillos primos
El subanillo primo de un $A$ es isomorfo a $\mathbb{Z}$ si $car(A) = 0$ y
a $\mathbb{Z}/n\mathbb{Z}$ si $car(A) = n \neq 0$.

***** Demostración
En ambos casos, ellos son subanillos por ser imágenes del 
homomorfismo unital, como se comprueba por primer teorema de
isomorfía.

**** 3.2.8. Subcuerpo primo
Llamamos subcuerpo primo de $K$ a la intersección de todos los
subcuerpos de $K$.

**** 3.2.9. Clasificación de subcuerpos primos
El subcuerpo primo de un cuerpo $K$ es isomorfo a $\mathbb{Q}$ cuando
$car(K)=0$ y a $\mathbb{Z}/p\mathbb{Z}$ cuando $car(K) = p \neq 0$.

***** Demostración
De nuevo, vuelve a tenerse una inyección de ambos por el 
homomorfismo unital. Cualquier subanillo contendrá a $1$ y por
tanto a este subcuerpo.

**** 3.2.10. Subanillo generado
Sea $F/K$ extensión con $S\subseteq F$; llamamos *subanillo generado*
$K[S]$ a la intersección de todos los subanillos de $F$ conteniendo
a $K$ y a $S$.

**** 3.2.10. Subcuerpo generado
Sea $F/K$ extensión con $S \subseteq F$; llamamos *subcuerpo generado*
$K(S)$ a la intersección de todos los subcuerpos de $F$ conteniendo
a $K$ y a $S$.

**** 3.2.11. Propiedades de subanillos y subcuerpos generados
Para $S,T \subseteq F$ extensión de $K$, tenemos:

 - $K[S \cup T] = K[S][T] = K[T][S]$
 - $K(S \cup T) = K(S)(T) = K(T)(S)$

***** Demostración
En cualquiera de los dos casos la definición es la intersección
de todos los que contienen a $K$, $T$ y $S$.

**** 3.2.12. Subcuerpo compuesto
Dados $K \subset E,F \subset L$, definimos el subcuerpo compuesto
$EF = E(F) = F(E)$.

***** Demostración
Son iguales trivialmente desde la definición.

**** 3.2.13. Conjunto de generadores
Sea $F/K$ con $S \subseteq F$, es un subconjunto de generadores si
$F = K(S)$.

**** 3.2.14. Extensión finitamente generada
Una extensión $F/K$ es finitamente generada cuando tiene un
conjunto finito de generadores. $F = K(u_1,u_2,\dots,u_n)$.

**** 3.2.15. Extensiones simples y elementos primitivos
Una extensión $F/K$ se llama simple cuando $F = K(u)$. Al $u$
se le llama *elemento primitivo* para la extensión.

**** 3.2.16. Elementos algebraicos
$\alpha \in F$ es *algebraico* sobre $K$ si existe polinomio $f \in K[x]$ tal 
que $f(\alpha) = 0$. 

Un no algebraico es *trascendente* y una extensión es *algebraica* 
si lo son todos sus elementos.

**** 3.2.16. Polinomios irreducibles
Dado $F/K$ con $\alpha \in F$ algebraico. Existe un único polinomio 
irreducible del que $\alpha$ es raíz salvo asociados, llamado $Irr(\alpha)$.
   
***** Existencia y unicidad del polinomio irreducible
Tomo el núcleo del homomorfismo que evalúa un polinomio en $\alpha$. 
Por ser un ideal en PID, estará generado por algún polinomio $f$ 
no nulo y no constante.

Este será irreducible, porque si no lo fuera, con $f = g_1g_2$ se 
tendría:

\[0 = f(\alpha) = g_1(\alpha)g_2(\alpha)\]

Un polinomio de grado mínimo debería estar dentro del ideal, 
y por tanto ser asociado de $f$, que lo genera.

**** 3.2.16. Propiedades de los polinomios irreducibles
Sea $F/K$ extensión con $u \in F$ algebraico. Se cumple:

 1. $K(u) = K[u]$
 2. $K[u] \cong K[X]/(Irr(u,K))$
 3. $[K(u):K]$ es igual al grado de $Irr(u,K)$.
 4. $\{1,u,u^2,\dots,u^{n-1}\}$ es una base de $K[u]$ sobre $K$.
 5. $f(u)=0$ ssi $Irr(u,K) \mid f$.

Llamamos *grado* del elemento $u$ al grado de $Irr(u,K)$.

***** Demostración
****** Punto 1
Sabemos que el anillo generado está dentro del cuerpo generado,
y además, $u^{-1}$ está en el anillo generado porque, si su polinomio
irreducible nos da $\sum a_iu^i = 0$, tenemos:

\[ u \left(a_nu^{n-1} + a_{n-1}u^{n-2} + \dots a_1 \right)\frac{1}{a_0} = 1\]

****** Punto 2
Aplicando el primer teorema de isomorfía al morfismo evaluación,
tenemos el resultado.

****** Punto 3
Tenemos que $1,u,u^2,\dots,u^{n-1}$ son linealmente independientes porque
una relación lineal entre ellos daría un polinomio menor que el
mínimo. Además son base trivialmente porque $u^{-1}$ puede expresarse
linealmente como polinomio suyo como mostramos [[*Punto 1][antes]] y porque
cuaquier expresión polinómica de grado mayor a $n$ puede dividirse 
por el polinomio irreducible para obtener otra de grado menor.

****** Punto 4
La misma demostración [[*Punto 3][anterior]].

****** Punto 5
Un polinomio verificando $f(u)=0$ está dentro del núcleo del
homomorfismo evaluación.

**** 3.2.17. Algebraicos en una torre de cuerpos
Sea $K \subset F \subset E$ con $u \in E$ algebraico sobre $K$, entonces $u$ es
algebraico sobre $F$ y $Irr(u,F)$ divide a $Irr(u,K)$.

***** Demostración
Notamos que $Irr(u,K)$ es también un polinomio sobre $F$ que anula
a $u$, así que, por las propiedades de los polinomios irreducibles,
se debe tener $Irr(u,F) \mid Irr(u,K)$.

**** 3.2.18. Extensiones algebraicas
Una extensión se llama *algebraica* si todos sus elementos lo son.
Es *trascendente* en otro caso.

**** 3.2.19. Elementos algebraicamente independientes
Los elementos $\{u_i \mid i\in I\}$ son algebraicamente independientes si el
homomorfismo de evaluación sobre el cuerpo de polinomios en varias
variables $K[X_i \mid i\in I]$ es inyectivo.

**** 3.2.20. Extensiones puramente trascendentes
Una extensión $F/K$ se llama puramente trascendente si $F = K(S)$
donde $S$ un conjunto de algebraicamente independientes.

**** 3.2.21. Generación finita de elementos
Para $F/K$ extensión cualquiera $S \subseteq F$, se tiene:

 1. Para $u \in K[S]$ existe un subconjunto $\{u_1,\dots,u_n\} \subset S$ tal que
    $u \in K[u_1,\dots,u_n]$.
 2. Para $u \in K(S)$ existe un subconjunto $\{u_1,\dots,u_n\} \subset S$ tal que
    $u \in K(u_1,\dots,u_n)$.

***** Demostración
Tener $u \in K[S]$ nos da una expresión polinómica finita como elementos
de $S$. Los elementos involucrados en esa expresión crean una extensión
finita en la que está $u$. Análogo en el caso de cuerpos.

**** 3.2.22. Generación del compuesto
Sean $K \subset E,K(S) \subset L$, entonces $EK(S) = E(S)$.

***** Demostración
Por definición del cuerpo compuesto, será $EK(S) = K(E)(S) = E(S)$.

**** 3.2.23. Grado de una extensión compuesta
Sean $K \subset E,F \subset L$. Entonces:

\[ [EF:K] \leq [E:K][F:K]\]

***** Demostración
En el caso finito, el cuerpo generado por las bases de $E$ y
de $F$ multiplicadas contiene a todo elemento de $E$ y de $F$, por 
lo que es sistema de generadores de $EF$.

**** 3.2.24. Extensiones primas relativas
Sean $K \subset E,F \subset L$ con $n = [E:K]$ y $m = [F:K]$ primos relativos.
Entonces $[EF:K] = [E:K][F:K]$.

***** Demostración
Sea $\{f_i\}$ base de $F$ sobre $K$. Tenemos que es sistema de generadores
de $F$ sobre $E$, y por tanto, de $EF$ sobre $E$. Así $[EF:E] \leq [F:K]$
y análogamente $[EF:F] \leq [E:K]$.

Por otro lado, por teorema del grado tenemos:

\[[EF:K] = [EF:F][F:K] = [EF:F]m\]
\[[EF:K] = [EF:E][E:K] = [EF:E]n\] 

Así, por ser primos relativos, tenemos $n \mid [EF:E]$ y $m \mid [EF:F]$; 
teniéndose finalmente:

\[ [EF:F] = [E:K] \]
\[ [EF:E] = [F:K] \]

**** 3.2.25. Extensión finitamente generada por algebraicos es finita
Una extensión $F = K(u_1,\dots,u_n)$ finitamente generada por $u_i$ algebraicos
es finita.

***** Demostración
Todo elemento algebraico cumple una relación polinómica. Así,
todo elemento de grado igual o mayor a esta relación, puede
expresarse como elementos de grado menor.

Si tenemos $e_i$ como el exponente mayor al que puedo elevar $u_i$ sin
que pueda ser reescrito, tenemos un sistema de generadores de $F$ 
finito como:

\[\{ 1, u_1^1, u_1^2, \dots u_1^{e_i}, u_2^1, \dots, u_2^{e_j}, u_3^1,\dots\}\]

**** 3.2.26. Extensión generada por algebraicos es algebraica
Una extensión $K(S)/K$ es algebraica sobre $K$ ssi todo $u \in S$ es 
algebraico sobre $K$.

***** Demostración
Si es algebraica, en particular lo es cada elemento de $S$.
Si lo son los elementos de $S$, podemos ver que cualquier 

**** 3.2.27. Finita es algebraica y finitamente generada
Una extensión es finita ssi es algebraica y finitamente generada

***** Demostración
Tenemos que extensión finitamente generada por algebraicos es
[[*3.2.25. Extensión finitamente generada por algebraicos es finita][finita]]. Por otro lado, si es finita, tendrá una base finita que
la genera; y para cada elemento de la base, $\{1,u,\dots,u^n\}$ no
será linealmente independiente. Luego será finita.

**** 3.2.28. Caracterización de elementos algebraicos
Un elemento $u \in F$ es algebraico sobre $K$ ssi existe una extensión
finita intermedia $E/K$ donde $u \in E$.

***** Demostración
Si es algebraico de grado $n$, tenemos $K(u)$ algebraica y finitamente
generada, [[*3.2.27. Finita es algebraica y finitamente generada][luego finita]]. Si existe una extensión finita intermedia, 
será algebraica.

**** 3.2.29. Torre algebraica
Sean $K \subset F \subset E$, $E/K$ es algebraica ssi $E/F$ y $F/K$ son ambas 
algebraicas.

***** Demostración
****** Si es algebraica, lo son sus partes
Un elemento algebraico sobre $K$ lo será sobre $F$. Y todo elemento
de $F$ está en $E$, luego será algebraico sobre $K$.

****** Si las partes son algebraicas, es algebraica
Todo elemento $e \in E$ es algebraico sobre $F$, luego cumple algún
polinomio con elementos en $F$. Los elementos que generan el polinomio
en $F$ son todos algebraicos sobre $K$, luego $K$ extendido con esos
elementos es finito. Si lo extiendo con $e$, que es algebraico sobre
ellos, llego a otra extensión finita. Toda finita es [[*3.2.27. Finita es algebraica y finitamente generada][algebraica]].

**** 3.2.30. Clausura algebraica relativa
Dada $F/K$ extensión, el conjunto de elementos algebraicos forman un
subcuerpo de $F$. Llamado *clausura algebraica relativa* de $K$ en $F$.

***** Demostración
Sean $a,b \in F$ algebraicos; entonces $K(a,b)$ es finito, luego $a+b$ y
$ab$ son también algebraicos.

***** Demostración constructiva para la suma
Se puede [[http://mathoverflow.net/a/81640/45365][encontrar constructivamente]] un polinomio que tenga como
raíz a la suma de dos algebraicos.

**** DONE Existencia de clausura
*Teorema de Steinitz*. Todo cuerpo tiene una extensión algebraicamente 
cerrada.

**** DONE Homomorfismos sobre un cuerpo
     Un *homomorfismo sobre cuerpos* $K,K'$ es un homomorfismo $\phi$ sobre extensiones
     $F,F'$ con un isomorfismo $\omega : K \longrightarrow K'$ debe cumplir: $\phi|_K = \omega$. 
     Cuando no se especifica, se asume la identidad.

     \[ \phi : F/K \longrightarrow F'/K' \]

**** DONE Automorfismos entre extensiones
     Un automofismo de extensiones es un homomorfismo sobre el cuerpo $K$, 
     $\phi : F/K \longrightarrow F/K$ que es isomorfismo.

** 4. Cuerpos de descomposición
*** DONE Extensión de homomorfismos
*Extensión de un homomorfismo*. $\tau : F_1 \longrightarrow F_2$ es extensión de 
$\sigma : K_1 \longrightarrow K_2$ cuando son dos extensiones $F/K$ y se cumple
que $\tau |_K_1 = \sigma$.

Cuando $\sigma = 1$, llamamos a $\tau$ *homomorfismo sobre K*.

**** Automorfismos de una extensión
*Automorfismos de una extensión*. Sea $\sigma : F/K \longrightarrow F/K$, entonces,
$\sigma$ es automorfismo.

***** Demostración
Para $u\in F$, tomamos $K(u_1,\dots,u_k)$ la extensión finita generada por todas 
las raíces del irreducible sobre $u$. Como $\sigma$ respeta raíces, puede restringirse
a esta extensión; y será inyectivo en ella por ser morfismo de cuerpos.

Como esta extensión es finita y esto es una aplicación lineal, la aplicación 
restringida es sobreyectiva.

**** Número de extensiones
#+begin_theorem
*Número de extensiones*. Teniendo $u_1$ de irreducible a $f_1$, hay tantas
extensiones $\tau : K_1[u_1] \longrightarrow F_2$ como raíces tenga su imagen $f_2$; ya que están 
completamente determinadas por la imagen de $u_1$.
#+end_theorem
#+begin_proofs
Existe un único isomorfismo llevando $\tau(u_1) = u_2$, tomando este diagrama de
isomorfismos:

\[ \begin{tikzcd}
K_1(u) \rar{\tau} & K_2(u) \\
\frac{K_1[X]}{(f_1)} \uar{p_1} \rar{\overline\sigma} &
\frac{K_2[X]}{(f_2)} \uar{p_2}
\end{tikzcd} \]

Donde $\sigma$ es inducido por el isomorfismo de extensión a polinomios.
#+end_proofs

*** 4.1. Cuerpo de descomposición
**** 4.1.1. Teorema de Kronecker
Sea $f$ de grado no nulo sobre $K$, entonces existe
una extensión $F/K$ tal que existe $u \in F$ con $f(u) = 0$.

***** Demostración
Puedo descomponer en irreducibles $f = f_1f_2\dots f_m$; y tener una 
extensión cumpliendo lo pedido:

\[ F = \frac{K[X]}{(f_1)}\]

Para el elemento $u = x + (f_1)$.

**** 4.1.2. Extensión de un homomorfismo
Dadas extensiones $F_1/K_1, F_2/K_2$, decimos que $\tau : F_1 \longrightarrow F_2$ 
es una extensión de $\sigma : K_1 \longrightarrow K_2$, ambos homomorfismos de 
cuerpos, cuando $\tau|_{K_1} = \sigma$.

**** 4.1.3. Isomorfismo extendido a polinomios
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos. Existe una única extensión a
un isomorfismo $\sigma : K_1[X] \longrightarrow K_2[X]$ cumpliendo que $\sigma(X) = X$.

***** Demostración
Por la propiedad universal por el anillo de polinomios. Las imágenes
de todos los elementos están fijas excepto la de $X$ y sus múltiplos.

**** 4.1.4. Isomorfismos respetan irreducibilidad
Sea $f_1 \in K_1[X]$ irreducible sobre $K_1$, entonces $\sigma(f)$ es irreducible sobre
$K_2$; donde $\sigma$ es la [[*4.1.3. Isomorfismo extendido a polinomios][extensión]] a polinomios de un isomorfismo.

***** Demostración
Si tuviéramos $f_1 = gh$ dos polinomios no triviales, su grado se 
conservaría al aplicar el isomorfismo y tendríamos
$\sigma(f)=\sigma(g)\sigma(h)$.

**** 4.1.5. Isomorfismo de raíces de polinomios
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos y sean $F_1/K_1, F_2/K_2$ 
extensiones algebraicas. Dado un homomorfismo $\tau : F_1 \longrightarrow F_2$ sobre $\sigma$;
si $u$ es raíz de $p$ entonces $\tau(u)$ es raíz de $\sigma(p)$.

***** Demostración
Por simple cálculo tomando $p(x) = a_nx^n + \dots + a_1x + a_0$:

\[ \begin{aligned}
\sigma(p)(\tau(u)) &= \sigma(a_n)\tau(u)^n + \dots + \sigma(a_1)\tau(u) + \sigma(a_0) \\ 
                   &= \tau(a_n)\tau(u)^n + \dots + \tau(a_1)\tau(u) + \tau(a_0) \\
                   &= \tau(a_nu^n + \dots + a_1u + a_0) \\
                   &= \tau(0) = 0
\end{aligned} \]

**** 4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos
Sea $F/K$ una extensión algebraica y $\tau : F/K \longrightarrow F/K$ un endomorfismo
sobre $K$. Entonces $\tau$ es un automorfismo.

***** Demostración
Tenemos que es una extensión de la identidad y que preserva [[*4.1.5. Isomorfismo de raíces de polinomios][raíces]].
Por eso, dado un elemento $u \in F$, tomamos su $f = Irr(u,K)$. Y tenemos
que $\{\tau^n(u)\}_{n\in \mathbb{N}}$ es una sucesión de raíces del polinomio que, por 
inyectividad, deberá repetir los elementos en algún momento.

Tendremos $\tau^m(u)=u$ y será sobreyectiva. Nótese que estamos usando
la inyectividad de todo homomorfismo de cuerpos.

**** 4.1.7. Isomorfismo intercambiando conjugadas
Sea $\sigma : K_1 \longrightarrow K_2$ isomorfismo de cuerpos. Sea $p$ irreducible
y $u_1,u_2$ raíces de $p$ y $\sigma(p)$ en extensiones $F_1,F_2$. Entonces existe un 
único isomorfismo $\tau : K_1(u_1) \longrightarrow K_2(u_2)$ sobre $\sigma$ tal que $\tau(u_1) = u_2$.

***** Demostración
El isomorfismo buscado sale como composición:

\[K_1(u_1) \cong 
  \frac{K_1[X]}{(f_1)} \cong 
  \frac{K_2[X]}{(f_2)} \cong
  K_2(u_2)\]

**** 4.1.8. Número de extensiones
El número de extensiones $\tau : K_1[u_1] \longrightarrow F_2$ sobre $\sigma$ es el número de
raíces distintas de $\sigma(p)$ en $F_2$.

***** Demostración
Por la [[*4.1.7. Isomorfismo intercambiando conjugadas][proposición anterior]], una para cada raíz de $\sigma(p)$. Nótese
que no puede haber más porque la imagen de $u_1$ determina completamente
el morfismo y porque la imagen de raíz [[*4.1.5. Isomorfismo de raíces de polinomios][debe ser]] una raíz.

**** 4.1.9. Cuerpo de descomposición
Un $E/K$ es un cuerpo de descomposición de $f$ ssi existen $u_1,\dots,u_n \in E$ 
tales que $f = (X-u_1)\dots(X-u_n)$ y $E = K(u_1,\dots,u_n)$.

***** Caracterización
Nótese que es el mínimo en el que factoriza linealmente. Cualquier
otro en el que factorice linealmente necesita contar con sus raíces.

**** 4.1.10. Cuerpo de descomposición en una torre
Sean $K \subset F \subset E$. Si $E$ es cuerpo de descomposición de $f$ sobre $K$, 
también lo es de $f$ sobre $F$.

***** Demostración
Se cumple trivialmente:

\[ E = K(u_1,\dots,u_n) \subset F(u_1,\dots,u_n) \subset E\]

**** 4.1.11. Existencia del cuerpo de descomposición
Cualquier $f \in K[X]$ de grado $n$ tiene un cuerpo de descomposición, 
que además verifica $[F:K] \leq n!$

***** Demostración
****** Inducción: caso base
Cuando $n=1$, tenemos una raíz en el cuerpo.

****** Inducción: caso inductivo
En otro caso, por [[*4.1.1. Teorema de Kronecker][Kronecker]], tenemos que existe una extensión
en la que hay una raíz del polinomio. Tomamos esa raíz para crear
$K(u)$. Por hipótesis de inducción, hay un cuerpo de descomposición
de $f/(X-u)$ sobre $K(u)$, llamado $F$. Por último, podemos construir 
un cuerpo de descomposición con sus raíces.

Tenemos además que la raíz tiene menos grado que el polinomio:

\[ [F:K] = [F:K(u)][K(u):K] \leq (n-1)!n = n! \]

**** 4.1.12. Isomorfismo entre cuerpos de descomposición
Sean $F_1/K_1, F_2/K_2$ cuerpos de descomposición de $f \in K_1[X]$ y 
$\sigma(f) \in K_2[X]$; para $\sigma : K_1 \longrightarrow K_2$ isomorfismo. Entonces son
isomorfos.

***** Demostración
****** Caso base
Si ambos son de grado $1$ tenemos extensiones triviales y 
hemos terminado.

****** Caso inductivo
Sea $u \in F_1$ raíz de $f$. Como $Irr(u,K) \mid f$, tenemos que 
$\sigma(Irr(u,K)) \mid \sigma(f)$. Si tomamos una raíz $v$ de $\sigma(Irr(u,K))$ y 
aplicamos el [[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo intercambiando conjugadas]] anterior,
tenemos una extensión $\tau : K_1(u) \longrightarrow K_2(v)$ que lleva una en 
otra. Ahora tomamos $f=g(X-u)$ y por inducción tenemos
un isomorfismo extendiendo hasta el cuerpo de descomposición.

**** 4.1.13. Unicidad del cuerpo de descomposición
Dos cuerpos de descomposición de $f \in K[X]$ sobre $K$ son isomorfos.

***** Demostración
Trivial aplicando lo [[*4.1.12. Isomorfismo entre cuerpos de descomposición][anterior]] al isomorfismo igualdad.

**** 4.1.23. Cuerpo de descomposición de una familia
Sea ${\cal P} \subseteq K[X]$ una familia de polinomios no constantes. Una
extensión $E/K$ es cuerpo de descomposición suyo si todo polinomio
factoriza linealmente y es además se tiene $E = K(S)$ con:

\[ S = \{ u \in E \mid \exists f \in {\cal P}: f(u)=0\} \]

**** 4.1.24. Existencia del cuerpo de descomposición de una familia
Para toda familia de polinomios existe un cuerpo de descomposición.

***** Demostración
****** Caso finito
En el caso finito, multiplicamos toda la familia para aplicar
la [[*4.1.11. Existencia del cuerpo de descomposición][existencia del cuerpo de descomposición]] al producto.

****** Caso infinito
Cuando tenemos una familia $\{ f_\lambda \mid \lambda \in \Lambda\}$. Si tomamos $I \subset \Lambda$ finito, 
podemos asignarle un cuerpo de descomposición $F_I$ tal que $I\subset J$ 
implique $F_I \subset F_J$.

Tomamos:

\[F = \bigcup_{J\text{ finito}} F_J \]

Las operaciones entre dos elementos de $F$ se definen en el menor
cuerpo que contenga a los dos.

Esto es un cuerpo de descomposición porque todo polinomio ya
descompone linealmente en cualquier $F_J$ que lo contenga; y cada
$F_J$ era ya cuerpo de descomposición de los polinomios que contenía,
así que el cuerpo de descomposición debe al menos contener a todos
los $F_J$.

**** 4.1.25. Unicidad (esencial) del cuerpo de descomposición de una familia
El cuerpo de descomposición de una familia de polinomios es único
salvo isomorfismos.

***** Demostración
****** Caso finito
Aplicamos que los cuerpos de descomposición de un polinomio
[[*4.1.13. Unicidad del cuerpo de descomposición][son isomorfos]] al polinomio producto.

****** Caso infinito
Sean $F_1,F_2$ dos cuerpos de descomposición sobre $K$ de una familia
de polinomios.

Ordenamos el siguiente conjunto por inclusión y por extensión del
isomorfismo. 

\[ (E,\tau) \leq (F,\psi) \Leftrightarrow 
(E \subset F) \wedge (\psi|_E = \tau)\]

Es una ordenación inductiva porque la unión arbitraria da una 
cota maximal de cualquier cadena:

\[ {\cal E} = 
\left\{ (E,\tau) 
\mid F_1 \supset E \supset K;\;
\tau : E/K \longrightarrow F_2/K
\right\}\]

Que sabemos no vacío por el caso finito. Sea $(F,\sigma)$ maximal. Y 
supongamos que $F \subsetneq F_1$, entonces existe alguna raíz de alguno
de los polinomios que no está en $F$. Creando un [[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo]]
sobre $F$ que la llevara a su conjugada, contravendríamos maximalidad.

Ahora, todo $f$ de la familia descompondría en $F$ y por $\sigma$, se las
llevarían a $F_2$.

**** DONE Descomposición
Un polinomio $f \in K[X]$ *descompone* en una extensión $E$ si
factoriza como polinomios lineales en $E[X]$. Llamamos *cuerpo de
descomposición* a un cuerpo extensión de $K$ minimal en el que
descompone.

Propiedades:

 - Sea $E$ cuerpo de descomposición, entonces $E = K(\alpha_1,\alpha_2,\dots,\alpha_n)$, 
   siendo una extensión finita con grado acotado por $n!$.
 - Todo polinomio tiene cuerpo de descomposición sobre $K$, ya que 
   podemos tomar una clausura algebraica y crear $K(\alpha_1,\dots,\alpha_n)$ con
   sus raíces.

*** 4.2. Clausura algebraica
**** 4.2.1. Algebraicamente cerrado
Un cuerpo tal que toda extensión algebraica suya sea trivial
es un cuerpo algebraicamente cerrado.

**** 4.2.2. Caracterización de los algebraicamente cerrados
Equivalen las siguientes propiedades:

 1. Todo polinomio no constante tiene raíz en $K$.
 2. Todo polinomio descompone linealmente en $K$.
 3. Un polinomio es irreducible en $K$ ssi es de grado $1$.
 4. Toda extensión algebraica de $K$ es trivial.

***** Demostración
****** Implicación 1 a 2
Como $f$ tiene raíz en $K$, podemos dividirlo por $(x-u)$, para
obtener un nuevo polinomio de grado menor.

****** Implicación 2 a 3
Trivialmente.

****** Implicación 3 a 4
Sea un elemento en la extensión algebraica, su irreducible
debe ser de grado $1$, luego debe estar en el cuerpo base.

****** Implicación 4 a 1
Si algún polinomio no constante no tuviera raíz, aplicamos
[[*4.1.1. Teorema de Kronecker][Teorema de Kronecker]] para crear una extensión algebraica
no trivial.

**** 4.2.3. Infinitud de algebraicamente cerrados
Todo cuerpo algebraicamente cerrado es infinito.

***** Demostración
Si tengo un cuerpo finito $K$ formo el polinomio irreducible
siguiente, que no tiene raíces en $K$:

\[ 
f(x) = \prod_{k \in K} (x-k) + 1
\]

**** 4.2.4. Cuerpo algebraicamente cerrado de elementos algebraicos
Sea $E/K$ con $E$ algebraicamente cerrado. Los elementos algebraicos
de $E$ forman un cuerpo algebraicamente cerrado.

***** Demostración
Sabemos que [[*3.2.30. Clausura algebraica relativa][forman un cuerpo]]. Para ver que es algebraicamente cerrado
vemos que todo polinomio sobre ellos tiene una raíz en $E$, por ser
este algebraicamente cerrado. Como una raíz es algebraica, tiene
una raíz en el subcuerpo de los algebraicos.

**** 4.2.5. Clausura algebraica absoluta
Una extensión algebraica $E/K$ es la clausura algebraica (absoluta) 
si es una extensión algebraica y $E$ es algebraicamente cerrado.

**** 4.2.6. Caracterización de la clausura algebraica
Equivalen:

 1. $E/K$ clausura algebraica.
 2. $E/K$ algebraica y todo polinomio no constante $f \in K[X]$ 
    descompone en factores lineales en $E[X]$.
 3. $E$ es cuerpo de descomposición de todos los polinomios no 
    constantes de $K$.
 4. $E/K$ algebraica y todo no constante tiene una raíz en $E$.

***** Demostración
****** Implicación 1 a 2
Como $E$ es algebraicamente cerrado, [[*4.2.2. Caracterización de los algebraicamente cerrados][sabemos que]] todos sus polinomios
descomponen en factores lineales en $E[X]$.

****** Implicación 2 a 3
Todo polinomio descompone. Además, todo elemento de $E$ es algebraico;
así que $K(S) = E$.

****** Implicación 3 a 1
$E$ está generado por elementos algebraicos, luego es algebraico.
Además, todo polinomio no constante descompone en él, luego
es algebraicamente cerrado por la [[*4.2.2. Caracterización de los algebraicamente cerrados][caracterización]].

****** TODO Equivalencia con 4
**** 4.2.7. Transitividad de la clausura algebraica
Sean $K \subset F \subset E$ torre de cuerpos, con $F/K$ algebraica. 
Entonces $E$ es clausura algebraica de $F$ ssi lo es de $K$.

***** Demostración
Ser algebraicamente cerrado es independiente del cuerpo base de
la extensión. Ser algebraico [[*3.2.29. Torre algebraica][equivale]] a que lo sean las dos partes.

**** 4.2.8. Teorema de Steinitz
Para todo cuerpo existe una clausura algebraica.

***** Demostración
Sabemos que [[*4.1.24. Existencia del cuerpo de descomposición de una familia][existe]] el cuerpo de descomposición de todos los polinomios
no constantes. Por la caracterización de [[*4.2.6. Caracterización de la clausura algebraica][clausura algebraica]] sabemos
que lo es.

**** 4.2.9. Unicidad esencial de la clausura algebraica
Dos clausuras algebraicas $E_1,E_2$ del mismo cuerpo $K$ son isomorfas 
sobre $K$.

***** Demostración
Tenemos unicidad de esencial de los cuerpos de descomposición,
y estos son cuerpos de descomposición de todos los polinomios
no constantes.

**** 4.2.10. Extensión de homomorfismos a algebraicas
Sea $K \subset F \subset E$ con $E/K$ algebraica. Entonces todo $\sigma : F \longrightarrow \overline{K}$ tiene
una extensión $\tau : E \longrightarrow \overline{K}$.

***** Demostración
Aplicamos Zorn sobre el siguiente conjunto ordenado para la inclusión,
sabiendo que toda cadena de cuerpos está acotada por su unión y que cada
$\sigma_i$ es extensión del anterior:

\[{\cal S} = \{(E_i,\sigma_i) \mid 
F \subset E_i \subset E;\; \sigma_i : E_i \longrightarrow \overline{K};\;
\sigma_i|_F = \sigma\}\]

Esto nos da el maximal $E_1$. Si $E_1 \subsetneq E$, tomo $u \in E - E_1$; y busco un
[[*4.1.7. Isomorfismo intercambiando conjugadas][isomorfismo]] intercambiando $u$ por una conjugada también raíz de $Irr(u,K)$.
Esto me da $(E_1,\sigma_1) \leq (E_1(u),\tau)$.

**** 4.2.11. Cardinalidad de la clausura algebraica
Sea $K$ cuerpo y $\overline{K}$ su clausura.

 1. Si $K$ es finito, $\overline{K}$ es infinito numerable
 2. Si $K$ es infinito, $\#K = \#\overline{K}$.

***** Demostración
****** Caso finito
Cuando $K$ es finito, el número de polinomios irreducibles
sobre él es infinito numerable.
****** Caso infinito
En el caso infinito, como máximo se tendrá la acotación que
da el número de polinomios, que es una unión numerable:

\[ \#\overline{K} \leq \#\left(\bigcup K^n\right) = \#K \]

** 5. Extensiones normales y separables
*** 5.1. Elementos conjugados y extensiones conjugadas
**** 5.1.1. Elementos conjugados
Sean $u,v \in \overline{K}$, clausura algebraica. Equivalen:

  1. $Irr(u,K) = Irr(v,K)$
  2. $\exists \tau : K(u) \longrightarrow K(v)$ *isomorfismo* con $\tau(u) = v$.
  3. $\exists\sigma : K(u) \longrightarrow \overline{K}$ *homomorfismo* con $\sigma(u) = v$.
  4. $\exists \sigma : \overline{K} \longrightarrow \overline{K}$ *automorfismo* con $\sigma(u) = v$.

Morfismos manteniendo $K$. Estos elementos se llaman 
*elementos conjugados*.

***** Demostración
****** Implicación 1 a 2
Supongamos que tienen el mismo polinomio irreducible $f(X)$, 
entonces podemos construir un isomorfismo que lleve $u,v$ a 
$X + (f(X))$ para tener:

\[K(u) \cong \frac{K[X]}{(f(X))}\cong K(v)\]

****** Implicación 2 a 3 y 4
Dado el isomorfismo, lo podemos prolongar a un homomorfismo.
Y dado un homomorfismo, lo podemos [[*4.2.10. Extensión de homomorfismos a algebraicas][prolongar]] a un automorfismo 
por ser una extensión algebraica.

****** Implicación 4 a 1
Supongamos que existe el automorfismo de $\overline{K}$. Sea $f(X) = Irr(u,K)$,
y tenemos $0 = \phi(f(u)) = f(\phi(u)) = f(v)$; luego $Irr(u,K) = Irr(v,K)$.

**** 5.1.3. Extensiones conjugadas
Sean $F_1/K$, $F_2/K$ extensiones algebraicas, equivalen:

 1. $\exists \sigma: F_1 \longrightarrow F_2$ *isomorfismo* sobre $K$.
 2. $\exists \sigma : F_1 \longrightarrow \overline{K}$, *homomorfismo* sobre $K$ con $\sigma(F_1) = F_2$.
 3. $\sigma : \overline{K} \longrightarrow \overline{K}$, *isomorfismo* tal que $\sigma(F_1) = F_2$.

Estas extensiones se llaman *extensiones conjugadas*.

***** Demostración
****** Implicación 1 a 2
Extendiendo el isomorfismo se pasa de 1 a 2.

****** Implicación 2 a 3
Dado el homomorfismo, lo podemos [[*4.2.10. Extensión de homomorfismos a algebraicas][prolongar]] a un homomorfismo
por ser $\overline{K}$ algebraica. Tenemos entonces un endomorfismo de un
cuerpo algebraico, que [[*4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos][debe ser]] un automorfismo.

****** Implicación 3 a 1
La restricción a $F_1$ es isomorfismo.

*** 5.2. Extensiones normales
**** 5.2.1. Extensiones normales
Sea $F/K$ extensión algebraica, subcuerpo de $\overline{K}$, equivalen:

  - $\sigma : F \longrightarrow \overline{K}$ sobre $K$, me da $\sigma(F) = F$.
  - Todo irreducible de $K[X]$ con una raíz en $F$ descompone en 
    lineales en $F[X]$.
  - $F$ es cuerpo de descomposición de una familia de polinomios
    sobre $K[X]$.

 A una extensión de este tipo se le llama *extensión normal*.

***** Demostración
****** Implicación 1 a 2
Dos raíces del mismo irreducible son conjugadas, luego existe
un [[*5.1.1. Elementos conjugados][homomorfismo]] que lleva una en otra, como debe llevar $F$ en $F$,
la otra debe estar en $F$.

****** Implicación 2 a 3
Para cada elemento de $F$ puedo tomar su irreducible, como tiene
una raíz en $F$ tiene todas. $F$ es el cuerpo de descomposición de la
familia de todos los irreducibles de sus elementos.

****** Implicación 3 a 1
Tenemos que $F = K(\alpha_1,\alpha_2,\dots)$, raíces de los polinomios. La imagen
de la raíz de un polinomio sobre $K$ debe ser raíz de ese mismo 
polinomio porque este debe quedar invariante sobre $\sigma$. Así tenemos
que $\sigma(F) \subset F$, y por ser algebraico, todo [[*4.1.6. Los endomorfismos de extensiones algebraicas son automorfismos][endomorfismo es 
automorfismo]].

**** 5.2.3. Propiedades de las extensiones normales
Sea $E$ extensión normal. Cumple:

 1. Si $A/K$ es algebraica, $EA/A$ es normal.
 2. Si $K \subset F \subset E$, entonces $E/F$ es normal.
 3. Si $E_1/K, E_2/K$ son normales, $E_1E_2/K$ es normal.
 4. Sea $E_\lambda$, familia de extensiones normales; $F = \bigcap E_\lambda$ es normal.

***** Demostración de 1
Dado $\sigma : EA/A \longrightarrow \overline{K}/A$, tengo que $\sigma(EA) = \sigma(E)\sigma(A) = EA$.

***** Demostración de 2
Sea $\sigma : E/F \longrightarrow \overline{K}/F$, como deja fijo $K$, debe tenerse $\sigma(E)=E$.

***** Demostración de 3
Sea $\sigma: E_1E_2/K \longrightarrow \overline{K}/K$, las restricciones dejan fijos ambos cuerpos
y $\sigma(E_1E_2) = \sigma(E_1)\sigma(E_2) = E_1E_2$.

***** Demostración de 4
Si un homomorfismo de la intersección lo extiendo y lo restrinjo a
cada uno, debe dejar todos los cuerpos fijos. Por tanto, un elemento
en la intersección debe quedarse en la intersección.

***** Contraejemplo: subcuerpo de normal no es normal
El último es cuerpo de descomposición de $x^3-2$,
pero el primero no es normal, porque no están todas las
raíces del polinomio irreducible de $\sqrt[3]{2}$.

$\mathbb{Q} 
\subset \mathbb{Q}(\sqrt[3]{2}) 
\subset \mathbb{Q}(\sqrt[3]{2}, i)$

**** 5.2.4. Clausura normal
Llamamos clausura normal de $F/K$ a:

\[ E = \bigcap \{ H \mid H \supset F;\; H/K\text{ normal}\}\]

**** 5.2.5. Existencia de la clausura normal
Para toda extensión algebraica existe una clausura normal. 

***** Demostración
Trivial porque la clausura algebraica es normal.

**** 5.2.6. Unicidad de la clausura normal
La clausura normal es única salvo isomorfismos.

***** Demostración
Sean las dos clausuras sobre la clausura algebraica absoluta.
Como son intersección de normales y ambas lo son, deben ser
la misma.

**** 5.2.7. Extensión de homomorfismos a extensión normal
Sea $K \subset F \subset E$ con $E/K$ normal. Todo $\tau : F \longrightarrow E$ extiende a
un $\tau : E \longrightarrow E$. 

***** Demostración
Tenemos que extiende a $\tau : F \longrightarrow \overline{K}$ y de ahí, por ser $E$ algebraica,
se [[*4.2.10. Extensión de homomorfismos a algebraicas][prolonga]] a $\tau : E \longrightarrow \overline{K}$. Por ser normal, $\tau(E) = E$.

**** 5.2.8. Clausura de una extensión finita
Sea $F = K(u_1,u_2,\dots,u_n)$ y $f_i = Irr(u_i,K)$; entonces la clausura normal
es el cuerpo de descomposición de $f = f_1f_2\dots f_n$.

***** Demostración
Es normal por ser cuerpo de descomposición de un polinomio, es
la mínima porque debe contener las raíces de $f_i = Irr(u_i,K)$
para ser normal, y si las contiene, debe contener al cuerpo de
descomposición de $f$.

**** 5.2.9. La clausura normal de extensión finita es finita
Sea $F/K$ finita, su clausura normal es finita.

***** Demostración
El cuerpo de descomposición anterior es finito porque puedo
crearlo insertando las raíces de cada polinomio.

**** 5.2.10. Polinomio normal
Un polinomio irreducible es normal si en toda extensión
algebraica $F/K$ con una raíz de $f$, descompone en factores 
lineales.

**** 5.2.11. Caracterización de polinomios normales
Sea $f$ un polinomio en $K[X]$, equivalen:

 1. $f$ es normal sobre $K$.
 2. El cuerpo de descomposición de $f$ es $K(u)$, una raíz de $f$.
 3. Todas las raíces de $f$ es expresan como polinomios de una de ellas.

***** Demostración
****** Implicación 1 a 2
Tenemos que en $K(u)$, $f$ descompone en factores lineales,
luego es cuerpo de descomposición.

****** Implicación 2 a 3
Trivial.

****** Implicación 3 a 1
Una extensión con una raíz $u$ contendría a $K(u)$, y como todas
las raíces se expresan como polinomios de $u$, contendría a todas
las raíces y $f$ descompondría en polinomios lineales.

**** DONE Caracterización de extensiones normales en torres
 #+begin_theorem
 Sea $K\subset F \subset E$ extensiones finitas, $E$ extensión normal. Equivalen:

   - $F/K$ es normal.
   - Para cada $\sigma : E/K \longrightarrow E/K$, se tiene $\sigma(F) = F$.
 #+end_theorem

 Sea $F/K$ normal, podemos cambiar el dominio y codominio para tener un
 $\sigma : F/K \longrightarrow \overline{K}/K$. Por ser $F$ normal, $\sigma(F) = F$.

 Sea $\sigma : F/K \longrightarrow \overline{K}/K$, podemos extenderla finitamente hasta $E$ como $\overline\sigma$. Como $E/K$ es
 normal, $\overline\sigma(E) = E$; y así tengo un $\overline\sigma : E/K \longrightarrow E/K$, que me da $\sigma(F)=\overline\sigma(F) = F$.

*** 5.3. Extensiones separables
**** 5.3.1. Elemento separable
Un elemento algebraico $u$ es separable en $K$ si $Irr(u,K)$ no tiene 
raíces múltiples.

**** 5.3.2. Extensiones separables
Una extensión algebraica $F/K$ es *separable* si todos sus elementos 
lo son.

***** Ejemplo de extensión normal no separable
Existen [[http://math.stackexchange.com/questions/982702/example-of-a-non-separable-normal-extension][ejemplos]] de extensiones normales no separables.

**** 5.3.3. Torres separables
Si $E \supset F \supset K$ es extensión separable, lo son $E/F$ y $F/K$.

***** Demostración
Que $F/K$ es separable es trivial. Y $E/F$ es separable porque
$Irr(u,F) \mid Irr(u,K)$, y si el segundo no tiene raíces múltiples,
no puede tenerlas el primero.

**** 5.3.4. Grado separable
El *grado separable* es cardinal del conjunto de homomorfismos de
$F/K \longrightarrow \overline{K}/K$.

\[ [F : K]_s = \#\{F/K \longrightarrow \overline{K}/K\}\]

**** 5.3.5. Grado separable en torres
Sea $K \subset F \subset E \subset \overline{K}$ entonces $[E:K]_s = [E:F]_s[F:K]_s$.

***** Demostración
****** Con dos homomorfismos construyo el mayor
Dados $\sigma : F/K \longrightarrow \overline{K}/K$ y $\psi : E/F \longrightarrow \overline{K}/F$, puedo extender $\sigma$ al
algebraico $E$ para tener $\sigma^\ast : \overline{K}/K \longrightarrow \overline{K}/K$. Ahora, la composición
$\sigma^\ast \circ \psi$, me da el homomorfismo buscado.

****** Y es único
Supongamos que $\sigma^\ast \circ \psi = \sigma'^\ast \circ \psi'$; como deben ser iguales para 
cualquier $f \in F$, se tiene $\sigma = \sigma'$. Ahora, si tomamos la misma extensión
para cada elemento, $\sigma^\ast = \sigma'^\ast$ y entoces por inyectividad $\psi = \psi'$.

Así tenemos ya:

\[ [E:K]_S \geq [E:F]_S[F:K]_S \]

****** Con el mayor construyo los dos menores
Dado un $\sigma : E/K \longrightarrow \overline{K}/K$, para cada $\sigma|_F$ construyo una extensión
a la clausura, $\tau : \overline{K}/K \longrightarrow \overline{K}/K$, que es isomorfismo. Como 
es isomorfismo podemos construirle inversa para tener 
$\tau^{-1} : \overline{K}/K \longrightarrow \overline{K}/K$. A la vez, podemos extender todos los $\sigma$ a
la clausura algebraica como $\sigma^\ast$.

Ahora bien, como $\tau|_F = \sigma|_F$, tenemos que $\sigma^\ast \circ \tau^{-1}$ deja fijo a $F$. 
Así, hemos partido $\sigma$ en $\sigma^\ast \circ \tau^{-1}|_E$ y $\sigma|_F$.

****** Y son únicas
Supongamos que tenemos $\sigma|_F = \sigma'|_F$, entonces las dos extensiones $\tau$
las hemos tomado iguales. Si tenemos $\sigma \circ \tau^{-1}|_E = \sigma' \circ \tau^{-1}|_E$, como $\tau^{-1}$
es sobreyectiva, tenemos $\sigma^\ast|_E = \sigma'^\ast|_E$, y por tanto $\sigma = \sigma'$.

Así tenemos que:

\[ [E:K]_S \leq [E:F]_S[F:K]_S \]

**** 5.3.6. Relación de grado y grado separable
Sea $F/K$ extensión finita, entonces $[F:K]_s \mid [F:K]$.

***** Demostración
****** Caso simple
Sea $K(u)$ la extensión, con polinomio $Irr(u,K)$ de grado $n$. 
Si la raíz $u$ tiene multiplicidad $m$ en el polinomio, todas las
raíces del polinomio tienen la misma multiplicidad y hay
$n/m$ raíces distintas.

Cada homormofismo quedará determinado por la imagen de $u$, y
tenemos $n/m$ imágenes distintas para $u$.

****** Caso compuesto
Ahora procedemos por inducción sobre el grado de la extensión. 
Tomamos un elemento de la base y hacemos:

\[ [F:K(u)]_S[K(u):K]_S  \mid  [F:K(u)][K(u):K] \]

Lo primero es divisible por el caso simple y lo segundo por
hipótesis de inducción.

**** 5.3.7. Igualdad de grados en torres
Sea $K \subset F \subset E$, con $E/K$. Entonces $[E:K]_s = [E:K]$ ssi 
$[E:F]_s = [E:F]$ y $[F:K]_s = [F:K]$.

***** Demostración
Tenemos que deben tenerse ambos casos de igualdad en:

\[
[E:K]_S = [E:F]_S[F:K]_S \leq [E:F][F:K] = [E:K]
\]

**** 5.3.8. Caracterización de extensión separable
La extensión finita $E/K$ es separable ssi $[E:K]_s = [E:K]$.

***** Demostración
Usamos la idea de la demostración de la relación con el grado.
En el caso simple tenemos $[K(u):K]_S = [K(u):K]$ sólo cuando la
multiplicidad de cada raíz es uno. En el caso compuesto exigimos
eso a cada paso.

**** 5.3.9. Extensión por un conjunto separable
Para $K(S)/K$ algebraica es separable ssi todo elemento de $S$ es
separable sobre $K$.

***** Demostración
La suma o producto de elementos separables [[http://math.stackexchange.com/a/82837/85067][es separable]].

**** 5.3.10. Propiedades de extensiones separables
Las extensiones separables cumplen:

 1. Para $K \subset F \subset E$, se tiene $E/K$ separable ssi 
    $E/F$ y $F/K$ separables.
 2. Sea $E/K$ algebraica separable y $H/K$ extensión, entonces $EH/H$ 
    es separable.
 3. Sean $E/K$ y $F/K$ separables, entonces $EF/K$ es separable.

***** Demostración
****** Punto 1
Simplemente usar que la igualdad de grados de separabilidad
se da en [[*5.3.7. Igualdad de grados en torres][torres]] y que equivale a la [[*5.3.8. Caracterización de extensión separable][separabilidad]].

****** Punto 2
Tenemos $EH/H = H(E)/H$. Que sea separable [[*5.3.9. Extensión por un conjunto separable][equivale]] a que
cada elemento de $E$ lo sea. Pero $Irr(e,H) \mid Irr(e,K)$, y si uno
tiene sólo raíces simples, el otro las tendrá también.

****** Punto 3
Tenemos $K(E,F)/K$ separable por serlo todos los elementos en 
$E,F$.

**** 5.3.11. La clausura normal de una separable es separable
Sea $F/K$ separable y $E/K$ su clausura normal. Entonces $E/K$ es
separable.

***** Demostración
Si extiendo $F$ con todas las raíces de los irreducibles de sus
elementos, tengo la clausura normal; porque debe contenerlas
y es normal, así que es la mínima. Como cada una de ellas es
separable porque tiene como irreducible el mismo irreducible
de un elemento de $F$, la [[*5.3.9. Extensión por un conjunto separable][extensión entera]] es separable.

**** 5.3.12. Clausura separable
El conjunto de todos los elementos de $\overline{K}$ separables sobre $K$ forman
un subcuerpo $K^{sep}$ que se llama *clausura separable*.

***** Demostración
La suma o producto de elementos separables es separable, como
demostramos [[*5.3.9. Extensión por un conjunto separable][anteriormente]].

**** 5.3.13. Teorema del elemento primitivo
Sea $E/K$ una extensión finita. Es simple ssi el conjunto de
cuerpos intermedios $\{ F \mid K \subset F \subset E\}$ es finito.

***** Demostración: si hay finitos subcuerpos es simple
****** Cuerpo base finito
Como la extensión es finita, tenemos que hay finitos elementos.
El grupo multiplicativo de un cuerpo es [[http://mathoverflow.net/a/54741/45365][cíclico]], luego es simple.

****** Cuerpo base infinito
Siendo $E = K(a_1,a_2,\dots,a_n)$, nos limitamos a probar $K(a,b)$ simple.
Consideramos las $K(a+xb)$ para $x \in F$. Como los elementos de $F$ son 
infinitos y las extensiones intermedias finitas, se tienen $x \neq y$
tales que $K(a+xb) = K(a+yb)$, y por tanto:

\[
b = \frac{ a +bx - (a + by)}{x-y} \in K(a+bx)
\]

***** Demostración: si es simple, hay finitos subcuerpos
Sea $K(\alpha)$ simple, como es finita es algebraica, y $Irr(a,K)$ tiene 
finitos divisores en la clausura. 

Para cada divisor del polinomio irreducible creo $K[|p|]$, el subcuerpo
generado por los coeficientes del polinomio. Todo cuerpo intermedio $E$,
en el que el irreducible de $\alpha$ sea $p$ contendrá a $K[|p|]$, pero como:

\[ [K(\alpha): E] = [K(\alpha) : K[|p|]] \]

Se tendrá forzosamente $E = K[|p|]$.

***** Contraejemplo:
Hay extensiones finitas de cuerpos que no son simples

**** 5.3.13. Finita y separable es simple
Una extensión finita y separable es simple.

***** Demostración
Si es finita y separable, podemos tomar su clausura normal.
La clausura normal de una extensión finita es [[*5.2.8. Clausura de una extensión finita][finita]], así que
tengo una extensión de Galois finita. Habrá finitas subextensiones
porque habrá finitos subgrupos de Galois.

***** Contraejemplo: cuerpo finito no simple
Tenemos [[https://en.wikipedia.org/wiki/Primitive_element_theorem#Counterexamples][contraejemplos]] en característica $p$ con cuerpos de
dimensión $p^2$.

**** 5.3.14. Endomorfismo de Frobenius
Sea $K$ de característica de $p$. Llamamos *endomorfismo de Frobenius* 
a $\phi(u) = u^p$.

**** 5.3.15. Cuerpos perfectos
Para $K$ cuerpo equivalen:

 1. Todo $f \in K[X]$ irreducible tiene raíces simples.
 2. Toda $E/K$ algebraica es separable.
 3. Toda $E/K$ finita es separable.
 4. $car(K)=0$ ó $car(K)=p$, y el endomorfismo de Frobenius es 
    sobreyectivo.

Y en ese caso, llamamos a $K$ *cuerpo perfecto*.

***** Demostración
****** Implicación 1 a 2
Todo elemento de la extensión cumple algún polinomio,
y son todos separables, luego es separable.

****** Implicación 2 a 3
Trivial

****** Implicación 3 a 4
Supongamos que no fuera sobreyectivo, existe $y \neq x^p$.
Si tomamos $(x^p-y)$, tenemos que es irreducible. Y si creamos
entonces $z^p = y$ tendríamos una extensión no finita no
separable.

# TODO: ¿Por qué es irreducible? Por Eisenstein en algún cuerpo 
# raro podría tenerse.

****** Implicación 4 a 1 en característica 0
En característica $0$, un polinomio no puede dividir a su derivada,
que será de grado menor, así que un irreducible no puede tener raíces
dobles.

****** Implicación 4 a 1 en característica p
Para que un irreducible divida a la derivada, que debe ser de grado
menor, se debe tener que sea $0$. El polinomio original debe ser
de la forma  $f(x^p)$ para que al derivarlo se anule.

Cuando Frobenius es automorfismo, podemos escribir $f(x^p) = (g(x))^p$,
contraviniendo irreducibilidad. Todos los polinomios deben tener
raíces simples.

**** 5.3.17. Ejemplos de cuerpos perfectos
Son perfectos:

 1. Todo cuerpo de característica cero.
 2. Todo cuerpo finito.
 3. Todo cuerpo algebraicamente cerrado.
    
***** Demostración
****** Punto 1
Trivial desde la [[*5.3.15. Cuerpos perfectos][caracterización]].
****** Punto 2
En todo cuerpo finito, un endomorfismo es sobreyectivo.
****** Punto 3
En un algebraicamente cerrado, todo irreducible es lineal y
tiene raíces simples.

*** 5.4. Derivada y raíces múltiples
**** 5.4.1. Raíces
Sea $f\in F[X]$ y $u \in F$. Es raíz de multiplicidad $k$ cuando $f = (X-u)^kf_0$,
con $f_0(u) \neq 0$.

**** 5.4.2. Derivada
Se define la derivada de $f$ como:

\[ f' = \sum_{i=1}^n ia_iX^{i-1} = na_nX^{n-1} + \dots + a_1\]

**** 5.4.3. Propiedades de la derivada
La derivada verifica:

 1. $(f+g)' = f'+g'$
 2. $(f \cdot g)' = f' \cdot g'$
 3. $(f^m)' = mf^{m-1}\cdot f'$

**** 5.4.4. Condición de raíces simples
Las raíces de $f$ son simples ssi $mcd(f,f') = 1$.

**** 5.4.5. Condición de raíces simples en irreducibles
Sea $f$ irreducible con $f' \neq 0$, las raíces de $f$ son simples.

**** 5.4.6. Propiedades de las raíces simples
Se cumple:

 1. En característica $0$, todo irreducible tiene raíces simples.
 2. En característica $p$ prima, un irreducible tiene raíces múltiples
    ssi $f(x) = g(x^p)$.

**** 5.4.7. Polinomio separable
Un polinomio $f \in K[X]$ se llama separable $K$ si sus factores 
irreducibles tienen sólo raíces simples.

** 6. Teoría de Galois Finita
*** 6.1. Grupos de automorfismos
**** 6.1.1. Espacio vectorial de las aplicaciones de un conjunto
Sea $S$ un conjunto. Las aplicaciones $Fun(S,F)$ con la suma y producto
de escalares elevados desde $F$ forman un espacio vectorial de dimensión
$|S|$.

***** Demostración
Simplemente comprobar que cumplen las propiedades de espacio 
vectorial.

**** 6.1.2. Proposición al lema de Dedekind
Sean $\sigma_1,\dots, \sigma_m : G \longrightarrow F^\times$ homomorfismos desde un grupo $G$. 
Son distintos ssi son linealmente independientes sobre $F$.

***** Demostración
****** Caso base
Procedemos por inducción. Cuando $n=1$, es trivial.

****** caso inductivo
Tomemos un subconjunto mínimo de linealmente dependientes, 
$\sigma_1,\dots,\sigma_s$. Tendríamos $b_i \in F^\times$:

\[\sigma_s = b_1\sigma_1 + \dots + b_{s-1}\sigma_{s-1}\]

Si tomamos ahora $y$ tal que $\sigma_1(y) \neq \sigma_s(y)$, evaluamos en $xy$ y 
multiplicamos por $\sigma_s(y)$ obtenemos dos ecuaciones que restadas
dan:

\[0 = b_1(\sigma_1(y)-\sigma_s(y))\sigma_1 + \dots + b_{s-1}(\sigma_{s-1}(y) - \sigma_s(y)) \sigma_{s-1}\]

Relación de dependencia no nula menor que la anterior.

**** 6.1.3. Lema de Dedekind
Un conjunto de homomorfismos de cuerpos $F_1 \longrightarrow F_2$ distintos
son linealmente independientes sobre $F_2$.

***** Demostración
Desde el lema anterior, tenemos que son linealmente independientes 
una vez restringidos a $F_1^\times \longrightarrow F_2^\times$, el añadirles el $0$ no los vuelve 
dependientes.

**** 6.1.4. Acotación del número de homomorfismos sobre K
Existen como máximo $[F:K]$ morfismos distintos sobre $K$ 
hacia cualquier otro cuerpo:

\[|Hom(F/K,E/K)| \leq [F:K]\]

***** Demostración
Sea $\{u_1,\dots,u_n\}$ base de $F$, y supongamos $n+1$ homomorfismos
distintos. El siguiente sistema de ecuaciones tiene solución 
no trivial porque tiene $n+1$ incógnitas y tiene $n$ ecuaciones.
      
\[ X_1\sigma_1(u_j) + \dots + X_{n+1}\sigma_{n+1}(u_j) = 0 
\qquad j = 1,\dots,n\]
      
Pero una solución es una relación de dependencia sobre toda la
base de $F$. Si son dependientes, por Dedekind son [[*6.1.3. Lema de Dedekind][iguales]].

**** 6.1.5. Grupo de extensión
Para toda extensión finita $F/K$ llamamos *grupo de la extensión* a:

\[ G(F/K) = \{ \sigma \in  Aut(F) \mid \forall u \in K : \sigma(u) = u\}\]

**** 6.1.6. Acotación de elementos del grupo
Para toda extensión finita $F/K$ se verifica que $|G(F/K)| \leq [F : K]$.

***** Demostración
Caso particular de la [[*6.1.4. Acotación del número de homomorfismos sobre K][acotación]] del número de homomorfismos
sobre el cuerpo.

**** 6.1.7. Cuerpo fijo
Sea $G < Aut(E)$, llamamos *cuerpo fijo* por $G$ al conjunto al que 
dejan fijos todos los elementos de $G$:

\[ E^G = \{ u \in E \mid \forall\sigma\in G: \sigma(u) = u\}\]

Es un *subcuerpo* de $E$.

***** Demostración: es un subcuerpo
Trivialmente desde la definición de automorfismo de cuerpos.

**** 6.1.8. Teorema de Artin
Para $G < Aut(E)$ finito, $[E : E^G] = |G|$.

***** Demostración
Ya tenemos $n = |G| \leq [E : E^G]$. Supongamos la 
desigualdad estricta con $u_1,\dots,u_{n+1} \in E$ independientes sobre $E^G$. 
Y tenemos el sistema de $n+1$ incógnitas y $n$ ecuaciones, sobre los
$\sigma_j$ de $G$:

\[ X_1\sigma_j(u_1) + \dots + X_{n+1}\sigma_j(u_{n+1}) = 0 \qquad
j = 1,\dots,n\]
      
Sea $a_1,\dots,a_{n+1}$ una solución no trivial con número mínimo de 
elementos no nulos. Suponemos s.p.g. que $a_1 \neq 0$ y despejamos para 
tener:

\[\sigma_j(u_1) = b_2\sigma_j(u_2) + \dots + b_n\sigma_j(u_{n+1}) 
\qquad j = 1,\dots,n\]
      
En particular, en el caso $\sigma_j = id$, tenemos:

\[u_1 = b_2u_2 + \dots + b_{n+1}u_{n+1}\]

que obliga a que uno de los coeficientes no esté en $E^G$. Supongamos 
s.p.g. que $b_2 \notin G$, y sea $\tau \in G$ tal que $\tau(b_2) \neq b_2$. Si aplicamos ahora
$\tau$ a cada una de las ecuaciones y restamos tenemos:

\[0 = (b_2-\tau(b_2))\sigma_j(u_2) + \dots + (b_n-\tau(b_n)\sigma_j(u_{n+1}))
\qquad j = 1,\dots,n\]

Esta solución es no trivial porque $(b_2-\tau(b_2)) \neq 0$, pero tiene más 
elementos nulos que la anterior.

**** 6.1.9. Extensión de Galois
Una extensión finita $E/K$ es *de Galois* cuando:
     
\[\exists Gal(E/K) < Aut(E): E^{Gal(E/K)} = K\]

Llamamos a $Gal(E/K)$ el *grupo de Galois* de la extensión.

**** 6.1.10. Caracterización de extensión de Galois
Una extensión finita es de Galois ssi es normal y separable.

***** Demostración
Sea $Gal(E/K) = G$, entonces cada automorfismo se extiende a un 
homomorfismo $\sigma' : E/K \longrightarrow \overline{K}/K$, luego $[E:K]_S \geq |G| = [E:K] \geq [E:K]_S$, 
y por tanto la extensión es separable. Como además el número total 
de homomorfismos es $[E:K]_S = |G|$, son todos automorfismos y la 
extesión es normal.

Sea $E/K$ normal y separable. Tenemos $n = [E:K]_S = [E:K]$ 
homomorfismos $\tau : E \longrightarrow \overline{K}$ sobre $K$, con $\tau(E) = E$. Entonces 
$G = G(E/K)$ tiene orden $n$. Por Teorema de Artin, 
$[E:E^G] = n = [E:K]$, luego $[E^G:K] = 1$.

**** 6.1.11. Caracterización para extensiones finitas
Sea $F/K$ una extensión separable finita y sea $E/K$ su clausura 
normal. Entonces $E/K$ es una extensión finita de Galois.

***** Demostración
La clausura normal de una extensión separable es [[*5.3.11. La clausura normal de una separable es separable][separable]].

*** 6.2. Correspondencia de Galois, caso finito
**** Correspondencia
Definimos una correspondencia entre los subgrupos de una 
extensión de Galois y los cuerpos intermedios como:

\[ H^\ast = E^H = \{u \in E \mid \forall\sigma\in H: \sigma(u)=u\}\]
\[F^\ast = Gal(E/F) = \{\sigma\in G \mid \forall u\in F: \sigma(u) = u\}\]

**** 6.2.1. Contenidos de cuerpos
Sean $F_i$ cuerpos intermedios de $E/K$ y $H_i$ subgrupos de $Gal(E/K)$.
Se cumple:

 1. Si $F_1 \subset F_2$, entonces $F_1^\ast \supset F_2^\ast$.
 2. Si $H_1 \subset H_2$, entonces $H_1^\ast \supset H_2^\ast$.
 3. $F \subset F^\ast^\ast$; $H < H^\ast^\ast$.
 4. $F^\ast = F^\ast^\ast^\ast$; $H^\ast = H^\ast^\ast^\ast$.

***** Demostración
1. Trivial.
2. Trivial.
3. Trivial.
4. Componiendo el apartado 3 con el 2 y el 1.

**** 6.2.2. Correspondencia de Galois
El par de aplicaciones $\ast$ se llama *correspondencia de Galois*.

**** 6.2.3. Teorema fundamental
Sea $E/K$ una extensión de Galois finita con $G = Gal(E/K)$:

  1. La correspondencia es biyección, ${\cal F}(E/K) \cong {\cal S}(G)$.
  2. $A \supset B$ ssi $B^\ast \subset A^\ast$.
  3. La correspondencia es un antiisomorfismo de retículos, 
     $(F_1 \cdot F_2)^\ast = F_1^\ast \cap F_2^\ast$ y $(F_1 \cap F_2)^\ast = F_1^\ast \vee F_2^\ast$.
  4. Las extensiones $F_1/K$ y $F_2/K$ son conjugadas ssi los subgrupos
     $F_1^\ast$ y $F_2^\ast$ son conjugados en $G$.
  5. La extensión $F/K$ es normal ssi $F^\ast$ es un subgrupo normal de $G$.
     En este caso $Gal(F/K) \cong G/F^\ast$.
  6. Para $H<G$ se verifica $|H| = [E:H^\ast]$ y $[G:H]=[H^\ast:K]$. 
     Para $F \in {\cal F}(E/K)$ se verifica $[E:F] = |F^\ast|$ y $[F:K] = [G:F^\ast]$.
 
***** Demostración de 6 para grupos
Por [[*6.1.8. Teorema de Artin][teorema de Artin]] tenemos $|G| = [E:K]$ y $|H| = [E : H^\ast]$.
Por teorema de Lagrange y teorema del grado tenemos:

\[ |G| = [G:H] |H| \]
\[ [E:K] = [E:H^\ast][H^\ast : K] \]

Simplificando obtenemos $[G:H] = [H^\ast : K]$.

***** Demostración de 6 para cuerpos intermedios
Como $E/F$ es de Galois, $|F^\ast| = [E:F]$ y sabemos $|G| = [E:K]$.
Volvemos a aplicar Lagrange y teorema del grado.

***** Demostración de 1
Tenemos la torre $G > H^\ast^\ast > H$ y:

\[ [G : H^\ast^\ast] = [H^\ast^\ast^\ast : K] = [H^\ast : K] = [G : H] \]

Tenemos la torre $E \supset F^\ast^\ast \supset F$ y:

\[ [E:F] = |F^\ast| = |F^\ast^\ast^\ast| = [E : F^\ast^\ast] \]

***** Demostración de 2 y 3
Se cumple por ser biyección y la proposición anterior. Trivial
desde esto el antiisomorfismo de retículos.

***** Demostración 4
Sean $F_2 = \sigma(F_1)$, para $\tau \in F_1^\ast$, tenemos $\sigma\tau\sigma^{-1} \in F_2^\ast$.
Luego $\sigma F_1^\ast \sigma^{-1} \subset F_2^\ast$. Aplicando lo mismo sobre $\sigma^{-1}$ llegamos
a la otra igualdad. Dando la vuelta al razonamiento, tenemos
que $\sigma F_1^\ast \sigma^{-1} = F_2^\ast$ nos da $F_2 = \sigma(F_1)$.

***** Demostración 5
Desde el cuarto apartado, se conserva normalidad.

Si aplicamos primer Teorema de Isomorfía a la restricción
$\Phi : Gal(E/K) \longrightarrow Gal(F/K)$:

\[ \frac{Gal(E/K)}{F^\ast} 
= \frac{Gal(E/K)}{\ker(\Phi)} 
\cong \im(\Phi) 
= Gal(F/K) \]

El que $\im(\Phi) = Gal(F/K)$ usa la normalidad de $F$.

*** 6.4. Propiedades de las extensiones de Galois
**** 6.4.1. Subgrupos en Galois
Sean $E \supset F \supset K$ con $E/K$ Galois finita. Entonces $E/F$ es Galois
finita y $Gal(E/F)$ es subgrupo de $Gal(E/K)$.

***** Demostración
La finitud se tiene trivialmente. La normalidad se tiene
sobre cuerpos [[*5.2.3. Propiedades de las extensiones normales][intermedios]] y la separabilidad [[*5.3.10. Propiedades de extensiones separables][también]]. Y sabemos
que normal y separable es de [[*6.1.10. Caracterización de extensión de Galois][Galois]].

Que uno es subgrupo de otro está claro por las propiedades de la
[[*6.2.3. Teorema fundamental][correspondencia]].

**** 6.4.2. Extensiones abelianas, cíclicas y solubles
Una extensión finita se dice *abeliana, cíclica o soluble* si es
de Galois y su grupo lo es.

**** 6.4.3. Subgrupos abelianos, cíclicos y solubles
Sea $E\supset F\supset K$ con $E/K$ finita y *abeliana, cíclica o soluble*,
entonces $E/F$ también es abeliana, cíclica o soluble, respectivamente.

***** Demostración
Tenemos que el subgrupo de un abeliano, cíclico o soluble
es también abeliano, cíclico o soluble.

**** 6.4.4. Subextensiones finitas abelianas y cíclicas
Sean $K \subset F \subset E$ torre de extensiones *finitas*:

 1. Si $E/K$ es Galois abeliana, $F/K$ es Galois abeliana.
 2. Si $E/K$ es Galois cíclica, $F/K$ es Galois cíclica.

***** TODO Demostración

**** 6.4.5. Galois para cuerpos compuestos
Sea $E/K$ Galois finita y $F/K$ extensión con $E,F \subset L$.
Entonces $EF/F$ y $E/(E\cap F)$ son extensiones finitas de Galois.
Además la aplicación restricción $\sigma \mapsto \sigma|_E$ define un isomorfismo:

\[ \bullet|_E : Gal(EF/F) \longrightarrow Gal(E/(E\cap F)) < Gal(E/K) \]

***** TODO Demostración

**** 6.4.6. Relación de Galois con el grado
Sea $E/K$ extensión finita de Galois y sea $F/K$ con $E,F \subset L$.
Entonces $[EF:F] \mid [F:K]$.

***** TODO Demostración

**** 6.4.8. Composición de extensiones de Galois
Sean $E_1/K$, $E_2/K$ extensiones Galois finitas con $E_1,E_2 \subset L$.
Entonces $E_1E_2/K$ es extensión finita de Galois y existe un
monomorfismo restricción:

\[
\lambda : Gal(E_1E_2/K) 
\longrightarrow 
Gal(E_1/K) \times Gal(E_2/K)
\]

Cuando además tenemos $E_1 \cap E_2 = K$, $\lambda$ es un isomorfismo.

***** TODO Demostración
**** 6.4.9. Extensión del producto
Sean $E_i/K$ extensiones contenidas en un $L$ con grupos de Galois
$G_1,G_2,\dots,G_n$. Si cumplena demás $E_i \cap (E_1E_2\dots E_{i-1}) = K$, entonces:

\[Gal(E_1E_2\dots E_n/K) \cong G_1 \times G_2 \times \dots \times G_n\]

***** TODO Demostración
**** 6.4.10. Cuerpos fijos del producto
Sea $E/K$ una extensión finita de Galois con grupo $G = G_1 \times \dots \times G_n$.
Sea $E_i$ el cuerpo fijo de $G_1 \times \dots \times \{1\} \times \dots \times G_n$. Entonces $E_i/K$ es
de Galois con grupo $Gal(E_i/K) = G_i$, $E_i \cap (E_1\dots E_{i-1})$ y $E=E_1\dots E_n$.

***** TODO Demostración

** 7. Cuerpos finitos
*** 7.1. Estructura de los cuerpos finitos
**** 7.1.1. Propiedades de un cuerpo finito
Sea $F$ cuerpo finito con $|F| = q$,

 1. $car(F) = p$ es un primo
 2. El cuerpo primo es $\mathbb{Z}/p\mathbb{Z}$
 3. $F/\mathbb{Z}_p$ es extensión finita
 4. $[ F : \mathbb{Z}_p] = n$, entonces $|F| = p^n$
 5. $F^\times$ es cíclico de orden $|F|-1$
 
***** TODO Demostración

**** 7.1.2. Clasificación de cuerpos finitos
Dos cuerpos finitos del mismo cardinal son isomorfos. De hecho,
son el cuerpo de descomposición de $X^{|F|} - X$.

***** TODO Demostración

**** 7.1.3. Existencia de cuerpos finitos
Dado $p$ primo, existe cuerpo de $p^n$ elementos.

***** Demostración
Sea $f(x) = x^{p^n}-x$ polinomio en $\mathbb{Z}_p$. Su derivada es $-1$ y no tiene raíces, 
luego tiene sólo raíces simples. Vemos que con sólo añadir las 
raíces del polinomio, llega a ser cuerpo de descomposición.

Sean $u,v \in S$, conjunto de raíces:

- $(u+v)^{p^n} - (u+v) = u^{p^n}-u+v^{p^n}-v = 0$
- $(uv)^{p^n}-uv = u^{p^n}v^{p^n} - uv = 0$
- $(-u)^q - (-u) = 0$
- $(u^{-1})^q-u^{-1} = 0$

**** 7.1.4. Teorema de Moore
Para cada $p^n$ existe exactamente un cuerpo con $p^n$ elementos; 
que es el cuerpo de descomposición de $x^{p^n}-x$ sobre $\mathbb{Z}_p$. 
No existen otros cuerpos finitos.

***** Demostración
Sabemos los cuerpos [[*7.1.1. Propiedades de un cuerpo finito][deben]] tener cardinal $p^n$ y que dos cuerpos
con el mismo cardinal son [[*7.1.2. Clasificación de cuerpos finitos][isomorfos]]. Además, sabemos que
[[*7.1.3. Existencia de cuerpos finitos][existe]].

**** 7.1.5. Cuerpos de Galois
Notamos por $GF(p^n)$ o $\mathbb{F}_{p^n}$ al único cuerpo con esa cardinalidad,
lo llamamos cuerpo de Galois de orden $p^n$.

*** Factorización de polinomios
**** Clasificación de polinomios irreducibles
 #+begin_theorem
 Los factores irreducibles de $x^{p^n}-x$ son exactamente los polinomios irreducibles de
 $\mathbb{F}_p[X]$ cuyo grado divida a $n$.
 #+end_theorem
** 8. Extensiones ciclotómicas
*** Raíces de la unidad
**** Subgrupos finitos del grupo multiplicativo
 Todo subgrupo finito del grupo multiplicativo $K^\times$ es cíclico.

**** Grupo de raíces n-ésimas
 Llamamos *grupo de raíces n-ésimas* de la unidad al grupo siguiente. Cualquier
 generador del grupo se llama una *raíz primitiva* de la unidad.

 \[ \mu_n = \{\zeta\in K \mid \zeta^n = 1\} \]

*** Polinomios ciclotómicos
**** Definición
 Se llama *polinomio ciclotómico* al polinomio:

 \[\Phi_n = \prod_{\zeta \text{ primitiva}} (X -\zeta)\]

 Se cumple que $\operatorname{grad}(\Phi_n) = \phi(n)$.

**** Cálculo de los polinomios ciclotómicos
 Tenemos que:

 \[\Phi_n  = \frac{X^n-1}{\prod_{d|n, d\neq n} \phi_d}\]

**** Función de Möbius
 Se define $\mu : \mathbb{N} \longrightarrow \mathbb{Z}$ como:

 \[\mu(n) = \threepartdef
 {0}{\exists p: \text{ primo con } p^2|n}
 {(-1)^r}{n = p_1p_2\dots p_n \text{ primos distintos}}
 {1}{n=1}\]

*** Extensiones ciclotómicas
* Álgebra Conmutativa y Computacional
** 1. Anillos e ideales
*** La categoría CRing
**** Categoría
Consideramos la categoría de los *anillos conmutativos*, que consta de
los anillos conmutativos como objetos y de los homomorfismos de
anillos; respetando suma, producto y unidad; como morfismos.

**** Z es objeto inicial
El anillo $\mathbb{Z}$ es inicial en =CRing=. El homomorfismo único
queda unívocamente definido con $f(1) = 1$ y $f(n) = nf(1)$.

**** Subanillos
*Subanillo*. Subconjunto cerrado para la suma y el producto
conteniendo a 1.

**** Monomorfismos y epimorfismos
En =CRing= coinciden la inyectividad con ser monomorfismo y la
sobreyectividad con ser epimorfismo.

*** Ideales
**** Definición
Un *ideal* es un subconjunto cerrado para la suma y el producto de cualquier 
elemento desde $R$.

**** Retículo de ideales
Los ideales forman un retículo con la suma y la intersección.
Dos ideales son *primos relativos* cuando suman el anillo.

**** Generación de ideales
Llamamos $(X)$ al *ideal generado* por $X$; el menor ideal que 
contiene a $X$:

\[ (X) = \bigcap_{X \subset \alpha \text{, ideal}} \alpha\]

Lo llamamos *ideal finitamente generado* cuando $X$ es finito,
cumpliéndose:

\[ (X) = \left\{ \sum^{finita} r_ix_i \mid r_i \in R, x_i \in X\right\}\]

E *ideal principal* cuando $X$ tiene un sólo elemento.

**** Producto de ideales
Se define para $\alpha, \beta$ ideales como:

\[ \alpha\beta = \left\{ xy \mid x\in\alpha, y\in\beta \right\}\]

**** Ideales primos relativos
Dos ideales son *primos relativos* cuando $\alpha+\beta = R$. Cuando son primos relativos
se cumple que $\alpha\beta = \alpha \cap \beta$.

*** Anillo cociente
**** Definición
Sea $R$ anillo y $\alpha \subset R$ ideal, tomamos la relación de equivalencia
$x \sim y \Leftrightarrow x-y \in \alpha$, para obtener:

\[ R/\alpha = \{x+\alpha \mid x\in R\}\]

**** Ideales del anillo cociente
Los ideales del anillo cociente sobre $\alpha$ están en correspondencia biyectiva
con los ideales que lo contienen, $\beta \supset \alpha$. Son siempre de la forma $\beta/\alpha$.

**** Primer Teorema de Isomorfía
Para cualquier homomorfismo de anillos $f$:

\[ R/\ker(f) \cong \im(f)\]

Y además, los ideales están en correspondencia biyectiva por $f_\ast$ y $f^{-1}$:

\[ \{ \alpha \mid \ker(f)\subset\alpha \} 
\longleftrightarrow 
\{ \beta \mid \beta \subset \im(f)\}\]

***** Demostración
Trivial desde la descomposición de morfismos en una categoría arbitraria.
Además, se comprueba que $f^{-1}$ preserva ideales y que $f_\ast$ preserva ideales en su 
imagen.

*** Ideales primos y maximales
**** Definición
$P$ es ideal *primo* si no es el total y $xy\in P \Rightarrow x \in P \vee y \in P$.
$P$ es ideal *maximal* si $M \neq R$ y es maximal en el retículo de ideales sin $R$.

**** Caracterización de ideales primos y maximales
En relación a su cociente en el anillo:

 - $P$ es primo ssi $R/P$ es un dominio de integridad no trivial.
 - $M$ es maximal ssi $M/P$ es un cuerpo no trivial.

***** Demostración trivial
**** Preservación de ideales primos y maximales
Dado un homomorfismo $f$,

 1. Si $\Pi$ es ideal primo, entonces $f^{-1}(\Pi)$ es ideal primo.
 2. La imagen y preimagen de ideales preserva primos y maximales 
    entre el núcleo y sobre la imagen del anillo.

Como consecuencia los ideales primos (resp. maximales) de un
anillo $R/\alpha$, son los ideales de la forma $\Pi/\alpha$ con $\alpha\subset\Pi$ primo (resp.
maximal).

***** Demostración
*1* es trivial. *2* tenemos que demostrarlo en cuatro pasos:

 - Si $M$ es maximal en $\im f$, entonces $f^{-1}(M)$ es maximal entre los 
   ideales que contienen a $\ker f$.
 - Si $M$ es maximal, entonces $f(M)$ es maximal en $\im f$.
 - Si $\Pi$ es primo en $\im f$, entonces $f^{-1}(\Pi)$ es primo, ya demostrado.
 - Si $\ker f \subset \Pi$ es primo, entonces $f(\Pi)$ es primo en $\im f$.

*** Teorema de Krull
**** Subconjunto multiplicativamente cerrado
Es un subconjunto $S$ con:
 - $1 \in S$
 - $x,y\in S \Rightarrow xy \in S$

**** Teorema de Krull
Sea $\alpha \cap S = \varnothing$, entonces existe un ideal $M \supset \alpha$ tal que $M \cap S = \varnothing$ y maximal respecto
a esta condición. Además, $M$ es un ideal primo.

***** Demostración
Dada una cadena de ideales cumpliendo la condición su unión también la cumple
y es cota de la cadena. Aplicando lema de Zorn obtenemos un maximal.

Para ver que es primo tengamos $xy\in M$, los dos no pueden estar en $S$. Si $x \notin S$;
entonces $xz \notin S$, ya que se tiene $xzy \in M$; tomamos $M + (x) \supset M$, y contravendríamos
la maximalidad de $M$.

**** Corolario al teorema de Krull
Tomando $S=\{1\}$ tenemos que; dado un ideal, existe un ideal maximal que lo contiene.
En particular, todo elemento no unidad está contenido en un ideal maximal,
tomando $S = (x)$.

*** Anillos locales
**** Inclusión en ideales primos
Sean ideales $\alpha_1,\dots,\alpha_n$ y $\pi$ un ideal primo. Si $\bigcap \alpha_i \subset \pi$, entonces

$\exists i: \alpha_i \subset \pi$.

***** Demostración
Supongamos que $\forall i: \alpha_i \nsubseteq \Pi$; entonces tenemos $x_i \in \alpha_i - \Pi$ tales que ninguno
está en $\Pi$, pero su producto debe estar, contraviniendo que sea primo.

**** Inclusión de ideales primos
Sean ideales primos $\pi_1,\dots,\pi_n$, (salvo quizá 2) y $\alpha$ un ideal. Si $\alpha \subset \bigcup \pi_i$, entonces,

$\exists i: \alpha \subset \pi_i$.

***** Demostración
En el caso $n=2$, supongamos $\alpha \subset \pi_1 \cup \pi_2$; y tomemos $x \in \alpha, x\in\pi_1$, $y\in\alpha, y\in\pi_2$.
Entonces tendría $x+y \in \alpha$, pero no estaría en $\pi_1 \cup \pi_2$. Nótese que no usamos
todavía que sean primos.

En el caso inductivo, aplico la hipótesis de inducción para obtener, para cada $k$:

\[\exists x_k \in \alpha : x_k \notin \bigcup_{i\neq k} \pi_i\]

Luego debe tenerse $x_k \in \alpha_k$; sea ahora $x = x_1x_2\dots x_{n-1}+x_n \in \alpha$; 
luego $\exists r: x\in\alpha_r$. Si $r=n$, tendríamos $x_1x_2\dots x_{n-1} \in \alpha_r$, y por primalidad debería
estar alguno; si no, tendríamos $x_n \in \alpha_r$.

**** Definición de anillo local
Un *anillo local* es aquel con un único ideal maximal. A $R/M$ se le llama 
*cuerpo residual*.

**** Caracterización de anillos locales
Un anillo $R$ es local ssi $\{x\in R \mid x \text{ no unidad}\}$ es un ideal.

Sea $M$ ideal propio:

 - $R$ es local con maximal $M$ ssi $R-M \subset {\cal U}(R)$.
 - Si $M$ es maximal y $\{1+x \mid x\in M\}\subset {\cal U}(R)$ entonces es $R$ local y $M$ su maximal.

***** Demostración
1. Una no unidad debe estar contenida en el único maximal que hay, que no contiene
   a las unidades y además es ideal.
   Por otro lado, por Krull, el ideal de las no unidades debería estar contenido
   en un ideal maximal que tampoco tuviera unidades, luego debe ser él mismo.
2. Por la caracterización anterior tenemos una implicación. Sea $R-M \subset {\cal U}(R)$,
   si tenemos $M \not\subset \beta$, entonces tendríamos un $x \in \beta,x \notin M$, luego $x \in {\cal U}(R)$
   y entonces $\beta = R$. Si tenemos otro $M'$ maximal, entonces $\exists x\in M': x \notin M$,
   pero eso me vuelve a dar $M' = R$. Luego $M$ es el único ideal y maximal.
3. Por la caracterización anterior tenemos una implicación. Sea $x \notin R-M$,
   tenemos por maximalidad: $rx+m = 1$, luego $rx = 1-m \in {\cal U}(R)$.
   En conclusión, $R-M \subset {\cal U}(R)$ y es local.

*** Radicales
**** Nilradical
Sus elementos se llaman *nilpotentes*:

\[\operatorname{Nil}(R) = \{x \in R \mid \exists n: x^n = 0\}\]

El nilradical es un ideal.

**** Anillo reducido
Un anillo es *reducido* si $\operatorname{Nil}(R) = 0$. Los dominios de integridad son reducidos. 
Además, podemos reducir un anillo dividiendo por su nilradical $R/\operatorname{Nil}(R)$.

**** Espectro
El *espectro* de un anillo R es la intersección de sus ideales primos:

\[\spec(R) = \{\pi\in R \mid \pi \text{ es un ideal primo}\}\]

**** Caracterización del nilradical
El nilradical de $R$ es la intersección de los ideales del espectro:

\[ \operatorname{Nil}(R) = \bigcap_{\pi \in \operatorname{Spec}(R)} \pi\]

***** Demostración
Sea $x^n = 0$, entonces $x^n \in \pi, \forall \pi \in \operatorname{Spec}(R)$; y, por primalidad, $x \in \pi$.
Sea $x \notin \operatorname{Nil}(R)$, entonces $S = \{1,x,x^2,\dots\}$ es multiplicativamente cerrado con
$S \cap \operatorname{Nil}(R) = \varnothing$. Por Krull, tenemos un $\pi$ tal que $\pi \cap S = \varnothing$.

**** Radical de un ideal
Se define como:

\[\sqrt{\alpha} = \{x \in R \mid \exists n \geq 1 : x^n \in \alpha\}\]

Tenemos que $\operatorname{Nil}(R/\alpha) = \sqrt{\alpha}/\alpha$. Cuando $\alpha = \sqrt{\alpha}$ decimos que es *ideal radical*.
Se caracteriza como:

\[\sqrt{\alpha} = \bigcap_{\alpha \subset \pi \in Spec(R)} \pi\]

***** Demostración
Tenemos claramente $\pi^{-1}(\operatorname{Nil}(R/\alpha)) = \sqrt{\alpha}$, pero entonces:

\[\sqrt{\alpha} = 
\pi^{-1}(\operatorname{Nil}(R/\alpha))=
\pi^{-1} \left( 
\bigcap_{\alpha \subset \pi \in \operatorname{Spec}(R/\alpha)} \pi/\alpha 
\right) = \bigcap_{\alpha \subset \pi \in \operatorname{Spec}(R)} \pi\]

**** Radical de Jacobson
Se define el *radical de Jacobson* como:

\[{\cal J}(R) = \bigcap_{{\cal M} \text{ maximal}} {\cal M}\]

Tenemos que $x \in {\cal J}(R)$ ssi $1-xy \in U(R)$ para cualquier $y$.

***** Demostración
Un elemento $1-xy$ para $x \in {\cal J}(R)$ no puede estar en ningún ideal maximal, porque
estaría entonces el $1$. Por corolario de Krull, debe ser unidad.

Supongamos $x \notin M$, entonces $(x) + M = R$, y por tanto $1-xy \in M$. Pero un maximal
no puede contener una unidad.

*** Ideales residuales y anulador
**** Definición
Definimos el *ideal residual* (a veces llamado *cociente*) de dos ideales como:

\[ (\alpha : \beta) = \{x\in R\mid x\beta \subset\alpha\}\]

**** Anulador
\[\operatorname{Ann}(\beta) = \{x \in R \mid x\beta = 0\} = (0 : \beta)\]

*** Ideales contraídos y extendidos
**** Ideal extendido
Dado un homomorfismo $f : R \longrightarrow S$ llamamos *ideal extendido* de $\alpha$ al ideal:

\[ \alpha^e = (f(\alpha)) = \left\{ \sum s_i f(x_i) \mid s_i \in S, x_i \in \alpha\right\}\]

**** Ideal contraído
Dado un homomorfismo $f$ llamamos *ideal contraído* de $\beta$ al ideal:

\[ \beta^c = f(\beta) \]

**** Biyección entre ideales
Para $f : S \longrightarrow R$ homomorfismo y $\alpha,\beta$ ideales:

  1. $\alpha \subset \alpha^e^c$, y $\beta \supset \beta^c^e$.
  2. $\alpha^e = \alpha^e^c^e$, y $\beta^c = \beta^c^e^c$
  3. Hay una biyección con $(-)^e,(-)^c$ entre ideales contraídos y extendidos, que 
     además pueden escribirse como $\{\alpha \subset R \mid \alpha = \alpha^e^c\}$
     y $\{\beta \subset S \mid \beta = \beta^c^e\}$.

***** Demostración
Pueden escribirse así porque si tengo $\beta = \alpha^e$, entonces $\beta^c^e = \alpha^e^c^e = \alpha^e = \beta$. La biyección
es trivial desde la definición y las proposiciones anteriores.

*** Producto directo de anillos
**** Definición
Para $R_1,R_2,\dots,R_n$ tomamos como su producto directo a:

\[R_1\times \dots \times R_n = \prod_{i=1}^n R_i\]

con las operaciones definidas componente a componente.

**** Proyecciones e inclusiones
Las *proyecciones*, $p_k$, a cualquier factor son homomorfismos.
Las *inclusiones*, $u_k$, *no* son homomorfismos, ya que no respetan
el uno del anillo, que en este caso es $(1,\dots,1)$. Cumplen además:

 - $p_k \circ u_k = \delta_{kj}id$
 - $\sum u_i \circ p_i = id$
 - $\ker(p_j) = \sum \im(u_j)$

**** Propiedad universal
El producto con las proyecciones es el *producto categórico* de la
categoría de los anillos:

\[ \begin{tikzcd}
& & S \dar[dashed]{\exists!} \arrow[bend right]{ddll} \arrow[bend right]{ddl} \arrow[bend left]{ddr} \arrow[bend left]{ddrr} & &\\
& & \prod R \arrow{dll}[swap]{\pi_1} \arrow{dl}{\pi_2} \arrow{dr}[swap]{\pi_3} \arrow{drr} & & \\
R_1 & R_2 &  & R_3 & \dots
\end{tikzcd} \]

**** Teorema Chino del Resto
En la situación la propiedad universal sobre el cociente por unos
ideales $\alpha_1,\dots,\alpha_n$:

\[ \begin{tikzcd}
\prod_{i=1}^n R/\alpha_i \rar{\pi_i} &  R/\alpha_i \\
R \uar{\exists! f} \urar{p_i}
\end{tikzcd} \]

Tenemos que:

 1. Si los $\alpha_i$ son primos entre sí, $\prod \alpha_i = \bigcap \alpha_i$.
 2. La $f$ es sobreyectiva ssi los $\alpha_i$ son primos entre sí.
 3. La $f$ es inyectiva ssi $\bigcap \alpha_i = 0$. De hecho, $\ker(f) = \bigcap \alpha_i$.

***** Demostración
1. El caso $n=2$ es conocido. En el caso $n>2$, aplicamos la hipótesis
   de inducción a $\alpha_1\alpha_2\dots\alpha_{n-1}$ y a $\alpha_n$, que se demuestran primos relativos.
2. Doble implicación:
   - Si $f$ es sobreyectiva, existe $f(x) = (1,0,0,\dots)$, que me da $x \in \alpha_i$
     para cualquier $i$, y además, $x-1 \in \alpha_1$; luego $1 \in \alpha_i+\alpha_1$.
   - Si son primos relativos, existen $x_i + y_i = 1$ con $x_i \in \alpha_1$, $y_i \in \alpha_i$.
     $y = y_2y_3\dots y_n = 1 + x$, con $x \in \alpha_1$; luego $y \equiv_{\alpha_1} 1$ pero $y \equiv_{\alpha_i} 0$; luego
     puedo crear una base del anillo.
3. Trivial.

** 2. Bases de Gröbner
*** R-álgebras
**** R-álgebras
Una *R-álgebra*, $S$ es un anillo con estructura de R-módulo, tal que:

\[\forall r\in R, x,y\in S:\; (rx)y = r(xy) = x(ry)\]

**** Homomorfismo de estructura
Equivalentemente, una R-álgebra es un anillo $S$ junto a un *homomorfismo 
de estructura* $\lambda : R \longrightarrow S$. 

***** Equivalencia
Nótese que puedo pasar de una a otra definición tomando $\lambda(r) = r1$.

**** Homomorfismos de R-álgebras
Un homormorfismo de R-álgebras $f : S_1 \longrightarrow S_2$ es un homomorfismo de anillos
que sea también homomorfismo de R-módulos.

\[ f(rs) = rf(s) \]

**** Anillo de polinomios
Definimos el anillo de polinomios en varias variables recursivamente:

\[ R[X_1,X_2,\dots,X_n] = R[X_1,X_2,\dots,X_{n-1}][X_n] \]

Y forma una R-álgebra con la inclusión desde $R$.

**** Propiedad universal del anillo de polinomios
Sea $S$ anillo y $f : R \longrightarrow S$ homomorfismo de anillos. Sean $s_1,\dots,s_n \in S$
elementos arbitrarios; entonces existe un único homomorfismo de R-álgebras
$f_{s_1,\dots,s_n} : R[X_1,\dots,X_n] \longrightarrow S$ tal que:

\[ \begin{tikzcd}
R \rar[hook]{i} \drar[swap]{f} & R[X_1,X_2,\dots,X_n] \dar[dashed]{f_{s_1,\dots,s_n}} \\
  & S
\end{tikzcd} \]

**** Representación distributiva de un polinomio
Llamamos representación distributiva a la siguiente:

\[ p = \sum a_{(\alpha_1,\dots,\alpha_n)} 
X_1^{\alpha_1} X_2^{\alpha_2} \dots X_n^{\alpha_n} \]

Si escribimos los monomios como $X^{\alpha}$ para cada $\alpha \in \mathbb{N}^n$, nos
queda:

\[p = \sum_{\alpha \in \mathbb{N}^n} a_\alpha X^\alpha\]

*** Órdenes monomiales
**** Órdenes compatibles
Un orden $\leq$ en $\mathbb{N}^n$ es compatible si:

\[\alpha \leq \beta \Longrightarrow \alpha + \gamma \geq \beta + \gamma\]

**** Órdenes monótonos
Un orden $\leq$ es monótono si:

\[ 0 \leq \alpha \]

**** Órdenes monomiales
Un orden monomial es compatible, monótono y total.

**** Orden lexicográfico
Definimos $\alpha \geq_{lex} \beta$ cuando para el primer $\alpha_i \neq \beta_i$, se tiene
$\alpha_i \geq \beta_i$.

**** Orden lexicográfico graduado
Definimos $\alpha \geq_{grlex} \beta$ cuando $\sum \alpha > \sum \beta$ ó se da la igualdad
y se tiene $\alpha \geq_{lex} \beta$.

**** Orden lexicográfico graduado inverso
Definimos $\alpha \geq_{invgrlex} \beta$ cuando $\sum \alpha > \sum \beta$ ó se da la igualdad
y para el primer $\alpha_i \neq \beta_i$ a la derecha, se tiene $\alpha_i < \beta_i$.

**** Relación de equivalencia en preódenes
Dado un preorden $\sqsubseteq$, se define la relación de equivalencia $x \equiv y$,
que se tiene cuando $x \sqsubseteq y \wedge y \sqsubseteq x$.

**** Producto lexicográfico de preórdenes
Definimos el producto lexicográfico de dos preórdenes $\sqsubseteq_1,\sqsubseteq_2$
como:

\[x \sqsubseteq_{12} y = \left\{
\begin{array}{l} 
x \sqsubseteq_1 y \wedge y \not\sqsubseteq_1 x \\ 
\text{  ó} \\
x \equiv_1 y \wedge x \sqsubseteq_2 y
\end{array}\right.\]

**** Propiedades del producto
El producto de preórdenes cumple:

 1. $\sqsubseteq_{12}$ es un preorden
 2. $\sqsubseteq_2$ orden $\Rightarrow$ $\sqsubseteq_{12}$ orden
 3. $\sqsubseteq_1,\sqsubseteq_2$ totales $\Rightarrow$ $\sqsubseteq_{12}$ total
 4. $\sqsubseteq_1,\sqsubseteq_2$ compatible $\Rightarrow$ $\sqsubseteq_{12}$ compatible
 5. $\sqsubseteq_1,\sqsubseteq_2$ monótono $\Rightarrow$ $\sqsubseteq_{12}$ monótono

***** TODO Demostración

**** Los órdenes son monomiales
Los órdenes $\leq_{lex},\leq_{grlex},\leq_{invgrlex}$ son monomiales.

***** TODO Demostración

**** Lema de Dickson
Sea $S \subseteq \mathbb{N}^n$ no vacío, existe $G \subseteq S$ finito tal que:

\[\forall x\in S: \exists y \in G: \quad x = y + \alpha\]

***** TODO Demostración
**** Orden monomial es buen orden
Todo orden monomial en $\mathbb{N}^n$ es buen orden.

**** Monoideales
Un subconjunto $E \subseteq \mathbb{N}^n$ es monoideal cuando $E = E + \mathbb{N}^n$.

**** Sistemas de generadores
Si $E$ es monoideal, existe $G \subset E$ finito con $E = G + \mathbb{N}^n$.
Llamamos a $G$ sistema de generadores de $E$.

***** TODO Demostración

**** Sistema de generadores minimal
Un sistema de generadores es minimal si ningún subconjunto
suyo es sistema de generadores.

**** Unicidad del sistema de generadores minimal
El sistema de generadores minimal de un $E$ es único.

***** TODO Demostración

*** División de polinomios
**** Diagrama de Newton
El conjunto de exponentes para un polinomio $p = \sum a_\alpha x^\alpha$:

\[N(p) = \{\alpha\in\mathbb{N}^n \mid a_\alpha \neq 0\}\]

**** Exponente
El exponente de un polinomio, fijado un orden monomial, es el máximo
exponente de su diagrama

\[Exp(p) = \max \{\alpha\in\mathbb{N}^n \mid a_\alpha \neq 0\} \]

**** Grado total
Definimos el grado total, fijado un orden monomial, como el máximo
valor absoluto de los exponentes:

\[Grtot(p) = \max \{|\alpha| \mid a_\alpha \neq 0\} \]

**** Término líder
Llamamos término líder al que aporta el exponente, $a_{Exp(p)}X^{Exp(p)}$.
Su coeficiente líder es $a_{Exp(p)}$ y su monomio líder, $X^{Exp(p)}$.

**** Propiedades del exponente
Dados $F,G \in K[X_1,\dots,X_n]$ no nulos: 

 1. $Exp(FG) = Exp(F) + Exp(G)$.
 2. Si $F+G \neq 0$ entonces $Exp(F+G) \leq \max\{Exp(F),Exp(G)\}$.
 3. Si $Exp(F) < Exp(G)$; entonces $Exp(F+G) = Exp(G)$.
 
***** TODO Demostración

**** Partición de generadores
Dada una lista de elementos $a_1,\dots,a_k$, tenemos una partición que
definimos inductivamente:

\[\Delta^1 = a_1 + \mathbb{N}^n\]

\[\Delta^i = (a_i + \mathbb{N}^n) \setminus \bigcup_{j < i} \Delta^j \]

\[ \overline{\Delta} = \mathbb{N}^n \setminus \bigcup_{j \leq k} \Delta^j\]

**** Teorema de la división
Dado un orden monomial y una lista de polinomios $G_i$; consideramos la
partición $\Delta_1,\dots,\overline{\Delta}$ dada por los exponentes $Exp(G_i)$. Tenemos que para
cada $F \in K[X_1,\dots,X_n]$, existen $Q_1,\dots,Q_t,R$ únicos tales:

 1. $F = Q_1G_1 + \dots + Q_tG_t + R$
 2. $R = 0$ ó $N(R) \subseteq \overline\Delta$
 3. $Exp(G_i) + N(Q_i) \subseteq \Delta^i$

***** TODO Demostración

*** Ideales monomiales
**** Ideales monomiales
Un ideal es monomial si es de la forma:

\[ I = (X^\alpha \mid \alpha\in A)\]

Para algún $A \subseteq \mathbb{N}^n$.

**** Pertenencia a ideal monomial
Sea $I = (X^\alpha \mid \alpha\in A)$. $X^\beta \in I$ ssi existen $F$ y $\alpha$ tales que: $X^\beta = FX^\alpha$.

***** TODO Demostración

**** Monomios en ideal monomial
Sea $I = (X^\alpha \mid \alpha\in A)$ y $F \in K[X_1,\dots,X_n]$, equivalen entonces:

 1. $F \in I$.
 2. Todo monomio de $F$ está en $I$.
 3. $F$ es combinación lineal de monomios de $I$.

***** TODO Demostración
 
**** Exponente de un ideal
Dado un ideal $I$ no nulo, definimos su exponente como el subconjunto:

\[Exp(I) = \{Exp(F) \mid 0\neq F \in I\} \subseteq \mathbb{N}^n\]

**** El exponente es monoideal
$Exp(I)$ es un monoideal de $\mathbb{N}^n$.

***** TODO Demostración

**** Lema de Dickson para ideales monomiales
Todo ideal monomial tiene un sistema de generadores finito y formado
por monomios.

***** TODO Demostración

**** Unicidad del sistema de generadores minimal
Sea $I$ monomial no nulo. Existe un único sistema de generadores minimal
formado por monomios, es decir, ningún subconjunto suyo es generador.

***** TODO Demostración

**** TODO Otros sistemas de generadores
**** Bases de Gröbner
Sea $I \subseteq K[X_1,\dots,X_n]$ un ideal no nulo. Una base de Gröbner es un conjunto
$\mathbb{G} = \{G_1,\dots,G_n\} \subseteq I$ cumpliendo $Exp(I) = \{Exp(G_1),\dots,Exp(G_n)\} + \mathbb{N}^n$.

**** Resto en bases de Gröbner
Sea $I$ ideal con bases de Gröbner $\mathbb{G},\mathbb{G}'$. Para $F \in K[X_1,\dots,X_n]$:

\[R(F;\mathbb{G}) = R(F;\mathbb{G}')\]

***** TODO Demostración

**** Caracterización de bases de Gröbner
Sea $I$ ideal no nulo, equivalen:

 1. $\mathbb{G}$ es base de Gröbner de $I$.
 2. $R(F,\mathbb{G}) = 0$, para todo $F \in I$.

***** TODO Demostración

**** Semizigia
Sean $F,G \in K[X_1,\dots,X_n]$ no nulos. Definimos el S-polinomial o semizigia
como el siguiente polinomio:

\[{\cal S}(F,G) 
= \frac{1}{cl(F)}X^{\gamma-\alpha}F - \frac{1}{cl(G)}X^{\gamma-\beta}G\]

donde $\gamma_i = \max\{\alpha_i,\beta_i\}$, para $\gamma = (\gamma_1,\dots,\gamma_n)$.

**** Teorema de Buchberger
Sea $I$ ideal no nulo de $K[X_1,\dots,X_n]$ y $\mathbb{G} = \{G_1,\dots,G_n\}$ un sistema de
generadores. Equivalen:

 1. $\mathbb{G}$ es base de Gröbner.
 2. Para cualquier ordenación de $\mathbb{G}$, se tiene $R({\cal S}(G_i,G_j); \mathbb{G}) = 0$
    para cualesquiera $i \neq j$.

***** TODO Demostración

** 3. Tema 3
* Análisis Funcional
** 1. Espacios normados
*** Espacios normados
**** Norma y seminorma
*Norma*. Función $\|\_\| : X \longrightarrow \mathbb{R}$ verficando:

- $\|x\| = 0 \Leftrightarrow x = 0$
- $\|ax\| = |a| \|x\|$
- $\|x+y\| \leq \|x\|+\|y\|$

Cuando $\|x\| = 0$ no implica $x = 0$, se llama *seminorma*.

**** Distancia de la norma
Desde la norma se puede definir una *distancia* asociada $d(x,y) = \|x-y\|$, 
que hace a $X$ un *espacio métrico*. La distancia cumple

- $d(x+a,y+a) = d(x,y)$
- $d(\lambda a) = |\lambda| d(a)$

**** Topología de la norma
La distancia hace a un espacio normado un *espacio topológico*
con abiertos:

\[\tau = \{G \subset X \;|\; \forall a \in G: \exists r > : B(a,r) \subset G\}\]

Dos normas que generan el mismo espacio topológico son 
*equivalentes*.

**** Espacios vectoriales topológicos
Cualquier espacio vectorial sobre $\mathbb{R}$ o $\mathbb{C}$ admite una norma.
Dada una base \(\{e_i\}\) del espacio, podemos escribir $x = \sum \alpha_i e_i$ 
y definirla como:

\[ \|x\| = \sum |\alpha_i|\]

**** Continuidad de la norma, suma y producto
La norma es continua en su espacio por ser lipschitziana:

\[ |\|x\| -  \|y\|| \leq \|x-y\| \]

***** Continuidad de suma y producto
La suma y el producto por escalares son continuos, usando 
que la convergencia en el producto equivale a la 
convergencia por coordenadas.

***** Continuidad de homotecias y translaciones
Como corolario, lo son las *homotecias* y *translaciones*, 
todas las bolas cerradas son homeomorfas a la bola unidad.

**** Operaciones sobre conjuntos
Para $X$ espacio normado:

  1. $A$ abierto $\Rightarrow$ $A+B$ abierto.
  2. $A$ cerrrado, $B$ compacto $\Rightarrow$ $A+B$ cerrado.
  3. $A,B$ compactos $\Rightarrow$ $A+B$ compacto.
  4. $M$ subespacio $\Rightarrow$ $\overline{M}$ subespacio.

***** Demostración de 1
Es la unión de abiertos, $A + B = \bigcup_{b \in B} (b + A)$.

***** Demostración de 2
Sea $x \in \overline{A+B}$, $\exists \{a_n,b_n\} : \{a_n,b_n\} \longrightarrow x$, por compacidad
tenemos $\{b_{\sigma_n}\} \longrightarrow b \in B$ y por tanto $\{a_{\sigma_n}\} \longrightarrow x-b \in A$.

***** Demostración de 3
Se tiene $A\times B$ compacto, y la suma es continua, luego
$A+B$ es compacto.

***** Demostración de 4
Usando la continuidad de la suma y del producto por 
escalares:

\[(+)(\overline{M}\times\overline{M}) 
= (+)\overline{(M\times M)}
\subset \overline{(+)(M \times M)}\]
\[(*)(\mathbb{K}\times\overline{M})
= (*)\overline{(\mathbb{K}\times M)}
\subset \overline{(*)(\mathbb{K} \times M)}\]
      
***** Contraejemplo de suma de cerrados
La suma de dos cerrados puede no ser cerrado:

\[\left\{n + \frac{1}{n} \mid n \in \mathbb{N}\right\} + 
\left\{-n \mid n \in \mathbb{N}\right\} = 
\left\{\frac{1}{n} \mid n \in \mathbb{N}\right\}\]

**** Conexión de espacios normados
Todo espacio normado es *arcoconexo* y *localmente arcoconexo*; 
por tanto *conexo*. De hecho, la bola unidad es *convexa*.

**** Espacios de Banach
Un *espacio de Banach* es un espacio normado completo.

*** Equivalencia de normas y desigualdades
**** Equivalencia proporcional
Dos normas $\|.\|$ y $\|.\|_\ast$ son equivalentes cuando:

\[\exists m,M \in \mathbb{R^+}:\; m \|x\| \leq \|x\|_\ast \leq M \|x\|\]

Como se demuestra por mutua inclusión de bolas.

**** Desigualdad de Young
Para $a,b\in\mathbb{R}^+$ y $p>1$ con $\frac{1}{p}+\frac{1}{q} = 1$ se tiene:

\[ab \leq \frac{a^p}{p}+\frac{b^q}{q}\]

***** Demostración
Se demuestra aplicando desigualdad de Taylor al logaritmo con 
pesos $1/p$ y $1/q$.

\[\log(ab) = \frac{1}{p}\log(a^p) + \frac{1}{q}\log(b^q) \leq 
\log\left(\frac{a^p}{p} + \frac{b^q}{q}\right)\]

**** Desigualdad de Hölder
Para $a_1\dots a_nb_1\dots b_n \in \mathbb{R}^+_0$ con $\frac{1}{p} +\frac{1}{q} = 1$ se verifica:

\[\sum a_kb_k \leq \left(\sum a_k^p\right)^{1/p}\left(\sum b_k^q\right)^{1/q}\]

***** Demostración
Se demuestra aplicando Young a la división de ambos lados y
cuidando el caso $0$. Llamamos $\alpha = \left(\sum_k a^p_k\right)^{1/p}$, $\beta = \left(\sum_k \beta^q_k\right)^{1/q}$:

\[\frac{a_kb_k}{\alpha\beta}
\leq \frac{a_k^p}{p\alpha^p} + \frac{b_k^q}{q\beta^q}\]

Sumando cada desigualdad tenemos:

\[\frac{1}{\alpha\beta} \sum a_kb_k \leq 
\frac{1}{p\alpha^p}\sum a_k^p +
\frac{1}{q\beta^q}\sum b_k^q = 1\]

**** Desigualdad de Minkowski
Para $a_1\dots a_nb_1\dots b_n \in \mathbb{R}^+_0$ con $\frac{1}{p} +\frac{1}{q} = 1$ se verifica:

\[\left(\sum_{k=1}^n (a_k+b_k)^p \right)^{1/p} \leq 
\left(\sum_{k=1}^n a_k^p \right)^{1/p} + 
\left(\sum_{k=1}^n b_k^p \right)^{1/p} \]

Dicho de otra forma:

\[\|a+b\|_p \leq \|a\|_p + \|b\|_p\]

***** Demostración
Aplicando Hölder.

*** Ejemplos de espacios normados
**** Espacios de dimensión finita
Solemos notar por ${l}_p^n = (\mathbb{K}^n,\|.\|_p)$ al espacio de Banach sobre $\mathbb{R}^n$ o $\mathbb{C}^n$ 
que da la norma:

\[\|x\|_p = \left(\sum |x_{(k)}|^p \right)^{1/p}\]

Nótese el caso especial $l^n_\infty$ que da la norma del máximo.

***** Normas
Todas estas normas lo son gracias a la [[*Desigualdad de Minkowski][desigualdad de 
Minkowski]].

***** Equivalencia
Todas las normas son equivalentes y generan el mismo espacio
de Banach:

\[ \|x\|_\infty \leq \|x\|_p \leq \|x\|_1 \leq N \|x\|_\infty\]

**** Espacios de sucesiones
Las *sucesiones* tales que su p-suma es convergente,
con las normas $\|.\|_p$, dan los siguientes espacios de Banach:

\[\ell_p = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K} \,\middle|\,
\sum_{n=1}^\infty |x(n)|^p < \infty \right\}\]

siendo un caso particular el de la norma del supremo
sobre *sucesiones acotadas*:

\[{\cal \ell}_\infty = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ \left|\ x(n) \text{ acotada} \right\}\]

***** Forma un subespacio vectorial
Pasando la desigualdad de Minkowski al límite, tenemos:

\[\left(\sum^\infty_{k=1} (a_k+b_k)^p \right)^{1/p} \leq 
\left(\sum^\infty_{k=1} a_k^p \right)^{1/p} + 
\left(\sum^\infty_{k=1} b_k^p \right)^{1/p} \]

Por tanto, es un subespacio vectorial y se obtiene una norma
como:

\[\|x\|_p = \left(\sum_{n=1}^\infty |x(n)|^p \right)^{1/p}\]

***** Son espacios de Banach
Como tenemos $|x_n(k)-x_m(k)| \leq \|x_n-x_m\|$, cuando $\{x_n\}$ es 
Cauchy en $\ell_p$, también es Cauchy $\{x_n(k)\}$. Y por tanto, es
convergente por componentes $x(k) = \lim_{n\to\infty}x_n(k)$.

Usamos $\{x_n\}$ de Cauchy para tener:

\[\exists n_0 : \forall m,n\geq n_0 :
\|x_n-x_m\| < \varepsilon\]

Es decir,

\[\sum_{k=1}^N |x_n(k)-x_m(k)|^p \leq (\|x_n-x_m\|_p)^p < \varepsilon^p\]

Tomando $m \to \infty$, y luego tomando $N \to \infty$:

\[\sum_{k=1}^\infty |x_n(k)-x(k)|^p \leq \varepsilon^p\]

Así, tenemos que $x = x_n - (x_n-x) \in \ell_p$, y como $\|x_n-x\|_p \leq \varepsilon$,
tenemos $\{x_n\} \to x$.

**** Subespacios del espacio de sucesiones
El espacio de sucesiones cuenta con subespacios usando la misma 
norma:

***** Sucesiones convergentes
Es un subespacio de $\ell_\infty$ cerrado, y por tanto, de Banach.

\[c = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \{x(n)\} \text{ convergente } \right\}\]
 
***** Sucesiones nulas
Otro subespacio de $\ell_\infty$, también cerrado y por tanto, de Banach.

\[c_0 = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \{x(n)\} \longrightarrow 0 \right\}\]

***** Sucesiones casi-nulas o de soporte finito
Es un subespacio para todo $\ell_p$.

\[c_{00} = \left\{ x : \mathbb{N} \longrightarrow \mathbb{K}\ 
\left|\ \exists m: \forall n \geq m:\  x(n) = 0 \right\}\]

Este es un subespacio denso ya que toda sucesión es límite de
sucesiones de soporte finito. Pero no es el total, así que no
será completo.

**** Espacios de funciones continuas acotadas
Dado $T$ espacio topológico, tomamos el espacio de funciones
continuas y acotadas:

\[{\cal C}_b(T) = \left\{f : T \longrightarrow \mathbb{K} \mid
f \text{ continua, acotada}\right\}\]

Y lo dotamos de la *norma del supremo*:

\[ \|f\|_\infty = \sup\{ |f(t)| \mid t \in T\}\]

Que da la *convergencia uniforme*.

***** TODO Es espacio de Banach
***** Anuladas en infinito
Sea $L$ compacto y separado. Una función se *anula en 
el infinito* cuando:

\[\forall \varepsilon: 
\{t\in L \mid |f(t)|\geq\varepsilon\} 
\text{ es compacto}\]

Las funciones continuas que se anulan en el infinito 
forman ${\cal C}_0(L)$, subespacio de ${\cal C}_b(L)$. Es espacio cerrado
y por tanto de Banach.

***** Funciones de soporte compacto
El soporte de $f : L \longrightarrow \mathbb{K}$ para $L$ localmente compacto 
separado es:

\[sop(f) = 
\overline{\{t \in L \mid f(t) \neq 0\}} \subset
L \]

Las funciones con soporte compacto forman ${\cal C}_{00}(L)$, subespacio
vectorial de ${\cal C}_0(L)$.

***** Relación entre ambas
Tenemos que ${\cal C}_{00}(L)$ no es completo en general y que:

\[\overline{{\cal C}_{00}(L)} = {\cal C}_0(L)\]

**** Espacios de funciones derivables
Consideraremos el espacio de funciones sobre un intervalo que sean $d$
veces derivables con derivadas continuas, ${\cal C}^n([a,b],\mathbb{K}^d)$. Escritas:

\[f^{k)} = \left(f^{k)}_1,f^{k)}_2,\dots,f^{k)}_d\right)\]

Sobre él defimos una norma del supremo sobre cada derivada
$\|f^{k)}\| = max \{\|f^{k)}_i\|_\infty\}$. La norma del espacio es la
suma de la de cada una de las derivadas.

\[ \|f\|_\infty = \sum \|f^{k)}\|_\infty\]

***** Es espacio de Banach
Esto, por *Teorema de la convergencia uniforme* nos lleva a que una
sucesión de Cauchy converja de manera que respete la derivada. Este
será un espacio de Banach.

**** Espacios de funciones integrables
Consideramos el *espacio de funciones p-integrables* como:

\[ L_p(\Omega) =
\left\{ f \in L(\Omega) \mid \int_\Omega |f(t)|^p dt < \infty \right\}
\]

Normándolas con:

\[ \|f\|_p =
\left( \int_\Omega |f(t)|^p dt \right)^{1/p}\]

Debemos /identificar funciones que coinciden c.p.d./ para
deducir $\|f\|_p = 0 \Rightarrow f = 0$. Estos espacios son siempre completos.

***** Desigualdades integrales de Hölder y Minkowski
A partir de la desigualdad de Young llegamos
a la *desigualdad integral de Hölder* para funciones
tales que $|f|^p, |g|^p$ son Lebesgue-integrables:

\[\int_\Omega |f(t)g(t)| dt \leq
\left( \int_\Omega |f(t)|^p dt \right)^{1/p}
\left( \int_\Omega |g(t)|^q dt \right)^{1/q}\]

Y desde ella, la *desigualdad integral de Minkowski*:

\[\left( \int_\Omega |f(t) + g(t)|^p dt \right)^{1/p} \leq
\left( \int_\Omega |f(t)|^p dt \right)^{1/p} +
\left( \int_\Omega |g(t)|^p dt \right)^{1/p}\]

Esto nos da la naturaleza de norma.

***** Complitud en el caso real
Usando el Teorema de Riesz-Fisher.

**** Espacios de funciones esencialmente acotadas
Una función es *esencialmente acotada* cuando
es $f : [0,1] \longrightarrow \mathbb{K}$:

\[\exists M: |f| \leq M \text{ c.p.d.}\]

Al espacio de funciones esencialmente acotadas lo
llamamos ${\cal L}_\infty[0,1]$ y le damos una seminorma:

\[\phi_\infty(f) = \inf\{ M
 \mid |f|\leq M \text{ c.p.d.}\}\]

***** Naturaleza de seminorma
Se cumple por un lado que:

\[\phi_\infty(\alpha f) = |\alpha|\phi_\infty(f)\]

Y por otro lado que:

\[ \phi_\infty(f+g) \leq \phi_\infty(f) + \phi_\infty(g) \]

***** Estructura de espacio normado
Podemos convertirlo en espacio normado si tomamos
cociente sobre:

\[ N = \{ f \in {\cal L}_\infty[0,1] \mid
\phi_\infty(f)=0 \} \]

Esto es espacio de Banach con la norma $\phi_\infty$.

*** Categoría de espacios normados
**** Homomorfismos topológicos
Los morfismos de la categoría de espacios normados son los
*homomorfismos topológicos*, operadores lineales y continuos que
además son abiertos en su imagen. Hablamos igualmente de
*monomorfismos topológicos*, *epimorfismos topológicos* o de
*isomorfismos topológicos*.

**** Producto de espacios normados
Dados $X_1,\dots,X_n$ espacios normados, podemos definir normas 
sobre su producto cartesiano $\prod X_i$:

- *Norma del máximo*: $\|x\|_\infty = \max\{\|x_i\|\}$
- *Norma p*: $\|x\|_p = (\sum \|x_i\|^p)^{1/p}$

La convergencia es trivialmente coordenada a coordenada, y
las topologías asociadas son equivalentes a la topología
producto. El espacio es completo ssi lo son las componentes.

**** Cociente de un espacio normado
Sea $M$ subespacio vectorial cerrado de $X$. Podemos hacer a
$X/M$ normado con:

\[ \|x+M\| 
= \inf\left\{ \|x - m\| \mid m \in M \right\} 
= d(x,M)\]

Este espacio es de Banach ssi $X$ es de Banach. Y la topología
coincide con la topología cociente.

*** Operadores y funcionales lineales
**** Operadores lineales continuos
Para $T : X \longrightarrow Y$ lineal entre espacios normados, equivalen:

- $T$ lipschitziana
- $T$ uniformemente continua
- $T$ continua
- $T$ continua en $0$
- $\exists M: \|Tx\| \leq M\|x\|$
- $T$ preserva acotación, $A$ acotado da $TA$ acotado
- $TB_X$ acotado
- $TS_X$ acotado

Y lo llamamos *operador lineal continuo*.

***** TODO Demostración
La lipschitzianidad implica hasta la continuidad en $0$.

**** Norma de operadores
Se define la *norma de operadores* como:

\[\begin{aligned}
\|T\| =& \sup_{x \in B_X}\left\{\|Tx\|\right\} \\
=& \sup_{x \in S_X}\left\{\|Tx\|\right\} \\
=& \min\{k \mid \|Tx\| \leq k\|x\| \}
\end{aligned}\]

Y cumple que $\|Tx\| \leq \|T\|\|x\|$.

**** Cuatro teoremas sobre la norma de operadores
Dados $X,Y$ espacios normados:

1. $(L(X,Y),\|.\|)$ es espacio normado.
2. $\{T_n\}\longrightarrow T$ ssi $\{T_n\}\longrightarrow T$ uniformemente en $B_X$.
3. Si $Y$ es de Banach, $L(X,Y)$ es de Banach.
4. Para $X \overset{T}\longrightarrow Y \overset{S}\longrightarrow Z$, se tiene $\|S \circ T\| = \|S\|\|T\|$.

***** TODO Demostración

**** Espacio dual topológico
Sea $X$ normado, su *dual topológico* es el espacio
de funciones al cuerpo con la norma de operadores:

\[ X^\ast = 
L(X,\mathbb{K}) 
= \left\{ f : X \longrightarrow \mathbb{K} \mid
f \text{ lineal y continua} \right\}
\]

**** Extensión desde un subespacio denso
Sea $M$ subespacio denso en $X$, para cada $T \in L(M,Y)$ existe
un $S \in L(X,Y)$ tal que $S|_M = T$ y $\|S\| = \|T\|$

***** TODO Demostración

**** Caracterización de operadores abiertos
Sean $X,Y$ espacios normados, $T : X\longrightarrow Y$ lineal. Equivalen:

 - $T$ es abierta
 - $T(B_X)$ es entorno de $0$

***** TODO Demostración

**** Funciones lineales abiertas
Sea $T : X \longrightarrow Y$ lineal. Equivalen:

- $T$ abierta sobre $TX$
- $\exists r>0:\quad TX \cap rB_Y \subset TB_X$
- $\exists\alpha>0: \forall y\in TX: \exists x\in X: 
  \quad \|x\| \leq \alpha\|y\|$, cumpliendo $Tx = y$.

***** Demostración
Aplicando la caracterización anterior a $TX$ tenemos que
equivalen el primer y segundo apartado.

Por el apartado 2, tengo que dado un $y$, $y\frac{\delta}{\|y\|} \in \delta B_Y$.
Por tanto, $\exists x: Tx = y \frac{\delta}{\|y\|}$. Tomamos $x' = x \frac{\|y\|}{\delta}$, y tenemos
que $T(x') = y$, y además que $\|x'\| = \|x\|\frac{\|y\|}{\delta} \leq \frac{\|y\|}{\delta}$.


**** Descomposición canónica: proyección al cociente
Sea $X$ un espacio normado, $M$ subespacio cerrado y $\pi$
la proyección. Entonces $\pi \in L(X,X/M)$ es sobreyectiva y
abierta con $\|\pi\| = 1$.

***** TODO Demostración
Para ver que tiene norma 1, tomamos $x_0 \notin M$.

\[\forall m \in M: \|x+M\| = \|Q(x+m)\| \leq \|Q\|\|x+m\|\]

En particular,

\[ \|x+M\| \leq \|Q\| \inf\{\|x+m\|\} = \|Q\|\|x+M\| \]

**** Descomposición canónica: isomorfismo
Sea $T: X\longrightarrow Y$ lineal con $\ker(T)$ cerrado. Se define:

\[ \hat{T} : {X}/{\ker{T}} \longrightarrow Y\]

Cumpliendo $T = \hat{T}\circ\pi$. Y se tiene:

- $\hat{T}$ continua ssi $T$ continua. En cuyo caso $\|T\| = \|\hat{T}\|$.
- $T$ abierta en $TX$ ssi $\hat{T}$ abierta en $TX$.

***** Demostración

*** Teorema de Tychonoff
**** Lema a Tychonoff
Toda aplicación lineal desde $\ell_2^n$ es continua.

**** Teorema de Tychonoff
Sea $X$ espacio normado. Toda
biyección lineal de $\ell^n_2$ sobre $X$ es isomorfismo topológico.

***** Demostración
Por el lema sabemos que es continua.
La esfera de $\ell^n_2$ es compacta; y podemos aplicar la 
caracterización de aplicaciones abiertas anterior.

**** Corolarios al teorema
Se cumple que:

1. $T:X\longrightarrow Y$ lineal con $dim(X) < \infty$ nos da $T$ lipschiztiana.
2. Dos espacios de dimensión finita son isomorfos ssi 
   tienen igual dimensión.
3. En un espacio de dimensión finita, todas las normas son
   equivalentes.
4. Todo espacio de dimensión finita es Banach.
5. Todo subespacio de dimensión finita de espacio normado es
   cerrado
6. Un subconjunto de un espacio normado de dimensión finita
   es compacto ssi es cerrado y acotado.

**** Compacidad relativa y precompacidad
Llamamos $A$ *relativamente compacto* cuando $\overline{A}$ es compacto.
Llamamos $A$ *precompacto* cuando, dado un $\varepsilon$, existen $x_1,\dots,x_n$:

\[ A \subset \bigcup_{k=1}^n B(x_n,\varepsilon) \]

***** Cadena de implicaciones
Compacidad implica compacidad relativa, que a su vez implica
precompacidad, que implica acotación.

**** Corolario de Tychonoff de compacidad
En un espacio normado de dimensión finita, un subconjuto
es relativamente compacto ssi es acotado y ssi es 
precompacto.

**** Corolario de caracterización de continuas
Sea $T : X \longrightarrow Y$ lineal con $TX$ de dimensión finita. 
Equivalen:

1. $T$ es continua.
2. $\ker T$ cerrado en $X$.

***** TODO Demostración

**** Corolario de caracterización de abiertas
Sea $T : X \longrightarrow Y$ lineal con $X$ de dimensión finita.
Equivalen:

1. $T$ es abierta.
2. $T$ es sobreyectiva.

**** Corolario de caracterización de la dimensión finita.
Equivalen:

 1. Todo cerrado y acotado es compacto.
 2. Bola unidad compacta.

*** Teorema de Riesz
**** Lema al teorema de Riesz
Sea $X$ espacio normado con $M$ subespacio propio cerrado. Si
$\varepsilon \in (0,1)$, existe $x \in \mathbb{S}_X$ tal que:

\[ \|x+M\| = d(x,M) > 1-\varepsilon \]

***** Demostración
Sea $x_0 \notin X-M$. Por ser $M$ cerrado $d(x_0,M)>0$. Por ser
el ínfimo, tengo que existe $m_0$ tal que:

\[\frac{1}{1-\varepsilon}\| x_0 - M\| > \| x_0-m_0 \| \]

Por tanto, tomando $x = \frac{x_0-m_0}{\|x_0-m_0\|} \in \mathbb{S}_X$, tenemos:

\[ \| x + M \| 
=  \left\| \frac{x_0-m_0}{\|x_0-m_0\|} + M \right\|
= \frac{\|x_0-M\|}{\|x_0-m_0\|} > 0\]

**** Teorema de Riesz
Son equivalentes:

1. $X$ de dimensión finita.
2. $X$ es localmente compacto.
3. $B_X$ es compacta.
4. $B_X$ es [[*Compacidad relativa y precompacidad][precompacta]].

***** Demostración
1. Cuando la dimensión es finita, $X \cong \mathbb{K}^n$, que es localmente
   compacto.
2. Sea $U$ entorno compacto de $0$, $\exists r>0: \overline{B}(0,r) \subset U$. Por ser un cerrado
   en compacto, es compacto. Por homeomorfismo, lo es $B_X$.
3. Compacidad implica precompacidad.
4. Por ser $B_X$ precompacta,

   \[B_X \subseteq \bigcup B\left(x_i,\frac{1}{2}\right)\]
   
   Como $M = \langle x_1,x_2,\dots,x_k \rangle$ es de dimensión finita, es un subespacio 
   cerrado y propio en $X$. Por el lema de Riesz, existe $x_0 \in \mathbb{S}_X$ 
   tal que:

   \[ \|x_0 - x_i\| \leq d(x_0,M) > \frac{1}{2} \]
   
   Teniendo entonces $x \notin \bigcup B(x_i,\frac{1}{2})$, que nos lleva a 
   contradicción.

**** Dual topológico en dimensión finita
Sea $X$ normado de dimensión finita, su *dual topológico* 
es:

\[X^\ast =
\left\{ f:X \longrightarrow \mathbb{K} \mid
f \text{ lineal } \right\}\]

Ya que toda lineal también es continua.

** 2. Principios fundamentales del análisis funcional
*** Teorema de Hahn-Banach
**** Funcionales sublineales
Un funcional $p : X \longrightarrow \mathbb{R}$ se dice *sublineal* cuando:

- $\forall t \in \mathbb{R}^+_0: p(tx) = tp(x)$
- $p(x+y) \leq p(x)+p(y)$

Si además $p(\alpha x) = |\alpha|p(x)$, se llama *seminorma*.

**** Versión analítica de Hahn-Banach
Sea $X$ espacio vectorial sobre $\mathbb{K}$ con $p$ sublineal. Y 
sea $M$ subespacio con un $g : M \longrightarrow \mathbb{K}$, verificando:

\[Re(g(m)) \leq p(m)\]

Entonces existe $f : X \longrightarrow \mathbb{K}$ lineal extendiéndolo y verificando:

\[Re(f(x)) \leq p(x)\]

Cuando $p$ es seminorma, se tiene además que $|f(x)|\leq p(x)$.

***** Demostración
****** Primera extensión en los reales
En un primer caso, sea $\mathbb{K} = \mathbb{R}$. Podemos tomar $x_0 \notin X-M$, 
crear $Y = M\oplus x_0\mathbb{R}$ y extender como:

\[ f(m + \lambda x_0) = g(m) + \lambda\alpha\]

****** Elegir el coeficiente de la extensión
Nos falta elegir el $\alpha$. Sabemos que debe cumplir:

\[\alpha \leq \frac{1}{\lambda}\left( p(m+\lambda x_0) - g(m) \right)\]

Tomando un $\lambda$ positivo y negativo llegamos a dos 
condiciones:

\[ g(v) - p(v-x_0) 
\leq \alpha 
\leq p(u+x_0) - g(u)\]

Y tenemos un $\alpha$ cumpliendo esta condición por ser 
equivalente a:

\[ g(u+v) \leq p(u+v) \leq p(u+x_0) + p(v-x_0)\]

Y lo sacamos partiendo la desigualdad, minimizando
y maximizando cada lado de la desigualdad, y dando
la vuelta a todas las desigualdades:

\[ g(u) - p(v-x_0) \leq \alpha \leq p(u+x_0) - g(u)\]

****** Lema de Zorn
Puedo ordenar las extensiones por inclusión, teniendo
además que una cadena de extensiones tiene por maximal a
la unión de todos los espacios, con la función definida
por el primer conjunto en el que aparece el primer elemento.

Si $Y \neq X$ fuera el maximal, podría añadir $x_0 \in X-Y$ y
contravenir la maximalidad de $Y$ extendiendo una dimensión.

****** Caso complejo
Como todo espacio sobre los complejos lo es sobre los reales,
aplicamos el caso real a $g_0 = Re(g)$, y obtenemos $f_0$ cumpliéndolo
y siendo lineal en los reales.

Creo $f$ siendo lineal en los complejos como:

\[f(x) = f_0(x) - if_0(xi)\]

Que cumple las condiciones

****** Caso de la seminorma
Cuando $p$ es seminorma tengo, para $|\alpha|=1$ dando el giro
apropiado:

\[f(\alpha x) = |f(x)| = Re(f(\alpha x)) \leq p(\alpha x) = p(x)\]

**** Extensión equinórmica de Hahn-Banach
Sea $X$ espacio normado, $M$ subespacio vectorial, con $g \in M^\ast$. 
Existe $f\in X^\ast$ tal que $f|_M = g$ y $\|f\| = \|g\|$.

***** Demostración
Sea $p(x) = \|g\|\|x\|$, es una seminorma y cumple $Re(g(m)) \leq p(x)$.
Por *Hahn-Banach*, tenemos una extensión $f$, cumpliendo $\|f\| \geq \|g\|$
por ser extensión y:

\[ \|f\| 
= \sup\left\{ \frac{|f(x)|}{\|x\|} \mid x\in X-\{0\}\right\} 
\leq \|g\|\]

**** Separación en el dual topológico
Sea $x_0 \in X$, entonces existe $f \in \mathbb{S}_{X^\ast}$, tal que $f(x_0) = \|x_0\|$.
En consecuencia:

 1. El dual topológico separa los puntos de $X$. Si $x \neq y$,
    existe $f \in X^\ast: f(x) \neq f(y)$.
 2. \[\forall x \in X: \|x\| = max\left\{ |f(x)| : f \in B_{X^\ast} \right\}\]

***** Demostración
Podemos aplicar Hahn-Banach a $g : x_0\mathbb{K} \longrightarrow \mathbb{K}$ con $g(\lambda x_0) = \lambda \|x_0\|$,
para obtener una extensión de norma $1$.

***** Corolario 1
Aplicamos el resultado para $f(x-y) = \|x-y\| = 0$.

***** Corolario 2
Tenemos $|f(x)| \leq \|x\|$. El mínimo se alcanza en un $f$ dado por
la proposición.

**** Corolario para subespacios finitos
Sea $X$ un espacio normado $\{x_1,\dots,x_n\} \subseteq X$ linealmente independientes
y $\alpha_1,\dots,\alpha_n \in \mathbb{K}$, entonces existe $f \in X^\ast$ tal que $f(x_i) = \alpha_i$.

***** Demostración
Puedo crear una función lineal, sobre $\langle x_1,\dots,x_n \rangle$ que lo cumpla.
Por venir de dimensión finita será continua, así que podemos
aplicar Hahn-Banach para obtener una extensión.

*** Inyección canónica e isometrías
**** Inyección canónica
Se define $J_X(x_0) : X^\ast \longrightarrow \mathbb{K}$ como:

\[J_X(x_0)(f) = f(x_0)\]

**** Propiedades de la inyección
Se verifica:

 1. $\forall x \in S_X: J_X(x)$ es lineal y de módulo $1$.
 2. $J_X : X \longrightarrow X^\ast^\ast$ es isométrica y lineal.

**** Completación de un espacio
$X^\ast$ es completo para cualquier espacio normado $X$. 
Cuando $X$ no es completo, $J_X$ no es sobreyectivo y podemos
completarlo como:

\[ X \subset \overline{J_X(X)} \]

Que lo será por ser cerrado en $X^{\ast\ast}$, que es completo.

**** Polar de un subespacio
Dado $M \subset X$, el *polar* de $M$ se define como:

\[M^0 = \left\{ f\in X^\ast \mid f(M) = \{0\} \right\}\]

Si tenemos la restricción $S : X^\ast \longrightarrow M^\ast$, $M^0 = \ker S$.

**** Primer teorema de isometría
Sea $X$ espacio normado y $M$ subespacio, tenemos una biyección
lineal isométrica \[T : {X^\ast}/{M^0} \longrightarrow M^\ast \] dada por:

\[T(f + M^0) = f|_M\]

***** Demostración
Tenemos $\|T(f+M^0)\| \leq \|f+M^0\|$, y obtenemos igualdad gracias
a Hahn-Banach.

**** Segundo teorema de isometría
Sea $X$ espacio normado y $M$ subespacio vectorial cerrado de $X$.
La aplicación $T : (X/M)^\ast \longrightarrow M^0$ es una biyección lineal isométrica
definida por:

\[ T(f)(x) = f(x+M) \]

Además $T = f \circ Q$ para $Q$ proyección.

***** Demostración
Claramente es lineal. Tenemos que es lipschitziana por:

\[ \|T(f)\| = \| f \circ Q \| \leq \|f\|\|Q\| = \|f\| \]

Y por otro lado,

\[ \|T(f)\|\|x+m\| \geq 
\|T(f)(x+m)\| = 
\|f(x+M)\|\]

Por tanto, $\|T(f)\| \geq \|f\|$.

**** Función de aproximación
Sea $X$ normado, $M$ subespacio con $x_0 \in X-\overline{M}$. Existe $f\in X^\ast$ con $\|f\|=1$,
tal que $f \in M^0$ y $f(x_0) = d(x_0,\overline{M}) = d(x_0,M)$.

***** Demostración
Como $\overline{M}$ es subespacio cerrado, tenemos $g \in (X/M)^\ast$ con $\|g\|=1$ y
$g(x_0+\overline{M}) = \|x_0+\overline{M}\|$. Entonces $T(g) \in \overline{M}^0$ y $T(g)(x) = g(x+\overline{M})$.
Tenemos $\|T(g)\|= \|g\|=1$ y $f(x_0) = g(x_0+\overline{M}) = \|x_0+\overline{M}\|$.

**** Caracterización del polar
Sea $X$ normado, $M$ subespacio. Entonces:

\[ \overline{M} = \bigcap_{f \in M^0} \ker(f)\]

**** Distancia a un kernel
Sea $X$ espacio normado, $f \in X^\ast - \{0\}$ y $x_0 \in X$. Entonces,

\[d(x_0,\ker(f)) = \frac{|f(x_0)|}{\|f\|}\]

***** Demostración
Por el [[*Función de aproximación][teorema de aproximación]] con $M = \ker(f)$, tenemos una $g$ con
$\|g\| = 1$ tal que $g(x_0) = d(x_0,M)$; tenemos $\ker(f) \subseteq \ker(g)$.

\[\exists u\in X: f(u)=1\]. Para $x\in X$, tenemos $x -f(x)u \in \ker(f)$ y entonces:

\[ 0 = g(x-f(x)u) = g(x) - f(x)g(u)\]

Aplicando esto en $x_0$ tenemos $g(x_0) = g(u)f(x_0)$, que tomando módulos,
nos da $\|g\| = \|f\| |g(u)|$.

**** Funcional de Minkowski
Se define $p_U : X \longrightarrow \mathbb{R}^+$ para $X$ normado con $U$ entorno de $0$ como:

\[ p_U(x) = \inf\{ \lambda \in \mathbb{R}^+_0 \mid x \in \lambda U\} \]

Para un entorno de $0$ convexo, $p_U$ es sublineal.

**** Separación de un punto
Sea $U$ entorno de $0$ convexo con $x_0 \notin U$, existe $f \in X^\ast$ tal que 
$Re(f(x)) \leq 1$ para todo $x \in U$; mientras $Re(f(x_0)) \geq 1$.

Se cumple además:

\[ \{x \in X \mid p_U(x)<1\} \subset
U \subset
\{ x \in X \mid p_U(x) \leq 1\}\]

***** Demostración
Tomamos $g : x_0\mathbb{R} \longrightarrow \mathbb{R}$ definido por $g(\alpha x_0) = \alpha p_U(x_0)$.
Aplicamos [[*Versión analítica de Hahn-Banach][Hahn-Banach]] sobre $x_0\mathbb{R}$ y tenemos un $f$ extensión de $g$
verificando que $f(x_0) = p_U(x_0) \geq 1$, y que para $x \in U$ se tiene 
$f(x) \leq p_U(x) \leq 1$.

Ahora para el caso complejo, tenemos $f_0$ lineal y continuo
cumpliendo que $f_0(x_0)\geq 1$, pero $f_0(x) \leq 1$ para todo $x \in U$.
Definimos $f(x) = f_0(x) - if_0(ix)$, y entonces es lineal en los
complejos cumpliendo lo pedido.

**** Separación de convexos (para un abierto)
Sean $A,B \subset X$ convexos con $A \cap B = \varnothing$ con $A$ abierto.
Existe $f \in X^\ast$ con $\alpha \in \mathbb{R}$ tal que:

\[ Re(f(a)) < \alpha \leq Re(f(b)) \quad \forall a \in A, b \in B\]

**** Existencia de funcionales de soporte
Sea $X$ normado, $A \subset X$ convexo cerrado con $\mathring{A} \neq \varnothing$. Para cada 
$x_0 \in Fr(A)$; existe $f \in X^\ast$ tal que:

  - $\|f\| = 1$
  - $Re(f(x_0)) = \max\{Re(f(x)) \mid x \in A\}$

***** TODO Demostración

**** Separación de convexos (para un compacto)
Sean $A,B \subset X$ convexos con $A \cap B = \varnothing$ con $A$ compacto y 
$B$ cerrado. Existe $f \in X^\ast$ con $\alpha \in \mathbb{R}$ tal que:

\[ Re(f(a)) < \alpha < Re(f(b)) \quad \forall a \in A, b \in B\]

***** TODO Demostración

*** Lema de categoría de Baire
**** Teorema de Baire
Sea $E$ espacio métrico completo y sea $\forall n : G_n \subset E$ un abierto 
denso. $\bigcap_{n \in \mathbb{N}} G_n$ es denso.

***** Demostración
Empezando con un abierto $G$ y con $G_1$, puedo a cada paso tomar el 
abierto anterior, tomar un abierto dentro de él como 
$\overline{B}(a_i,r_i) \subset G_i \cap B(a_{i-1},r_{i-1})$, y construir una sucesión $\overline{B}(a_n,r_n)$.

Esta sucesión podemos tomarla para que cumpla $\{r_n\} \to 0$. Desde
aquí tenemos que converge $\{a_n\} \to a \in \bigcap \overline{B}(a_n,r_n) \cap G$ por ser de 
Cauchy.

**** Corolario al teorema de Baire
Sean $F_n \subset E$ cerrados con:

\[E = \bigcup_{n \in \mathbb{N}} F_n\]

Entonces, $\exists n \in \mathbb{N}$ tal que $\mathring{F}_N \neq \varnothing$.

***** Demostración
Aplicando el teorema de Baire en sus complementos.

**** Dimensión en espacios de Banach
Todo espacio de Banach tiene dimensión finita o no numerable.

***** Demostración
Si fuera $X$ espacio de Banach con base numerable, cualquier
subespacio de dimensión finita sería cerrado, pero entonces,
tomando $F_n = \langle e_1,\dots,e_n \rangle$:

\[ X = \bigcup F_n \]

Luego para algún $n$, se tiene $\mathring{F_n} \neq \varnothing$; así que debe ser 
$X=F_n$.

*** Teorema de la aplicación abierta
**** Teorema de la aplicación abierta
Sean $X,Y$ espacios de Banach con $T \in L(X,Y)$ sobreyectiva. 
Entonces, $T$ es abierta; y, por tanto, epimorfismo topológico.

***** Demostración
****** La imagen de bola tiene interior no vacío
Como $X = \bigcup_{n \in \mathbb{N}} n B_X$, tenemos que:

\[ Y 
= T\left(\bigcup_{n \in \mathbb{N}} n B_X \right)
= \bigcup_{n \in \mathbb{N}} n T(B_X) 
\subseteq \bigcup_{n \in \mathbb{N}} \overline{n T(B_X)}
= Y
\]

Aplicando corolario a Baire, $\exists N: \mathring{\overline{NT(B_X)}} \neq \varnothing$, luego
$\mathring{\overline{T(B_X)}} \neq \varnothing$. 

****** La imagen de la bola es entorno de 0
Sea ahora, $y_0 \in \mathring{\overline{T(B_X)}}$, se tendrá que:

\[ 0 \in \mathring{\overline{T(B_X)}} - y_0
\subset \overline{T(B_X)} - \overline{T(B_X)}
\subset 2\overline{T(B_X)}\]

Siendo por tanto $\overline{T(B_X)}$ un entorno de 0.

****** Acotación de la bola
Tenemos $\exists\delta > 0: \delta B_Y \subset \overline{T(B_X)}$, y en general:

\[ \forall n \in \mathbb{N}: \frac{\delta}{2^n}B_Y \subseteq \overline{T\left(\frac{1}{2^n}B_X\right)}\]

****** Sucesión
Tomamos $x_0 = 0$, $y \in \overline{T(\frac{1}{2}B_X)}$, y construimos sabiendo:

\[ y - T(x_i) \in
\frac{\delta}{2^i} B_Y \subseteq
T\left(\frac{1}{2^i} B_X\right)\]

Luego existe un $x_{i+1}$ verificando $\|x_{i+1}\|\leq \frac{1}{2^{i+1}}$ y que:

\[ \left\| y - \sum_{k=1}^{i+1} T(x_k) \right\| < \frac{\delta}{2^{i+2}} \]

****** La suma converge
Definimos la suma de la sucesión:

\[ S_n = \sum_{k=1}^n x_k \in X\]

Es de Cauchy por ser convergente $\sum_{n=1}^\infty \frac{1}{2^n}$. Por complitud,
converge, $S_n \to x$, con:

\[ \|S_n\| \leq \sum \|x_k\| \leq \sum \frac{1}{2^n} = 1 \]

Luego $\|x\| \leq 1$, $x \in B_X$. Como además se tiene:

\[\| y - T(S_n) \| \leq  
\|y - \sum T(x_k) \| < 
\frac{\delta}{2^{n+1}} \]

Tenemos $\lim\{T(S_n)\} = T(x)$ y concluimos $y \in T(B_X)$.

****** Conclusión
Hemos probado $\overline{T(\frac{1}{2} B_X)} \subseteq T(B_X)$, y finalmente,

\[ \frac{\delta}{2} B_Y \subset \overline{T\left(\frac{1}{2} B_X\right)} \subset T(B_X)\]

Así que $T(B_X)$ es un entorno de $0$ y $T$ es abierta.

**** Teorema de los isomorfismos de Banach
Sean $X,Y$ espacios de Banach con $T \in L(X,Y)$, biyección. 
Entonces $T$ es isomorfismo topológico.

***** Demostración
Por [[*Teorema de la aplicación abierta][teorema de la aplicación abierta]] $T$ y $T^{-1}$ son abiertas;
luego $T^{-1}$ y $T$ son continuas.

**** Teorema del homomorfismo de Banach
Sean $X,Y$ espacios de Banach con $T \in L(X,Y)$. Será homomorfismo 
topológico ssi $TX$ es cerrado en $Y$.

***** TODO Demostración
**** Equivalencia de normas en espacios de Banach
Sean $\|\cdot\|_1, \|\cdot\|_2$ dos normas en un espacio de Banach. Si cumplen que:

\[\exists M>0 : \|x\|_1 \leq M \|x\|_2\]

Entonces son equivalentes.

***** Demostración
Sea $T(x)=x$ biyección lineal entre las dos normas. Es 
lipschitziana por la condición. Por el
[[*Teorema de los isomorfismos de Banach][teorema de los isomorfismos de Banach]], es isomorfismo topológico.

*** Teorema de la gráfica cerrada
**** Gráfica de una función
La *gráfica* de una función $f$ se define como:

\[ Graf(f) = 
\{(x,f(x)) \mid x \in A\} \subseteq
A \times B\]

Una $T$ es lineal ssi $Graf(T)$ es subespacio vectorial, y
cuando $f$ es continua en Hausdorff, $Graf(f)$ es cerrada.

**** Teorema de la gráfica cerrada
Sean $X,Y$ de Banach con $T : X \longrightarrow Y$ lineal. Si la gráfica 
de $T$ es cerrada, $T$ es continua.

***** Demostración
Si tomamos $\|x+y\| = \|x\|+\|y\|$, que genera la topología
producto en $X \times Y$, tenemos un Banach. Como $T$ es lineal,
$G(T)$ es subespacio lineal de $X \times Y$, y como $G(T)$ es cerrado,
es de Banach con la norma inducida.

Sea $\Phi : GT \longrightarrow X$ definida por:

\[ \Phi(x,Tx) = x\]

Como $\Phi$ es lineal, su restricción es continua, lineal y biyectiva.
Por [[*Teorema de los isomorfismos de Banach][teorema de isomorfismo de Banach]], $\Phi^{-1}$ es continua; y entonces
$T = \pi_2 \circ \Phi^{-1}$ es continua.

**** Caracterización de la gráfica cerrada en espacios normados
Sean $X,Y$ normados con $T: X \longrightarrow Y$ lineal. Equivalen:

- $T$ con gráfica cerrada.
- Si $\{x_n\} \longrightarrow 0$ y $\{Tx_n\}\longrightarrow y$; entonces $y=0$.
 
***** TODO Demostración

*** Teorema de Banach-Steinhaus
**** Teorema de Banach-Steinhaus para funcionales
Sea ${\cal A} \subset L(X,Y)$ para $X$ de Banach e $Y$ normado; una familia de 
operadores acotada puntualmente:

\[ \forall x : \exists M(x): \forall T \in A: \quad \|T(x)\| \leq M(x)\]

Entonces está acotada:

\[\exists M : \forall T \in A: \quad \|T\| \leq M \]

***** Demostración
Tomamos $F_n$ como intersección de conjuntos que son cerrados
por la continuidad de $T$:

\[F_n = \bigcap_{T \in {\cal A}} \{ x \in X \mid \|Tx\| \leq n\}\]

Como está acotada puntualmente, $\bigcup F_n = X$; así que aplicamos
el [[*Corolario al teorema de Baire][corolario a Baire]] para tener un $\mathring{F_N} \neq \varnothing$. Eso quiere decir
que $\exists a: \exists r: a + rB_X \subseteq F_n$. Para $T \in {\cal A}$ se tiene:

\[\begin{aligned}
\|Tx\| =& \frac{1}{r} \| T(a+rx) - Ta\| \\
     \leq& \frac{1}{r} (\| T(a+rx) \| + \|Ta\|) \\
     \leq& \frac{N}{r} + \|Ta\|\frac{1}{r} \\ 
     \leq& \frac{N}{r} + M(a)\frac{1}{r}
\end{aligned}\]

Acotación independiente de $X$.

**** Teorema del cierre de Steinhaus
Sea $X$ de Banach, $Y$ normado, y una sucesión $\{T_n\} \in L(X,Y)$ 
convergiendo puntualmente. La convergencia puntual da un operador 
lineal y continuo:

\[ T(x) = \lim_{n \longrightarrow \infty} T_n(x) \in L(X,Y)\]

***** Demostración
La linealidad se tiene trivialmente:

\[ T(\alpha x_1 + \beta x_2) 
= \lim T_n (\alpha x_1 + \beta x_2)
= \alpha T(x_1) + \beta T(x_2) \]

Como $\{T_n\}$ es una familia acotada puntualmente por converger
puntualmente, se tiene por [[*Teorema de Banach-Steinhaus para funcionales][Banach-Steinhaus]] que está acotada.

Entonces para $x \in B_X$, tenemos $\|T_n(x)\| \leq M$, así que:

\[ \|\lim T_n(x)\| =
\lim \|T_n(x)\| \leq M\]

Luego $\|T(x)\| \leq M$, y por estar acotada en la bola unidad
y ser lineal, $T$ es continua.

**** Corolario a Banach-Steinhaus para el dual
Sea $X$ espacio de Banach y $A \subseteq X^\ast$, equivalen:

 1. $A$ acotado, $\exists M>0: \forall f \in A: \|f\| \leq M$
 2. $A$ puntualmente acotado, $\forall x\in X: \{f(x) \mid f \in A\}$ acotado.

***** Demostración
Por [[*Teorema de Banach-Steinhaus para funcionales][teorema Banach-Steinhaus]] con $X^\ast = L(X,\mathbb{K})$ se tiene
la segunda implicación. La primera se tiene simplemente por
tenerse:

\[ \|f(x)\| \leq \|f\|\|x\| \leq M \|x\| \]

**** Corolario a Banach-Steinhaus para el doble dual
Sea $X$ espacio normado con $A \subseteq X$. Equivalen:

 1. $A$ acotado.
 2. $\{ f(x) \mid x \in A\}$ acotado para cualquier $f \in X^\ast$.

***** Demostración
Sabemos $J_X$ isometría, luego $J_X(A)$ está acotado. Como 
la acotación equivale a la acotación puntual, para cualquier
punto del espacio $f \in X^\ast$ se tiene acotado:

\[ \{J_X(x)(f) \mid x \in A \} \]

** 3. Espacios de Hilbert
** 4. Topologías débiles
* Inferencia Estadística
** DONE Apuntes en clase
*** Introducción
**** Estadísticos
 *Estadístico*. Función medible sobre variables aleatorias $f(X_1,X_2,\dots,X_n)$.
 Se dice que es un *estimador consistente* de un parámetro cuando converge 
 en probabilidad a él.

*** Distribuciones continuas
**** Distribución uniforme
 *Distribución uniforme*. Sobre un intervalo $[a,b]$, definida por:

 \[f(x|a,b) = \frac{1}{b-a}\]

 \[EX = \int^b_a \frac{x}{b-a}dx = \frac{b+a}{2}\]
 \[Var X = \int_a^b \frac{(x-\frac{b+a}{2})^2}{b-a} dx = \frac{(b-a)^2}{12}\]

**** Distribución gamma
 #+begin_definition
 *Función gamma*. La siguiente integral converge para $\alpha > 0$:

 \[\Gamma(\alpha) = \int_0^\infty t^{\alpha-1}e^{-t}dt\]
 #+end_definition

 Cumple que: $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$, de esta forma, generaliza al factorial
 con $\Gamma(n) = (n-1)!$.

 #+begin_definition
 *Distribución gamma*. Sobre $[0,\infty)$, dada $\alpha$, se tiene la función de distribución:

 \[f(x|\alpha,\beta) = 
 \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1}e^{-x/\beta}\]
 #+end_definition

**** Distribución de Dirichlet
 #+begin_definition
 *Distribución de Dirichlet*. 

 \[f(x_1\dots x_n| \alpha_1, \dots \alpha_k,\alpha_{k+1}) = 
 \frac{\Gamma(\alpha_1+\dots+\alpha_{k+1})}
 {\Gamma(\alpha_1)\dots \Gamma(\alpha_{k+1})}
 x_1^{\alpha_1-1} \dots x_{k}^{\alpha_{k-1}-1}
 \]
 #+end_definition

 # Esperanza
 # Integral de dirichlet
 # Subvector
 # Dirichlet ordenada

*** Máxima verosimilitud
 #+begin_definition
 *Máxima verosimilitud*. Sean $X_1,\dots,X_n$ v.as. independientes extraídas de una
 función de probabilidad perteneciente a una familia 
 $\{f(\bullet | \theta), \theta \in \Theta\}$ llamada *modelo*, pero con $\theta$ desconocida

 Llamamos *estimador de máxima verosimilitud* de $\theta$ al $\hat\theta$ que maximiza 

 \[{\cal L}(\hat\theta | x_1,\dots,x_n) = \prod_{i=1}^n f(x_i|\theta)\]

 llamada *función de verosimilitud*.
 #+end_definition

 La idea del método es tomar la función de densidad conjunta asumiendo independencia:

 \[f(x_1,\dots,x_n | \theta) = f(x_1|\theta) f(x_2|\theta) \dots f(x_n|\theta)\]

 Y, suponiendo que los valores fueran fijos, estimar $\theta$ con la función de
 la función de verosimilitud o de su logaritmo:

 \[\hat l (\hat\theta | x_1,\dots,x_n) = \sum_{i=1}^n \ln f(x_i|\theta)\]


***** Ejemplo de una distribución binomial

 # Conjuntos creíbles

** Prerrequisitos
*** Varianza
**** Varianza
La varianza se define equivalentemente como:

\[Var(X) = E\Big[(X-EX)^2\Big] = E[X^2] - E[X]^2\]

**** Covarianza
La covarianza se define equivalentemente como:

\[cov(X,Y) = E[(X-EX)(Y-EY)] = E[XY] - E[X]E[Y]\]

Nótese que $cov(X,X) = Var(X)$. Nótese además se comporta como el 
[[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][producto interno]] de un espacio prehilbertiano.

**** Varianza de la suma
La varianza de una suma cumple:

\[
Var(X+Y) = Var(X) + Var(Y) + 2cov(X,Y)
\]

En el caso general:

\[Var\left(\sum X_i\right) = \sum_i\sum_j cov(X_i,X_j)\]

**** Cauchy-Schwarz para la covarianza
Se tiene la desigualdad:

\[cov(X,Y)^2 \leq Var(X)Var(Y)
\]

***** Demostración
Sabiendo que la varianza es siempre no negativa:

\[
0 \leq Var\left(X - \frac{cov(X,Y)}{Var(Y)} Y\right) =
Var(X) - \frac{\left(cov(X,Y)\right)^2}{Var(Y)}
\]

***** Demostración por Cauchy-Schwarz
Se comprueba que la covarianza da un producto escalar que genera
un [[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][espacio cociente]] prehilbertiano. Aplicamos Cauchy-Schwarz.

*** Esperanza condicional
**** Esperanza condicional en caso discreto
Definimos la esperanza condicional de dos variables discretas como:

\[\mathbb{E}[X|Y] = \sum_x xP(X=x\mid Y=y) = \sum_x x\frac{P(X=x, Y=y)}{P(Y=y)}\]

**** Esperanza condicional en el caso continuo
Más generalmente se define para el caso continuo:

\[
\mathbb{E}[X|Y] = \int_X x f_{X|Y}(x|y) dx = \int_X x \frac{f_{X,Y}(x,y)}{f_Y(y)} dx
\]

**** Ley de esperanza total
La esperanza condicional cumple:

\[\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]\]

***** Demostración en el caso discreto
Se tiene:

\[\begin{aligned}
E[E[X|Y]] &= \int_Y f(y)  \left(\int_X x \frac{f(x,y)}{f(y)} dx \right) dy \\ñ
&= \int_X x \int_Y f(x,y) dy dx \\&= \int_X x f(x) dx = E[X]
\end{aligned}\]

Nótese que asumimos una conmutatividad de las integrales discretas.

***** Demostración en el caso continuo
Puede consultarse la [[https://en.wikipedia.org/wiki/Law_of_total_expectation#Proof_in_the_general_case][Ley de la esperanza total]].

** 1. Introducción a la inferencia estadística. Estadísticos muestrales.
*** Planteamiento de un problema de inferencia
**** Modelo estadístico
Un modelo estadístico $(X,{\cal P})$ consta de:

  - $X : (\Omega, {\cal A},{\cal P}) \longrightarrow (\mathbb{R},{\cal B},P_X)$ variable aleatoria que describe el 
    objeto de estudio.
  - ${\cal P}$ familia de distribuciones que pueden ser la de $X$.

**** Modelo estadístico paramétrico
Cuando se conoce la forma funcional de $P_X$ y sólo desconocemos un 
parámetro tenemos una familia paramétrica de distribuciones $F(x,\theta)$ 
para $\theta$.

**** Modelo estadístico no paramétrico
Cuando la forma funcional de $P_X$ es desconocida.

**** Muestra aleatoria simple
Una muestra aleatoria simple es un vector $(X_1,\dots,X_n)$
de variables independientes idénticamente distribuidas. 

***** Realización muestral
Una realización muestral a un valor concreto obtenido al 
observar la muestra.

***** Espacio muestral
Conjunto de todas las posibles realizaciones.

*** Función de distribución empírica
**** Función de distribución muestral
La *función de distribución empírica* es una función de 
distribución razonable que podemos obtener desde una 
realización muestral.

 \[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(X_i < x)} \]

**** Propiedades de la función de distribución empírica
Fijado un $x \in \mathbb{R}$, $F^\ast(x)$ es una variable aleatoria siguiendo por 
definición una binomial:

\[ nF^\ast(x) \longrightarrow {\cal B}(n, F(x))\]

Calculamos su *esperanza* y *varianza* desde Bernoulli como:

 - Esperanza: $E[F^\ast(x)] = F(x)$
 - Varianza: $Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}$

Aplicando entonces el Teorema Central del Límite:

\[ \frac{F^\ast(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \leadsto {\cal N}(0,1) \]

**** Teorema de Glivenko-Cantelli
Las funciones de distribución muestrales convergen 
casi seguramente y uniformemente a la teórica.

\[ P\left\{ \lim_{n \rightarrow \infty} 
\sup_{x \in \mathbb{R}} |F^\ast_n(x) - F(x)| = 0\right\} = 1\]

***** Equivalentemente
Con probabilidad 1 se tiene que, al tomar sucesivas observaciones 
independientes y considerar las correspondientes funciones de 
distribución muestrales:

\[\forall x \in \mathbb{R}: \forall \epsilon>0: \exists n_\epsilon : \forall n \geq n_\epsilon:
\quad F^\ast_n(x) - \epsilon < F_X(x) < F^\ast_n(x) + \epsilon\]

***** Demostración
[[http://matematicas.unex.es/~nogales/estadisticamatematica/TGC.pdf][Teorema de Glivenko-Cantelli]].

*** Estadístico muestral
**** Estadístico muestral
Dada una muestra aleatoria simple, un *estadístico muestral* es una 
función sobre ella $T : (\mathbb{R}^n,{\cal B}^n)\longrightarrow (\mathbb{R}^k,{\cal B}^k)$ medible e independiente 
de cualquier parámetro desconocido.

**** Estadísticos de interés
***** Momentos muestrales no centrados
Para cada $k \in \mathbb{N}$:

\[A_k = \frac{1}{n}\sum_{i=1}^n X_i^k\]

***** Momentos muestrales centrados
Para cada $k \in \mathbb{N}$:

\[B_k = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^k\]

***** Media muestral
Caso particular,

\[A_1 = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X}\]

***** Varianza muestral
Caso particular,

\[B_2 = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2\]

***** Cuasivarianza muestral

1\[S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2\]

** 2. Distribuciones. Muestreo de poblaciones normales
*** Distribución normal
**** Definición
Definimos la distribución normal ${\cal N}(\mu,\sigma^2)$ como aquella con función de
densidad:

\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

***** Imagen de la distribución
#+BEGIN_SRC R :file images/normal.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dnorm, seq(-3, 3, 0.1),
                          mean = 0, sd = 1+0.1*i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "Distribución normal, variando σ²."))
#+END_SRC

#+RESULTS:
[[file:images/normal.png]]

***** Es una distribución
Tenemos que comprobar que integra la unidad sobre los reales, y
de hecho, tomando cambio de variable $y = (x-\mu)/\sqrt{2\sigma^2}$ queda:

\[\begin{aligned}
\int^{+\infty}_{-\infty} 
\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2} =
\frac{1}{\sqrt{\pi}}\int^{+\infty}_{-\infty} 
e^{-y^2} = 1
\end{aligned}\]

Que es la [[https://en.wikipedia.org/wiki/Gaussian_integral][integral de Gauss]].

**** Función característica
La función característica de ${\cal N}(\mu,\sigma^2)$ es:

\[\varphi_X(t) = e^{it\mu - t^2\sigma^2/2}\]

***** TODO Demostración
Usamos la definición de función característica y completamos
cuadrados para tener:

\[\begin{aligned}
\varphi_X(t) &= 
\mathbb{E}\left[e^{itX}\right] &= 
\int_{-\infty}^{+\infty}
e^{itx}\frac{1}{\sqrt{2\pi\sigma^2}} 
e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx \\&=
\frac{1}{\sqrt{2\pi\sigma^2}} 
\int_{-\infty}^{+\infty}
e^{it\mu}
\end{aligned}\]

**** Suma de normales
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ e $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. Entonces $X+Y\leadsto {\cal N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.

***** TODO Demostración
**** Teorema de Cramer
Sean $X,Y$ independientes. Si $X+Y$ es normal, $X$ e $Y$ son normales.

***** TODO Demostración

*** Distribución Gamma
**** Función Gamma
Se define la función gamma $\Gamma : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\Gamma(\alpha) = \int^\infty_0 t^{\alpha-1}e^{-t} dt\]

***** La integral existe
Por un lado, $t^{a-1}e^{-t} < t^{a-1}$, integrable en $[0,b]$. Por otro lado,

\[\lim_{t \to \infty} \frac{t^{\alpha-1}e^{-t}}{e^{-t/2}} = 0\]

Por lo que $t^{\alpha-1}e^{-t} < e^{-t/2}$ integrable, a partir de algún punto.
Partimos la integral como:

\[\int_0^b t^{\alpha-1}e^{-t}dt + \int^{\infty}_b t^{\alpha-1}e^{-t}dt
< \infty\]

**** Propiedades de la función Gamma
Sea $\alpha > 0$, se verifica:

  1. $\Gamma(1) = 1$.
  2. $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$.
  3. $\Gamma(n+1) = n!$ para $n \in \mathbb{N}$.
  4. $\Gamma(\alpha)\Gamma(1-\alpha) = \frac{\pi}{\sin(\alpha\pi)}$ para $0<\alpha<1$.
  5. $\Gamma(1/2) = \sqrt{\pi}$.
  6. $\Gamma(\alpha) = \beta^\alpha \int^\infty_0 t^{\alpha-1}e^{-\beta t} dt$ para $\beta > 0$.

***** Demostración
****** Punto 1
Trivial.
****** Punto 2
Integral por partes.
****** Punto 3
Inducción sobre los dos primeros apartados.
****** TODO Punto 4
****** Punto 5
Trivial desde el punto anterior.
****** Punto 6
Cambio de variable $\varphi(t) = \beta t$.

**** Distribución Gamma
Dados $\alpha,\beta > 0$, definimos la distribución Gamma $\Gamma(\alpha,\beta)$ como aquella con
función de densidad:

\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\]

para $x>0$.

***** Imagen de la distribución
#+BEGIN_SRC R :results graphics :file images/gamma.png
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dgamma, seq(0, 4, 0.05),
                          shape = i, rate = 1,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "Distribución gamma, variando α."))
#+END_SRC

#+RESULTS:
[[file:images/gamma.png]]
**** Propiedades de la distribución Gamma
La función de densidad de una distribución $\Gamma(\alpha,\beta)$ verifica:

  1. Cuando $0<\alpha<1$, $f$ es decreciente y $\lim_{x\to 0} f(x) = \infty$.
  2. Cuando $\alpha = 1$, $f$ es decreciente y $f(0)=1$.
  3. Cuando $\alpha>1$, $f$ es creciente en $[0,(\alpha-1)/\beta]$ y decreciente 
     en $[(\alpha-1)/\beta,\infty]$.

Y sobre convexidad y concavidad se tiene:

  1. Si $0<\alpha\leq 1$, es convexa.
  2. Si $1 < \alpha \leq 2$, es cóncava en $[0,(\alpha-1+\sqrt{\alpha+1})/\beta]$ y convexa
     en $[(\alpha-1+\sqrt{\alpha+1})/\beta,\infty]$.
  3. Si $2 < \alpha$, es cóncava en $[(\alpha-1-\sqrt{\alpha+1})/\beta,(\alpha-1+\sqrt{\alpha+1}/\beta)]$ y
     convexa en todo el resto del dominio.

***** TODO Demostración

*** Distribución Beta
**** Función Beta
Se define la función beta $\beta : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\beta(x,y) = \int^1_0 t^{x-1}(1-t)^{y-1}dt\]

***** Está bien definida
Por la [[*Relación con la función Gamma][relación con la función Gamma]] sabemos que debe estar
bien definida.

**** Relación con la función Gamma
Para cada $x,y$ se tiene:

\[\frac{\Gamma(x)\Gamma(y)}{\Gamma(xy)} = \beta(x,y)\]

***** TODO Demostración

**** Distribución Beta
Dados $p,q>0$, definimos la distribución Beta $\beta(p,q)$ como aquella con 
función de densidad:

\[f(x) = \frac{1}{\beta(p,q)} x^{p-1}(1-x)^{q-1}\]

*** Distribución χ² de Pearson
**** Distribución chi cuadrado
Es un caso particular de la distribución gamma, $X \leadsto \chi^2(k) = \Gamma(k/2,1/2)$.
Al parámetro $k$ se le llama *número de grados de libertad*.

**** Función de densidad

 \[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]

***** TODO Demostración
**** Función generatriz de momentos

\[M_X(t) = \frac{1}{(1-2t)^{n/2}}\], para $t < 1/2$.

***** TODO Demostración
**** Esperanza y varianza

 - $E[X] = k$
 - $Var[X] = 2k$

***** TODO Demostración

**** Propiedad de reproductividad
Si tengo una serie de variables independientes distribuidas 
por $X_i \leadsto \chi^2(k_i)$, entonces:

\[\sum_{i=1}^n X_i \leadsto \chi^2 \left(\sum_{i=1}^n k_i \right)\]

**** Relación con la distribución normal
Dadas variables independientes $X_i \leadsto {\cal N}(0,1)$,

 \[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]

***** TODO Demostración

**** Aproximación
Para valores pequeños, pueden usarse tablas. Para valores grandes
de $n$, podemos aproximarla mediante el Teorema Central del Límite
como:

\[ \chi^2(x) \approx {\cal N}(n,2n)\]

***** TODO Demostración

**** TODO Gráfica de la función de densidad
*** Distribución t de Student
**** T de Student
Dadas dos variables independientes $X \leadsto {\cal N}(0,1)$ e $Y \leadsto \chi^2(n)$, 
tenemos:

\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]

**** Función de densidad

\[ f(t) 
= \frac
{\Gamma\left(\frac{n+1}{2}\right)}
{\Gamma\left(\frac{n}{2}\right) \sqrt{n\pi}} 
\left(
1 + \frac{t^2}{n}
\right)^{-\frac{n+1}{2}}
\], $t \in \mathbb{R}$

***** TODO Demostración

**** Momentos
Tenemos que $\exists E[T^k] \iff k < n$. Cuando existen, se tiene

 - $E[T] = 0$
 - $Var[T] = \frac{n}{n-2}$

***** TODO Demostración

**** Aproximación
Tabulada para $n$ pequeños y aproximada por ${\cal N}(0,1)$ para valores
grandes.

**** TODO Gráfica de la función de densidad
*** Distribución F de Snedecor
**** Definición
*F de Snedecor*. Dadas dos variables independientes $X \leadsto \chi^2(n)$ e
$Y \leadsto \chi^2(m)$, su cociente nos da:

\[F = \frac{X/m}{Y/n} \leadsto F(m,n)\]

**** Función de densidad

\[g(t)
= \frac
{\Gamma(\frac{m+n}{2})}
{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}
\left(\frac{m}{n}\right)^{\frac{m}{2}}
t^{m/2-1}
\left(1+\frac{m}{n}t\right)^{-\frac{m+n}{2}}\], para $t>0$.

***** TODO Demostración

**** Momentos
Tenemos que $\exists E[T^k] \iff k < n/2$.

 - $n > 2 \Rightarrow \exists E[F] = \frac{n}{n-2}$
 - $n > 4 \Rightarrow \exists Var[F] = \frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)}$

***** TODO Demostración

**** Propiedades
\[ F \leadsto F(m,n) \iff F^{-1} \leadsto F(n,m)\]
\[T \leadsto t(n) \iff T^2 \leadsto F(1,n)\]

**** Aproximación
La distribución está tabulada y las tablas incluyen aproximaciones 
para valores grandes de $n$ y $m$.

**** TODO Gráfica de la función de densidad
*** Muestreo de la normal
**** Lema de Fisher
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple con $X \leadsto {\cal N}(\mu,\sigma^2)$.
Los estadísticos $\overline{X}$ y $S^2$ son independientes.

***** TODO Demostración 
**** Distribuciones del muestreo de una normal unidimensional
***** Inferir la media con varianza conocida

\[\frac
{\overline{X}-\mu}
{\sigma/\sqrt{n}}
\leadsto
{\cal N}(0,1)\]

***** Inferir la media con varianza desconocida

\[\frac
{\overline{X} - \mu}
{S/\sqrt{n}}
\leadsto
t(n-1)\]

***** Inferir la varianza con media conocida

\[\frac
{\sum^n_{i=1}(X_i - \mu)^2}
{\sigma^2}
\leadsto
\chi^2(n)\]

***** Inferir la varianza con media desconocida

\[\frac
{(n-1)S^2}
{\sigma^2}
\leadsto
\chi^2(n-1)\]

*** Muestreo de dos normales
**** Extensión del lema de Fisher
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes.
Los vectores $(\overline{X},\overline{Y})$ y $(S^2_1,S^2_2)$ son independientes.

***** TODO Demostración
**** TODO Distribuciones del muestreo de dos normales unidimensionales

** 3. Suficiencia y completitud
*** Estadísticos suficientes y completos
**** Estadístico suficiente
Un estadístico $t$ es *suficiente* para un parámetro $\theta$ cuando una vez 
conocido no puede obtenerse más información de sobre $\theta$ de los datos;
esto es:

 \[\Pr(\theta| t,x) = \Pr(\theta|t)\]

***** Definición equivalente
De forma equivalente, es *suficiente* si la distribución condicionada 
al estadístico es independiente del parámetro $\theta$:

 \[\Pr(x|t,\theta) = \Pr(x|t)\]

**** Teorema de factorización de Fisher-Neyman
*Teorema de factorización de Fisher-Neyman*. $T$ es suficiente para $\theta$
ssi existen funciones no negativas $g$,$h$ tales que:

\[f_\theta(x) = h(x)g_\theta(T(x))\]

Donde $g_\theta$ sólo depende de $x$ a través de $T$ y $h$ no depende de $\theta$.

***** TODO Demostración
**** Propiedades de los estadísticos suficientes
*Propiedades de los estadísticos suficientes*.

  1. Si $T$ es suficiente para $\{P_\theta \mid \theta \in \Theta\}$, lo es para $\{P_\theta \mid \theta \in \Theta' \subset \Theta\}$.
  2. Si $T$ es suficiente y $T = h(U)$, $U$ es suficiente.
  3. Toda transformación biunívoca de suficiente es suficiente.

***** TODO Demostración
**** Estadístico completo
Un estadístico es *completo* cuando para cualquier función medible $g$,
se tiene:

\[ E_\theta [g(T)] = 0 \; \forall\theta\in\Theta \quad \Rightarrow \quad
    P_\theta(g(T) = 0) = 1\; \forall\theta\in\Theta\]

*** Suficiencia y completitud en familias exponenciales
**** Familia exponencial k-paramétrica
Una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramétrica si:

 1. $\Theta$ es intervalo de $\mathbb{R}^k$.
 2. Los valores de la variable no dependen de $\theta$, esto es:
    $\{{ x \mid f_{\theta}(x) > 0 \} = \{{ x \mid f_{\theta'}(x) > 0 \}$ para cualesquiera $\theta,\theta' \in \Theta$.
 3. La familia es de la forma:

    \[f_\theta(x) = exp\left\{\sum_{h=1}^k {Q_h(\theta) T_h(x) + S(x) + D(\theta)}\right\}\]

**** Teorema de suficiencia y complitud
Si una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramétrica, cualquier muestra
aleatoria simple también lo es:

\[
f^n_\theta(x_1,\dots,x_n) = 
exp\left\{
\sum^k_{h=1} Q_h(\theta) \left(
\sum^n_{i=1} T_h(x_i)
\right) +
\sum^n_{i=1} S(x_i) + nD(\theta)
\right\}
\]

Teniéndose además:

 1. $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ estadístico *suficiente* para $\theta$.
 2. Para $k \leq n$, cuando $(Q_1(\Theta), \dots Q_k(\Theta))$ contiene un abierto; entonces
    $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ es *completo*.

***** TODO Demostración
** 4. Estimación puntual
*** Planteamiento del problema de estimación
**** Estimador puntual
Un *estimador puntual* de $\theta$ es un estadístico $T$ tomando valores en el 
dominio del parámetro, $\Theta$.

**** Función de pérdida y de riesgo
La *función de pérdida*, $L(\theta,T)$, nos dice la pérdida asociada a estimar 
un parámetro si su verdadero valor es otro.

**** Función de riesgo
La *función de riesgo* es la que asocia a cada valor del parámetro la 
pérdida media asociada al estimador.

\[ R^L_T(\theta) = E_\theta [L(\theta,T)]\]

**** Estimador óptimo
El *estimador óptimo*, $T$, dada una función de pérdida, es el que minimiza 
uniformemente la función de riesgo:

\[ R^L_T(\theta) \leq R^L_{T''}(\theta),\quad \forall \theta \in \Theta,\; \forall T''\]

*** Estimación de menor error cuadrático
**** Función de pérdida cuadrática
La función de pérdida cuadrática, ${\cal L}(\theta, t) = (t - \theta)^2$, hace a la función de 
riesgo de un estimador su error cuadrático medio:

\[R^L_T(\theta) = E_\theta[(T - \theta)^2]\]

Nótese que en el caso de $E[T] = \theta$, se tiene $R^L_T(\theta) = Var_\theta[T]$.

*** Estimación insesgada de mínima varianza
**** Estimador insesgado
Un estimador $T$ de $g(\theta)$, es *insesgado* o *centrado* si:

 $E_\theta[T] = g(\theta)$

**** UMVUE: Estimador insesgado uniformemente de mínima varianza
Un estimador $T$ insesgado y de segundo orden es *UMVUE* para $g(\theta)$ si para 
cualquier otro estimador insesgado $T'$ se tiene que:

\[ Var_\theta[T] \leq Var_\theta[T']\]

***** De segundo orden
# ¿Seguro? No parece estar escrito en ningún sitio.
Lo llamamos de segundo orden cuando existe la varianza.

**** Propiedades del UMVUE
El estimador UMVUE cumple:

 - Unicidad: El UMVUE de cualquier función paramétrica, si existe, es único.
 - Linealidad: Si $T,Q$ son UMVUE para $g,h$; $aT+bQ$ es UMVUE para $ag+bh$.

***** Unicidad
Si existieran dos UMVUE con $Var(T) = Var(T')$, tendríamos:

\[\begin{aligned}
Var\left(\frac{1}{2}(T+T')\right) &= 
\frac{1}{4}
\left(
Var(T) + Var(T') + 2cov(T,T')
\right) \\& \leq
\frac{1}{4}
\left(
Var(T) + Var(T') + 2\sqrt{Var(T)Var(T')}
\right) \\& = Var(T)
\end{aligned}\]

La igualdad se da por ser UMVUE, y entonces, $cov(T,T') = Var(T)$.
De aquí $cov(T-T',T-T') = 0$, haciendo constante la diferencia entre los
dos. La diferencia entre ellos debe ser constantemente $0$ por ser ambos 
insesgados.

***** TODO Linealidad
**** Teorema de Raó-Blackwell
Si $T$ es suficiente para $\theta$ y $S$ es un estimador insesgado de $g(\theta)$ de 
segundo orden:

  - $E[S \mid T]$ es estimador insesgado de $g(\theta)$ de segundo orden.
  - $Var_\theta[E[S \mid T]] \leq Var_\theta[S]$

Es decir, $E[S \mid T]$ será normalmente mejor estimador y nunca peor que $S$.

***** Demostración
Sabemos $E[S|T] = E[S] = \theta$ por la [[*Ley de esperanza total][ley de esperanza total]]. La desigualdad
entre varianzas la vemos como:

\[\begin{aligned}
E\Big[(E[S|T] - \theta)^2 \Big] &= 
E\Big[E[S-\theta | T]^2 \Big] \leq
E\Big[E[(S-\theta)^2|T] \Big] = E\Big[(S-\theta)^2\Big]
\end{aligned}\]

Donde volvemos a usar la ley de esperanza total. La desigualdad viene
de que la varianza es positiva, o de la desigualdad de Jensen para el
cuadrado.

**** Teorema de Lehmann-Scheffé
Para $T$ suficiente y completo para $\theta$; si $g(\theta)$ admite un estimador insesgado 
de segundo orden $S$, entonces existe el UMVUE de $g(\theta)$ y está dado por:

\[ \mathbb{E} [S \mid T]\]

De otra forma, un estimador insesgado que es función de estimador completo
y suficiente es el UMVUE.

***** Demostración
Por [[*Teorema de Raó-Blackwell][Raó-Blackwell]], sabemos que es un estimador insesgado; y que, dado
cualquier otro estimador insesgado $Q$, tenemos que:

\[ Var[E[Q|T]] \leq Var[Q]\]

Ahora bien, dado otro, tendríamos:

\[
E\Big[ E[S|T] - E[Q|T] \Big] = 0
\]

Y como $T$ es completo y ambos son dependientes de $T$, eso implica que:

\[P\Big(
E[S|T] - E[Q|T] = 0
\Big) = 1\]

Por lo tanto, ambos son el UMVUE.

*** Estimación eficiente
**** Condiciones de regularidad de Fréchet-Cramer-Rao
Una familia de distribuciones es *regular en el sentido de 
Fréchet-Cramer-Rao* si cumple que, siendo $\{P_\theta \; \theta\in\Theta\}$:

 - $\Theta$ es intervalo abierto de $\mathbb{R}$.
 - $\forall \theta,\theta'\in\Theta : \{x \mid f_\theta(x) > 0\} = \{x \mid f_{\theta'}(x) > 0\}$
 - Tenemos $f_\theta(x)$ derivable respecto a $\theta$ para todo $x\in\chi$ con:

   \[ \int_\chi \frac{d f_\theta(x)}{d\theta} dx = 
   \frac{d}{d\theta} \int_\chi f_\theta(x) dx = 
   0, \quad \forall \theta\in\Theta\]
   
   O, cuando la distribución es discreta:

   \[\sum_\chi \frac{d f_\theta(x)}{d\theta} = \frac{d}{d\theta}\sum_\chi f_\theta(x) = 0\]

**** Función de información de Fisher
Si $\{P_\theta : \theta \in \Theta\}$ es regular, definimos la *función de información* asociada
a $X$ como:

\[I_X(\theta) = E_\theta\left[\left( \frac{d}{d\theta} \ln(f_\theta(X))
\right)^2\right]\]

Y la función de información asociada a una muestra como:

\[
\[I_{X_1,\dots,X_n}(\theta) = 
E_\theta\left[\left( \frac{d}{d\theta}\ln(f_\theta(X_1,\dots,X_n))
\right)^2\right]\]

**** Propiedades de la función de información
La función de información tiene como propiedades:

  1. $I_X(\theta) \geq 0$.
  2. En el caso $I_X(\theta) = 0$, $f_\theta(X)$ no depende de $\theta$.
  3. \[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = 0\].
  4. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = I_X(\theta) \].
  5. \[E_\theta \left[\frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) \right] = 0\].
  6. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = I_{X_1,\dots,X_n}(\theta) \].
  7. Aditividad, $I_{X_1,\dots,X_n}(\theta) = nI_X(\theta)$.
 
***** TODO Demostración

**** Estadístico regular
Un estadístico $T$ es regular en el sentido de Fréchet-Cramer-Raó, si
siendo una distribución discreta:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\sum_{x \in \chi^n} T(x)f_\theta(x) \\&= 
\sum_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x)
\end{aligned}\]

O, siendo una distribución continua:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\int_{x \in \chi^n} T(x)f^n_\theta(x) \;dx \\&= 
\int_{x \in \chi^n} T(x) \frac{d}{d\theta} f^n_\theta(x) \;dx
\end{aligned}\]

**** Cota de Fréchet-Cramer-Raó
Si $\{P_\theta \mid \theta \in \Theta\}$ es *regular*, la función de información se 
acota $0 < I_X(\theta) < \infty$, y $T$ es un estadístico regular, de segundo orden e
insesgado en una función derivable $g(\theta)$, se tiene:

  1. \[Var_\theta[T] \geq \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]
  2. Para todo $\theta \in \Theta$ tal que $g'(\theta) \neq 0$:

     \[Var_\theta[T] = \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]
     
     ssi existe $a(\theta) \neq 0$ tal que:

     \[P_\theta\left(
     \frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = 
     a(\theta)[T(X_1,\dots,X_n) - g(\theta)]
     \right) = 1\]

***** TODO Demostración
**** Estimador eficiente
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con la función de información acotada 
$0 < I_X(\theta) < \infty$ y $g(\theta)$ función paramétrica derivable. Un estimador $T$ de
$g(\theta)$ es eficiente si es insesgado, regular, y su varianza alcanza la
cota de Fréchet-Cramer-Raó en todo punto:

\[Var_\theta[T] = \frac{(g'(\theta))^2}{I_{X_1,\dots,X_n}(\theta)},
\qquad \forall\theta \in \Theta\]

**** Caracterización de estimadores eficientes
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con $0 < I_X(\theta) < \infty$ y $g(\theta)$ función paramétrica
derivable y no constante. Un estimador $T$ es eficiente ssi existe un $a(\theta)$
cumpliendo:

  1. \[P_\theta\left(\frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = a(\theta)[T(X_1,\dots,X_n) - g(\theta)]\right) = 1\]
  2. \[I_{X_1,\dots,X_n}(\theta) = a(\theta)g'(\theta)\]

***** Demostración
*** Estimación de máxima verosimilitud
**** Función de verosimilitud
Para cada realización muestral se define la función de verosimilitud
de la realización, $L_{x_1,\dots,x_n} : \Theta \longrightarrow \mathbb{R}^+_0$, como:

\[L(\theta) = f_\theta^n(x_1,\dots,x_n)\]

**** Estimador de máxima verosimilitud
Tenemos $\hat\theta$ estimador de máxima verosimilitud de $\theta$ cuando la estimación
asociada a cada realización muestral maximiza la verosimilitud:

\[L_{x_1,\dots,x_n}\left(\hat\theta(x_1,\dots,x_n)\right)
= \max_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)\]

**** Relación con estadísticos suficientes
Si $\{P_\theta \mid \theta \in \Theta\}$ admite estadístico suficiente $T$, entonces $\hat\theta$ es función
de $T$.

***** TODO Demostración
**** Relación con estimadores eficientes
Si $T$ es estimador eficiente de $\theta$, entonces $T$ es el único estimador 
máximo verosímil de $\theta$.

***** TODO Demostración

**** Función de verosimilitud de una función paramétrica
Se define la función de verosimilitud de $g : \Theta \longrightarrow \Lambda$ asociada a
una realización, $M_{x_1,\dots,x_n} : \Theta \longrightarrow \mathbb{R}^+_0$, como:

\[M_{x_1,\dots,x_n}(\lambda)
= \sup_{\theta \in g^{-1}(\lambda)} L_{x_1,\dots,x_n}(\theta) \]

**** Estimador de máxima verosimilitud de una función paramétrica
Será $\hat\lambda$ estimador máximo verosímil de $\lambda$ cuando:

\[M_{x_1,\dots,x_n}(\hat\lambda(x_1,\dots,x_n)) 
= \max_{\lambda \in \Lambda} M_{x_1,\dots,x_n}(\lambda) \]

**** Teorema de invarianza de Zenha
Si $\hat\theta$ es estimador máximo verosímil de $\theta$, entonces $g(\hat\theta)$ es estimador
máximo verosímil de $g(\theta)$.

***** TODO Demostración

*** Método de los momentos
**** Descripción
El estimador máximo verosímil de una función dependiente en los momentos
poblacionales es el mismo dependiendo en los momentos muestrales.

\[g(\theta) = h(m_{\theta,1},\dots,m_{\theta,k}) 
\quad\Rightarrow\quad
\widehat{g(\theta)}(X_1,\dots,X_n) = h(A_1,\dots,A_k)\]

***** Momentos poblacionales
Definimos los momentos poblacionales como:

\[m_{\theta,j} = E_\theta[X^j]\]

***** Momentos muestrales
Definimos los momentos muestrales como:

\[A_j = \frac{1}{n}\sum_{i=1}^n X^j_i\]

***** TODO Demostración
*** Método de mínimos cuadrados
**** Descripción
Si $X_i$ son las observaciones aleatorias de una magnitud $\varphi(t,\theta)$ con
errores $\varepsilon_i$; es decir:

\[X_i = \varphi(t_i,\theta) + \varepsilon_i\]

Entonces el estimador de mínimos cuadrados de $\theta$ es el que minimice
la suma de cuadrados de los errores:

\[\sum^n_{i=1}(X_i - \varphi(t_i,\theta))^2\]

** 5. Estimación por intervalos de confianza
*** Definiciones y métodos de construcción
**** Intervalo de confianza
Para $X \leadsto P_\theta$, un intervalo de confianza $\alpha$ para $\theta$ es un intervalo 
aleatorio $(I_1,I_2)$ tal que para cualquier $\theta \in \Theta$:

\[P_\theta\left(
I_1(X_1,\dots,X_n) \leq \theta \leq I_2(X_1,\dots,X_n)
\right)
\geq 1 - \alpha\]

**** Intervalo de confianza de menor longitud esperada uniformemente
Un interavlo $(I_1,I_2)$ es el de menor longitud esperada uniformemente 
si para cualquier otro $(I_1',I_2')$ al mismo nivel, se tiene:

\[
E_\theta[I_2(X_1,\dots,X_n) - I_1(X_1,\dots,X_n)]
\leq
E_\theta[I_2'(X_1,\dots,X_n) - I_1'(X_1,\dots,X_n)]
\]

**** Intervalos obtenidos mediante desigualdad de Chevychev
Si $T$ es estimador insesgado de $\theta$ con varianza uniformemente acotada:

  - $E_\theta[T(X_1,\dots,X_n)] = \theta$
  - $Var_\theta[T(X_1,\dots,X_n)] \leq c$

Por lo que por Chevychev tenemos, dado $k>0$, un intervalo de confianza
para $\theta$ al nivel de confianza $1 - c/k^2$:

\[(T(X_1,\dots,X_n) - k,\  T(X_1,\dots,X_n) + k)\]

***** TODO Demostración

**** Pivote para un parámetro
Un pivote es una función $T(X_1,\dots,X_n,\theta)$ tal que fijado cualquier $\theta$,
$T(X_1,\dots,X_n,\theta)$ es una variable con distribución independiente de $\theta$.

**** Intervalos obtenidos mediante el método pivotal
Dado un pivote estrictamente monótono respecto a $\theta$, y dos valores $\lambda_1,\lambda_2$,
tales que para cualquier $\theta$:

\[P_\theta(\lambda_1 < T(X_1,\dots,X_n) < \lambda_2) \geq 1 - \alpha\]

Tomamos las soluciones $\hat\theta_1, \hat\theta_2$, cumpliendo $T(X_1,\dots,\hat\theta_1) = \lambda_1$ y
$T(X_1,\dots,\hat\theta_2) = \lambda_2$; y ellas forman un intervalo de confianza:

  - $P_\theta(\hat\theta_1 < \theta < \hat\theta_2) \geq 1 - \alpha$, para $T$ creciente.
  - $P_\theta(\hat\theta_2 < \theta < \hat\theta_1) \geq 1 - \alpha$, para $T$ decreciente.

***** TODO Demostración

**** Un pivote en distribuciones continuas
Si $X$ es continua con $F_\theta$ función de distribución, un pivote es:

\[ T(X_1,\dots,X_n,\theta) = -2 \sum_{i=1}^n \ln F_\theta(X_i) \leadsto \chi^2(2n)\]

***** TODO Demostración
**** Un pivote dado un estadístico
Sea $S$ un estadístico de distribución continua con $F^S_\theta$ función de 
distribución. Un pivote es:

\[T(X_1,\dots,X_n,\theta) = F^S_\theta(S(X_1,\dots,X_n)) \leadsto U(0,1)\]

*** Ejemplos de intervalos de confianza
**** Intervalo para la media de una normal con varianza conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2_0)$. Un intervalo de confianza para $\mu$ de menor longitud
media uniforme a nivel de confianza $1-\alpha$ será:

\[\left(
\overline{X}-z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}},
\overline{X}+z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}}
\right)\]

Siendo $z_{\alpha/2}$ el que cumple, con $Z \leadsto {\cal N}(0,1)$, que \[P\left(Z \leq z_{\alpha/2}\right) = \alpha/2\].

***** Pivote
Usamos como pivote a la normalizada:

\[T(X_1,\dots,X_n,\mu) = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \leadsto {\cal N}(0,1)\]

***** Intervalos candidatos
Usando el pivote, tenemos el siguiente candidato a intervalo de
confianza:

\[
1 - \alpha > 
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{\sigma_0 / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}}
\right)
\]

Debiendo tenerse que, si $\Phi$ es la función de distribución de la
normal $\Phi(\lambda_2)-\Phi(\lambda_1) = 1 - \alpha$.

***** Longitud media
Buscamos el que minimice la longitud media:

\[E_\mu \left[
\left( \overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}} \right) -
\left( \overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} \right)
\right] 
= (\lambda_2-\lambda_1)\frac{\sigma_0}{\sqrt{n}}\]

Por lo que tratamos de minimizar $(\lambda_2-\lambda_1)$.

***** Minimización
Usamos multiplicadores de Lagrange para definir:

\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]

Calculando las derivadas parciales tenemos:

\[\begin{aligned}
-1-\lambda\Phi'(\lambda_1) &= 0\\
1 + \lambda\Phi'(\lambda_2) &= 0
\end{aligned}
\]

Luego debe tenerse $\Phi'(\lambda_1) = \Phi'(\lambda_2)$. Sabiendo que $\Phi'$ es la función
de distribución de la normal, tenemos $\lambda_1 = \pm \lambda_2$. Como deben ser
distintos para cumplir la restricción, tenemos $\lambda_1 = -\lambda_2$.

***** Conclusión
La restricción nos fuerza a $\Phi(\lambda_2) - \Phi(-\lambda_2) = 1 - \alpha$, luego estamos
buscando el $z_{\alpha/2}$ que cumple, para una normalizada $Z$, $P(Z > z_{\alpha/2}) = \alpha/2$.

**** TODO Intervalo para la media de una normal con varianza desconocida
**** TODO Intervalo para la varianza de una normal con media conocida
**** TODO Intervalo para la varianza de una normal con varianza conocida
**** TODO Intervalo para la diferencia de medias de normales de varianza dada
**** TODO Intervalo para la diferencia de medias de normales de varianza igual
**** TODO Intervalo para el cociente de varianzas de normales de media dada
** 6. Contraste de hipótesis
*** Planteamiento del problema
**** Problema de contraste de hipótesis
Dada $(X_1,\dots,X_n)$ una muestra aleatoria simple de $X \leadsto P_\theta$, para
$\theta \in \Theta_0 \cup \Theta_1$, llamamos:

 - *Hipótesis nula*: $H_0 : \theta \in \Theta_0$
 - *Hipótesis alternativa*: $H_1 : \theta \in \Theta_1$

a dos hipótesis posibles.

**** Test de hipótesis
El *test de hipótesis* es un estadístico $\varphi$ tomando valores en $[0,1]$, que 
da la posibilidad de rechazar $H_0$ dada una realización muestral. Se 
llama:

 - *Test no aleatorizado*, si toma valores $0,1$.
 - *Test aleatorizado*, si toma valor distinto de $0,1$.

**** Tipos de errores de un test de hipótesis
Hay dos tipos de erorres:

 - *Error de tipo 1*: Rechazar $H_0$ siendo cierta. Falso negativo.
 - *Error de tipo 2*: Aceptar $H_0$ siendo falsa. Falso positivo.

**** Función de potencia de un test
Dado un test $\varphi$, su función de potencia $\beta_\varphi : \Theta \longrightarrow [0,1]$ se define:

\[\beta_\varphi(\theta) = E_\theta[\varphi(X_1,\dots,X_n)]\]

Que es la probabilidad media de rechazar $H_0$ bajo $P_\theta$.

**** Tamaño del test
El tamaño del test es $\sup_{\theta \in \Theta_0} \beta_\varphi(\theta)$, la máxima probabilidad media de
cometer un error de tipo 1.

**** Nivel de significación de un test
Un test $\varphi$ tiene nivel de significación $\alpha$ si su tamaño es menor o igual
que $\alpha$. Es decir,

\[
\forall \theta \in \Theta_0, \quad
\beta_\varphi(\theta) =
E_\theta[\varphi(X_1,\dots,X_n)] \leq
\alpha
\]

**** Test uniformemente más potente
Un test con nivel de significación $\alpha$ es uniformemente más potente a 
dicho nivel si para cualquier otro test $\varphi'$ con nivel de significación
$\alpha$, se tiene:

\[\beta_{\varphi'}(\theta) \leq \beta_\varphi(\theta)
\quad \forall \theta \in \Theta_1\]

*** Lema de Neyman-Pearson
**** El problema de contraste
Fijado un nivel de significación, encontrar el test uniformemente más
potente a dicho nivel.

**** Lema de Neyman-Pearson
Sea $X \longrightarrow \{P_{\theta_0}, P_{\theta_1}\}$ y $(X_1,\dots,X_n)$ una muestra aleatoria simple
con funciones de densidad $f_0,f_1$. Consideramos el problema de contraste 
con $H_0 : \theta = \theta_0$ y $H_1 : \theta = \theta_1$.

  1. Cualquier test de la forma:

     \[
     \varphi(X_1,\dots,X_n) = 
     \threepartdef
     {1}
     {f_1^n(X_1,\dots,X_n) > kf_0^n(X_1,\dots,X_n)}
     {\gamma(X_1,\dots,X_n)}
     {f_1^n(X_1,\dots,X_n) = kf_0^n(X_1,\dots,X_n)}
     {0}
     {f_1^n(X_1,\dots,X_n) < kf_0^n(X_1,\dots,X_n)}
     \]
     
     con $k \in \mathbb{R}^+_0$ y $\gamma(X_1,\dots,X_n) \in [0,1]$, es de máxima potencia entre todos
     los de nivel de significación $\alpha = E_{\theta_0}[\varphi(X_1,\dots,X_n)]$, su tamaño.
     
  2. Para todo $\alpha \in (0,1]$ existe un test de la forma anterior con
     $\gamma(X_1,\dots,X_n) = \gamma$ constante y tamaño $\alpha$.

  3. Si $\varphi'$ es de máxima potencia al nivel de significación
     $\alpha = E_{\theta_0}[\varphi'(X_1,\dots,X_n)]$, entonces $\varphi'(X_1,\dots,X_n)$ es de la forma anterior
     con probabilidad 1 bajo $P_{\theta_0}$ y $P_{\theta_1}$.

  4. El test de máxima potencia entre todos los de nivel de significación
     0 es:
     
     \[
     \varphi_0(X_1,\dots,X_n) = \twopartdef
     {1}{f^n_0(X_1,\dots,X_n) = 0}
     {0}{f^n_0(X_1,\dots,X_n) > 0}
     \]

***** TODO Demostración

*** Test de la razón de verosimilitudes
**** Test de la razón de verosimilitudes
Sea $(X_1,\dots,X_n) \in \chi^n$ una muestra aleatoria simple de $X \leadsto \{P_\theta \mid \theta \in \Theta_0 \cup \Theta_1\}$.
El test de razón de verosimilitudes para el problema de contraste con
$H_0 : \theta \in \Theta_0$ y $H_1 : \theta \in \Theta_1$; se define como:

\[
\varphi(X_1,\dots,X_n) = \twopartdef
{1}{\lambda(X_1,\dots,X_n) < c}
{0}{\lambda(X_1,\dots,X_n) \geq c}
\]

donde se define:

\[\lambda(x_1,\dots,x_n) = \frac
{\sup_{\theta \in \Theta_0} L_{x_1,\dots,x_n}(\theta)}
{\sup_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)}
\]

siendo $L$ la función de verosimilitud y $c$ una constante que se determina
imponiendo el tamaño o nivel de significación requerido.

*** TODO Dualidad entre tests de hipótesis y regiones de confianza
** 7. Teoría general de modelos lineales
*** Modelo lineal general y modelo de Gauss Markov
**** Modelo lineal general
El modelo general lineal queda descrito por:

\[\mathbf{Y = X\beta + \varepsilon}\]

***** Vector observable
$Y = (Y_1,\dots,Y_n)$ es un vector aleatorio observable.

***** Matriz de diseño
Una matriz conocida $X$ de dimensión $n \times k$, cuyo rango determina el 
rango del modelo.

***** Vector de efectos
Un vector desconocido $\beta = (\beta_1,\dots,\beta_k)$.

***** Vector de errores
Un vector aleatorio no observable $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)$ representando el 
error entre $Y$ y $X\beta$.

**** Modelo de Gauss-Markov
Modelo lineal donde las componentes del vector de errores son variables
aleatorias de segundo orden, centradas, homocedásticas (igual varianza)
e incorreladas:

  - $E[\varepsilon_i] = 0$
  - $E[\varepsilon_i^2] = \sigma^2$
  - $E[\varepsilon_i\varepsilon_j] = 0$

***** Enunciado vectorial
Las condiciones sobre el vector de errores equivalen a exigir:

  - $E[\varepsilon] = 0$
  - $E[\varepsilon\varepsilon^T] = \sigma^2 I_{n \times n}$

**** Objetivo del modelo
Inferir $\beta$ y $\sigma^2$ a partir de observaciones del vector $Y$.

*** Estimación de mínimos cuadrados del vector de efectos
**** Modelo
Partimos del modelo de Gauss-Markov, y queremos minimizar la suma de
cuadrados de los errores:

\[S^2(\beta) = 
\sum^n_{i=1} \varepsilon^2_i =
\|Y - X\beta\|^2\]

***** Minimización
Para minimizarlo, calculamos la derivada:

\[\frac{\partial}{\partial \beta_h} S^2(\beta) = 
-2 \sum^n_{i=1} \left(
Y_i - \sum^k_{j=1} x_{ij}\beta_j
\right) x_{ih} = 0
\]

Y obtenemos las ecuaciones normales siguientes:

\[
\sum^n_{i=1} Y_i x_{ih} = \sum^n_{i=1}\sum^k_{j=1} x_{ij}x_{ih}\beta_j
\]

Que pueden expresarse matricialmente como:

\[X^TY = (X^TX)\beta\]

**** Estimador de mínimos cuadrados de beta
Llamamos $\widehat\beta$ al estimador de mínimos cuadrados de $\beta$.

  - Existencia: existe al menos un estimador de mínimos cuadrados de $\beta$.
  - Unicidad: garantizada cuando el modelo es de rango máximo por tenerse
    la solución \[\widehat\beta(Y) = (X^TX)^{-1}X^TY\].

**** Función lineal estimable
Una $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable si admite un estimador insesgado,
lineal en las componentes de $Y$. Es decir:

\[\exists \widehat\psi(Y) = c_1Y_1 + \dots + c_nY_n\]

tal que $E[\widehat\psi(Y)] = \psi(\beta)$.

**** Teorema de Gauss-Markov
Si $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable, admite un único estimador lineal
insesgado uniformemente de mínima varianza en la clase de estimadores
lineales insesgados. Dicho estimador es:

\[\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y) \]

Donde $\widehat{\beta}(Y)$ es un estimador de mínimos cuadrados de $\beta$.

***** TODO Demostración

**** Propiedades del estimador de mínimos cuadrados en modelos de rango máximo
Sea $\widehat\beta(Y) = (X^TX)^{-1}X^TY$ el estimador de mínimos cuadrados en un modelo 
de rango máximo.

  1. $\widehat\beta_j(Y)$ es el estimador lineal insesgado de mínima varianza de $\beta_j$.
  2. Las varianzas y covarianzas vienen dadas: $Cov(\widehat\beta(Y)) = \sigma^2(X^TX)^{-1}$.
  3. Toda función lineal de las componentes de $\beta$ es estimable con 
     estimador lineal insesgado de mínima varianza
     $\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y)$.

***** TODO Demostración

* Topología II
** 1. El grupo fundamental
*** 1. Espacios conexos por arcos
**** Arcoconexión
*Arcoconexión*. Las curvas definen relación de equivalencia cuando unen dos puntos.

 \[\exists f\ \text{arco}: f(0) = x, f(1) = y \Rightarrow x \sim y\]

Cada componente de esta partición es una *componente arcoconexa*. Un espacio es
*arcoconexo* cuando tiene una sola componente.

**** Operaciones en arcos y arcoconexión como equivalencia
Las propiedades de la relación de equivalencia se cumplen por:

 - 1. *Reflexividad*. Arco constante, $f(t) = x$, lleva a $x \sim x$.
 - 2. *Simetría*. Arco inverso, $\widetilde f(t) = f(1-t)$, lleva a $x\sim y \Rightarrow y \sim x$.
 - 3. *Transitividad*. Composición de arcos $f \ast g$.

La composición de arcos se define como:

\[f \ast g = \twopartdef{f(2t)}{t \leq \frac{1}{2}}{g(2t-1)}{t \geq \frac{1}{2}}\]

**** Arcoconexión y conexión
Un espacio *arcoconexo es conexo*. Un espacio *conexo localmente arcoconexo 
es arcoconexo*, que es conexo y donde todo punto posee un entorno arcoconexo.

***** Demostración
Dado un punto en un espacio arcoconexo, los caminos a los demás serán conexos y
compartirán un punto, luego su unión será conexa. Por otro lado, en un localmente
arcoconexo, todo punto está dentro de un abierto (tiene un entorno arcoconexo),
luego cada componente arcoconexa será abierta y si hubiera varias, contravendría
la conexión.

***** Contraejemplo del recíproco
Hay un contraejemplo de espacio conexo no arcoconexo en el
[[https://es.wikipedia.org/wiki/Seno_del_top%25C3%25B3logo][seno del topólogo]].

*** 2. Grupo fundamental
**** Homotopía de lazos
Un arco con $f(0) = f(1) = x$ es un *lazo* alrededor de $x$. Dos lazos son *homotópicos*
y escribimos $f \sim g$ cuando $\exists H: [0,1] \times [0,1] \longrightarrow X$, cumpliendo:

  - $H$ continua.
  - $H(t,0) = f(t)$
  - $H(t,1) = g(t)$
  - $H(0,s) = H(1,s) = x$

lo escribimos como $H : f \simeq g$.

**** Clases de homotopía
La homotopía es una relación de equivalencia entre lazos:

 - *Reflexividad*. Definiendo $H(t,s) = f(t)$, tenemos $H : f \simeq f$.
 - *Simetría*. Dada $G : g \simeq f$, definimos $H(t,s) = G(t,1-s)$, tenemos $H : f \simeq g$.
 - *Transitividad*. Dadas $F : f \simeq g$, $G : g \simeq h$, definimos
    \[H(t,s) = \twopartdef{F(t,2s)}{0\leq s \leq 1/2}{G(t,2s-1)}{1/2 \leq s \leq 1}\]
    Y tenemos $H : f \simeq h$.

Llamamos *clase de homotopía* $[f]$ a la clase de equivalencia de un lazo $f$.

**** Producto de clases de homotopía
El producto de lazos está bien definido entre las clases de homotopía. Sean 
$H : f_1 \simeq f_2$ y $G: g_1 \simeq g_2$; entonces definimos $F: f_1 \ast g_1 \simeq f_2 \ast g_2$ como:

\[ F(t,s) = \twopartdef{H(2t,s)}{0\leq t \leq 1/2}{G(2t-1,s)}{1/2 \leq t \leq 1}\]

**** El grupo fundamental
Comprobamos que las clases de homotopía sobre un $x$ forman un grupo con el producto:

\[\Pi(X,x) = \{[f] \mid f \text{ lazo alrededor de } x\}\]

***** Asociatividad
Para la *asociatividad*, definimos $H : (f \ast g) \ast h \simeq f \ast (g \ast h)$:

\[H(t,s) = \threepartdef
{f\left(t \frac{4}{1+s}\right)}{0\leq t\leq \frac{1+s}{4}}
{g\left(4t-1-s\right)}{\frac{1+s}{4}\leq t \leq \frac{2+s}{4}}
{h\left(t\frac{4}{2-s}-1\right)}{\frac{2+s}{4}\leq t\leq 1}\]

***** Elemento neutro
Como *elemento neutro* tomaremos el lazo constante $f_x(t)=x$. Y definimos  
$H : f_x \ast f \simeq f$:

\[ H(t,s) = \twopartdef
{f(t\frac{2t}{1+s})}{0\leq t\leq \frac{1+s}{2}}
{x}{\frac{1+s}{2}\leq t \leq 1}\]

***** Elemento inverso
Como *elemento inverso* tendremos el lazo $\hat{f}(t) = f(1-t)$. Y definimos 
$H : f \ast \hat{f} \simeq f$:

\[H(t,s) = \threepartdef
{f(2t)}{0\leq t\leq\frac{1-s}{2}}
{f(1-s)}{\frac{1-s}{2}\leq t\leq \frac{1+s}{2}}
{\hat{f}(2t-1)}{\frac{1+s}{2}\leq t\leq 1}\]

**** El grupo fundamental como funtor
Sea $\Phi : (X,x) \longrightarrow (Y,y)$ /continua/, entonces tenemos una aplicación bien definida 
por:

\[\Phi_\ast ([f]) = [\Phi \circ f] \]

Que es un homomorfismo de grupos. Podemos comprobar fácilmente que $Id_\ast = Id$ y que
$(\Psi\circ\Phi)_\ast = \Psi_\ast\circ\Phi_\ast$. Es decir, el grupo fundamental es un funtor de la categoría de 
los espacios topológicos punteados a la de los grupos.

***** Demostración
Sean $H: f \simeq g$, veamos que $\Phi\circ H: \Phi f \simeq \Phi g$. Tenemos que:

  - $\Phi\circ H$ continua
  - $\Phi\circ H(t,0) = \Phi\circ f(t)$
  - $\Phi\circ H(t,1) = \Phi\circ g(t)$
  - $\Phi \circ H(0,s) = \Phi \circ H(1,s) = y$

El que respeta el producto se tiene por $\Phi\circ (f \ast g) = \Phi\circ f \ast \Phi\circ g$.

**** Homotopía de arcos
Decimos que dos arcos cumpliendo $f(0)=g(0)$, $f(1)=g(1)$ son homotópicos cuando 
existe una función continua con características similares a la dada para lazos.

***** Propiedades
De demostración similar al caso de lazos:

  - Hay un arco simétrico $\tilde{f}$ tal que $f\ast \tilde{f} \simeq f_x$.
  - El producto es asociativo por homotopía $f \ast (g \ast h)\simeq (f\ast g)\ast h$-
    
**** Isomorfismo entre grupos fundamentales en distintos puntos
Sea un arco $\gamma : [0,1] \longrightarrow X$ cumpliendo $\gamma(0) = x$, $\gamma(1)=y$. Entonces se tiene un 
isomorfismo entre $\Pi(X,x)$ y $\Pi(X,y)$:

\[ F_\gamma([f]) = [\tilde\gamma\ast f\ast\gamma]\]

Por tanto, el grupo fundamental es el mismo en cualquier punto de la componente
arcoconexa.

***** Demostración
Para ver que está bien definido basta notar:

\[f \simeq g \Rightarrow 
\gamma\ast f\ast\tilde\gamma\simeq \gamma\ast g\ast\tilde\gamma\]

Que es homomorfismo se tiene viendo $F([f]\ast [g]) = F([f]) \ast F([g])$, y que es biyectivo
porque tiene inversa $G_\gamma([f]) = [\gamma\ast f\ast \tilde\gamma]$.

**** El grupo fundamental del producto
Dado el producto de dos espacios topológicos $X\times Y$ con proyecciones $\pi_1,\pi_2$. Tenemos
un isomorfismo entre grupos fundamentales:

\[F: \Pi(X\times Y, (x,y)) \longrightarrow \Pi(X,x)\times\Pi(Y,y)\]

Dado por:

\[F([f]) = (\pi_1([f]), \pi_2([f]))\]

*** 3. El grupo fundamental del círculo
**** Cálculo del grupo fundamental del círculo
Usaremos la teoría de recubridores que se probará luego.

Probamos que $\pi : \mathbb{R} \longrightarrow \mathbb{S}$ es un recubridor con $\pi(t) = e^{it}$; podemos tomar como entorno
fundamental de $p$ a $\mathbb{S}-\{p\}$, que tiene en la preimagen componentes arcoconexas 
homeomorfas a él. Siendo $t_0 \in \pi^{-1}(p)$:

\[V_m = (t_0 + 2\pi m, t_0 + 2\pi(m+1))\]

Definimos ahora el grado de un lazo en el círculo como:

\[\operatorname{deg}(f) = \frac{\hat{f}(1) - \hat{f}(0)}{2\pi} \]

Y comprobamos que está bien definido entre las clases de homotopía levantando
las homotopías y viendo que debe ser constante el punto final para que se proyecte
en un punto constante. Es decir $f \simeq g \Rightarrow \hat{f}(0) = \hat{g}(0)
\Rightarrow \hat{f}(1) = \hat{g}(1)$.

Como el levantamiento de la composición es la composición de levantamientos
y levantando de forma que $\hat{g}(0) = \hat{f}(1)$:

\[ \operatorname{deg}(f \ast g) =
   \frac{\widehat{f\ast g}(1) - \widehat{f\ast g}(0)}{2\pi} = 
   \frac{\hat{f}(1) - \hat{g}(0) + \hat{g}(1) - \hat{f}(0)}{2\pi} =
   \operatorname{deg}(f) + \operatorname{deg}(g) \]

Siendo un homomorfismo del grupo fundamental del círculo con $\mathbb{Z}$.

**** Teorema fundamental del álgebra
*Teorema fundamental del álgebra*. Todo polinomio con coeficientes
en $\mathbb{C}$ de grado $n$ tiene $n$ raíces en $\mathbb{C}$.

***** Demostración
Supongamos un $P$ que no tuviera raíz en $\mathbb{C}$.

Fijado un $r$, restringimos el polinomio desde la circunferencia de
radio $r$ a la circunferencia unidad, girándolo
además para que en $0$ valga $1$.

\[f_r(t) = \frac{|P(r)|}{P(r)} \frac{P(re^{2\pi it})}{|P(re^{2\pi it})|}\]

Tenemos una homotopía de este lazo al constante, definida como:

\[H(t,s) = f_{(1-s)r}(t)\]

Y por tanto, $deg(f_r) = 0$.

Por otro lado, sea ahora $R > 1, \sum |a_i|$, y tomemos $|z| = R$, tenemos
por un lado:

\[|z^n| 
 = R^n > R^{n-1}\left(\sum |a_i|\right) 
 \geq |a_1z^{n-1} + \dots + a_n|\]

Y por otro lado, si tomo un $t \in [0,1]$, puedo definir una familia
de polinomios $P_s(z) = z^n + t(a_1z^{n-1} + \dots + a_n) = 0$, que no tienen raíces
porque, por otro lado:

\[|z^n| = |t||a_1z^{n-1} + \dots + a_n| \leq |a_1z^{n-1} + \dots + a_n|\]

Luego esta familia de polinomios no tiene raíces en el círculo 
de radio $R$. Podemos ahora definir otra homotopía:

\[H(t,s) = \frac{P_s(Re^{2\pi it})}{|P_s(Re^{2\pi it})|}  \frac{|P_s(R)|}{P_s(R)}\]

Que es homotopía entre $e^{2\pi int}$ y $f_R(t)$. Luego $deg(f_R) = n$.

**** Lema al punto fijo de Brower
No existe aplicación continua $f : D^2\longrightarrow \mathbb{S}$ tal que $f|_\mathbb{S} = id$.

***** Demostración
Estamos buscando una aplicación cumpliendo:

\[ \begin{tikzcd}
\mathbb{S} \rar[hook]{i} & D^2 \rar{f} & \mathbb{S}
\end{tikzcd} \]

Aplicando el funtor obtendríamos:

\[ \begin{tikzcd}
\mathbb{Z} \rar[hook] & 0 \rar & \mathbb{Z}
\end{tikzcd} \]

Pero así es imposible obtener la identidad como composición.

**** Teorema del punto fijo de Brower
Toda función continua $f : D^2 \longrightarrow D^2$ tiene un punto fijo.

***** Demostración
Si no lo hubiera, para cada $x$ tomo $f(x)$ y la intersección de la recta que los une
con el círculo más cercana a $x$. Tengo una aplicación cuya restricción es la 
identidad.

**** Grupos topológicos
Un grupo topológico es un grupo en la categoría de espacios punteados. Esto es,
tal que la función producto y la función inverso son continuas.
**** Grupos topológicos y grupo fundamental
El producto en un grupo topológico respeta clases de homotopía:

\[ [f]\cdot [g] = [f\cdot g]\]

Y además, actúa sobre ellas igual que la composición:

\[ [f]\ast [g] = [f] \cdot [g]\]

*** 4. Tipo de homotopía. Equivalencias homotópicas
**** Aplicaciones homotópicas
Sean $F,G : X \longrightarrow Y$ continuas. Las decimos *homotópicas* si existe 
$H : X \times [0,1] \longrightarrow Y$ tal que $H(x,0) = F(x)$ y $H(x,1) = G(x)$. Lo notamos
por $H: F\simeq G$.

**** Grupo fundamental entre dos puntos
Entre cualesquiera puntos de dos funciones homotópicas $F(x_0)$, $G(x_0)$; tenemos un 
arco $\gamma = H(x_0,t)$. Se cumple que:

\[ \begin{tikzcd}
& \Pi(Y,F(x_0)) \arrow[leftrightarrow]{dd}{F_\gamma}\\
\Pi(X,x_0) \urar{F_\ast}\drar{G_\ast} \\
& \Pi(Y,G(x_0))
\end{tikzcd} \]

***** Demostración
Sea $f \in \Pi(X,x_0)$. Si defino la función $\hat{H}(t,s) = H(f(t),s)$ tengo una homotopía como
la siguiente:

\begin{tikzpicture}
\draw (A) -- node [above] {$G f$} ++ (-1, 0)
-- node [left]  {$\gamma$} ++ (0, -1)
-- node [below] {$F f$} ++ (1, 0)
-- node [right] {$\gamma$} ++ (0, 1);
\end{tikzpicture}

Rotándola obtengo:

\begin{tikzpicture}
\draw (A) -- node [above] {$G f \circ \gamma$} ++ (-1, 0)
-- node [left]  {$F(x_0)$} ++ (0, -1)
-- node [below] {$\gamma \circ F f$} ++ (1, 0)
-- node [right] {$G(x_0)$} ++ (0, 1);
\end{tikzpicture}

Lo que me da $[Gf \circ \gamma] = [\gamma \circ Ff]$, y por tanto $[Gf] = [\gamma \circ Ff \circ \tilde{\gamma}]$.

**** Equivalencia homotópica
Una *equivalencia homotópica* es una aplicación continua $F$, para la que
existe $G$ cumpliendo $F \circ G \simeq G \circ F \simeq Id$.

**** Propiedades de la equivalencia homotópica
Cumple:

1. Los homeomorfismos son equivalencias homotópicas.
2. La inversa de una equivalencia homotópica es equivalencia homotópica.
3. La composición de equivalencias homotópicas es equivalencia homotópica.

**** Conservación del grupo fundamental por equivalencia homotópica
Sea $F: X\longrightarrow Y$ equivalencia homotópica:

\[\forall x\in X : F_\ast : \Pi(X,x) \longrightarrow \Pi(Y,F(x))\]

Es un isomorfismo

***** Demostración
Por ser equivalencia homotópica tengo que $F \circ G \simeq Id$, luego se cumple:

\[ \begin{tikzcd}
& \Pi(X,F\circ G(x_0)) \arrow[leftrightarrow]{dd}{\cong}\\
\Pi(X,x_0) \urar{(F\circ G)_\ast}\drar{Id} \\
& \Pi(X,x_0)
\end{tikzcd} \]

Así, $(F\circ G)_\ast$, y de la misma forma $(G\circ F)_\ast$ son isomorfismos. Tenemos por tanto:

\[ \begin{tikzcd}
\Pi(X,x) \rar{F_\ast} \arrow[bend left=20]{rr}{\cong}
& \Pi(Y,F(x)) \rar{G_\ast} \dlar{\cong}
& \Pi(X,(G\circ F)(x)) \dlar{\cong}
\\
\Pi(Y,F(x)) \rar{F_\ast} \arrow[bend right=20]{rr}{\cong}
& \Pi(X,(G\circ F)(x)) \rar{F_\ast}
& \Pi(X,(G\circ F)(x))
\end{tikzcd} \]

Demostrando ambas filas que $F_\ast$ y $G_\ast$ son isomorfismos.

**** Lema a Borsuk-Ulam
No existe $F: \mathbb{S}^2 \longrightarrow \mathbb{S}^1$ continua respetando antípodas.

\[F(-x) = -F(x)\]

***** TODO Demostración
**** Teorema de Borsuk-Ulam
Sea $F:\mathbb{S}^2 \longrightarrow \mathbb{R}^2$ continua, entonces:

\[\exists x\in\mathbb{S}^2 : F(-x) = F(x)\]

***** Demostración
Supongamos que no se cumpliera, definimos:

\[G(x) = \frac{F(x)-F(-x)}{|F(x)-F(-x)|}\]

Y entonces $G$ sería continua respetando antípodas.

**** Teorema del sandwich de jamón
Sean $A,B\subset \mathbb{R}^2$ compactos y conexos. Existe una recta dividiendo a ambos en dos
trozos de igual área.

***** TODO Demostración
**** Espacio proyectivo
En $\mathbb{S}^n$ defino la ralación de equivalencia $p \sim q$ ssi $p = \pm q$. Definimos el 
espacio proyectivo como el cociente bajo esta relación:

\[\mathbb{RP}^n = \mathbb{S}^n}/\sim\]

*** 5. Teorema de Seifert-Van Kampen
**** Producto libre de grupos
Si definimos el *producto libre* de grupos en términos de palabras; podemos 
llamar *grupo libre sobre un conjunto de generadores* al producto libre del 
grupo libre generado por cada uno de ellos.

**** Subgrupo normal generado
El *subgrupo normal generado* por un subgrupo $B$ o por un conjunto de 
generadores es:

\[ \{g \cdot b \cdot g^{-1} \mid g\in G, b\in B\}\]

**** Producto libre amalgamado
Sean tres grupos $A$, $G_1$, $G_2$ y dos proyecciones de $A$ en ellos, llamadas 
$\Phi_1,\Phi_2$. Definimos el *producto libre amalgamado* como:

\[G_1 \ast_{A} G_2 =
\frac{G_1 \cdot G_2}{N} = \frac{G_1 \cdot G_2}{\{\Phi_1(a)=\Phi_2(a)\}}\]

Donde estamos dividiendo por $N$, el subgrupo normal generado por 
$\{\Phi_1(a)\Phi_2(a)^{-1} \mid a \in A\}$. Es decir, imponemos la relación $\Phi_1(a) = \Phi_2(a)$.

**** Teorema de Seifert-Van Kampen
Sea $X$ espacio topológico con $U,V$ abiertos arcoconexos no vacíos de $X$ tales 
que $X = U \cup V$ y $U\cap V$ son arcoconexos. Existe un isomorfismo:

\[\Theta : \Pi(U,x) \ast_{\Pi(U\cap V,x)} \Pi(V,x) \longrightarrow \Pi(X,x)\]

donde se amalgama usando $i_\ast$, $j_\ast$, homomorfismos dados por las inclusiones.

**** Seifert-Van Kampen para intersección simplemente conexa
En las condiciones del teorema, con $U\cap V$ simplemente conexo, se tiene:

\[\Pi(X,x) \cong \Pi(U,x)\ast\Pi(V,x)\]

**** Seifert-Van Kampen para abierto simplemente conexo
En las condiciones del teorema, con $V$ simplemente conexo, se tiene:

\[\Pi(X,x) \cong \Pi(U,x)/N\]

Con $N$ es el subgrupo normal generado por $i_\ast(\Pi(U\cap V,x))$.

*** Extra: Teoría de categorías
**** El grupo fundamental como funtor
El grupo fundamental $\Pi$ es un funtor entre las categorías:

- =Top.= de los espacios topológicos con un punto base, usando como morfismos
  las funciones continuas respetando punto base.
- =Grp= de los grupos con los homomorfismos de grupos.

Estamos llamando $f_\ast$ a los morfismos creados por el funtor, $\Pi(f)$.

**** Producto categórico
El producto de dos espacios con un punto base es, usando la topología producto:

\[(X,x) \times (Y,y) \cong (X\times Y, (x,y))\]

El funtor lo lleva al producto de grupos.

**** Coproducto categórico
El coproducto de dos espacios con un punto es la suma directa de los espacios
identificando el punto. Intuitivamente, consiste pegar los dos espacios por ese
punto.

\[ X \wedge Y \cong (X \amalg Y)/(x\sim y)\]

Cuando además tenemos espacios localmente contractibles, el coproducto se lleva
al coproducto de grupos, esto es, al producto libre:

\[\Pi(X \wedge Y) \cong \Pi(X) \ast \Pi(Y)\]

Nótese que esto es un caso particular de Seifert-Van Kampen.

**** TODO Seifert-Van Kampen
** 2. Recubridores
*** 1. Introducción
**** Localmente arcoconexo
Un espacio es *localmente arcoconexo* si todo punto posee una base de 
entornos arcoconexos.

/Durante este tema tomamos los espacios como arcoconexos y localmente 
arcoconexos/.

**** Recubridores
Un *recubridor* de $X$ es un par $(Y,p)$ donde $p : Y \longrightarrow X$ es continua; 
cumpliendo que todo $x\in X$ tiene un entorno abierto $U$, llamado *entorno 
fundamental* tal que toda componente arcoconexa de $p^{-1}(U)$ se aplica 
homeomórficamente por $p$ sobre $U$.

**** Ejemplos de recubridores
Ejemplos básicos de recubridores son:

- Cualquier homeomorfismo $p : Y \longrightarrow X$
- $p : \mathbb{S}^1\longrightarrow\mathbb{S}^1$, con $p(z) = z^n$

**** Homeomorfismos locales
Un *homeomorfismo local* es una aplicación continua $f : Y \longrightarrow X$ tal que para 
todo $y\in Y$ existe $y \in V\in\tau_Y$ tal que $f|_V$ es homeomorfismo.

**** Propiedades de un recubridores
Sea $(Y,p)$ recubridores de $X$. Entonces:

1. $p$ es sobreyectiva.
2. $p$ es una aplicación abierta.
3. $p$ es un homeomorfismo local.

***** Demostración
La sobreyectividad es trivial por la definición. Dado un abierto $y\in O$; 
tengo $y \in V_y \cong U_x$ su entorno abierto, luego $p(O\cap V_y)$ es abierto. De esta 
forma,

\[p(O) = \bigcup_{y\in O} p(O \cap V_y)\]

es abierto.

*** 2. Grupo fundamental y levantamiento de aplicaciones al recubridor
**** Levantamiento de arcos
Sea $(Y,p)$ recubridor con $x_0\in X$ y $y_0 \in p^{-1}(x_0)$. Sea $f : [0,1]\longrightarrow X$ arco 
continuo con $f(0) = x_0$, entonces existe un único arco continuo $\check{f}$ cumpliendo:

  - $\check{f}(0) = y_0$
  - $p \circ \check{f} = f$

    Llamado el *levantamiento* de $f$.

***** Existencia
Tomo $\{f^{-1}(U^x) \mid x\in X\}$, que recubre por abiertos a $[0,1]$. Sabemos que 
existirá una partición del intervalo cumpliendo:

\[\exists 0 < t_1 <\dots < t_n < 1: f([t_i,t_{i+1}]) \subseteq U^{x_i}\]

La correspondiente $y_i \in V^{y_i}$ nos da un isomorfismo $p_i$ que nos 
deja definir $\check{f} : [t_i,t_{i+1}] \longrightarrow V^{y_i}$ mediante $\check{f} = (p|_{V^{y_i}})^{-1} \circ f$. Nótese que para 
tomar cada $y_i$ necesitamos usar la componente arcoconexa de la última $\check{f}(t_{i-1})$.

***** Unicidad
Sean dos levantamientos $g_1,g_2$. Su conjunto ecualizador es cerrado:

\[ A = \{ t \mid g_1(t) = g_2(t)\} \neq \varnothing\]

Pero también es abierto porque dado un punto donde coincidan, puedo tomar la 
componente arcoconexa que es isomorfa por $p$ a un entorno abierto; y las 
curvas deben coincidir en él.

**** Levantamiento de homotopías
Sea $(Y,p)$ recubridor con $x_0\in X$ y $y \in p^{-1}(x_0)$. Sea $H : [0,1]\times[0,1] \longrightarrow X$ 
continua con $H(0,0) = x_0$, entonces existe una única aplicación
continua $\check{H} : [0,1]\times [0,1] \longrightarrow Y$ cumpliendo:

  - $p \circ \check{H} = H$
  - $\check{H}(0,0) = y_0$

    Llamada el *levantamiento* de $H$.

***** Existencia
Tomamos las $\{ H^{-1}(U^x) \mid x \in X \}$ y tenemos recubrimiento por abiertos de $[0,1]^2$. 
Tendremos alguna partición cumpliendo:

\[\exists 0 < t_1 < \dots < t_n < 1 : 
H([t_i,t_{i+1}]\times[s_i,s_{i+1}]) \subset U^{x_i}\]

En la correspondiente $y_i \in V^{y_i}$ podemos definir 
$\check{H} = (p|_{V^{y_i}}^{-1})\circ H$. Nótese que tenemos que usar en cada paso la componente arcoconexa
del último lado unido a nuestro cuadrado, que no puede salirse de esa componente
por ser arcoconexa.

***** TODO Unicidad
**** Hojas del recubridor
Sea $p : Y\longrightarrow X$ recubridor. Los cardinales de los $p^{-1}(x)$ son un invariante 
llamado el *cardinal de hojas del recubridor*.

***** Demostración
Puedo definir una biyección entre $p^{-1}(x_1)$ y $p^{-1}(x_2)$ tomando un arco entre 
ellas $\gamma$, y levantándolo en cada componente. Los arcos arriba me relacionan 
los dos conjuntos. La biyección se obtiene levantando $\tilde\gamma$, que sé que es $\tilde{g}$ 
porque ella ya es un levantamiento y es único.

**** Los recubridores crean monomorfismos
Sea $p: Y \longrightarrow X$ recubridor, entonces $p_\ast : \Pi(Y,y) \longrightarrow \Pi(X,x)$ es monomorfismo.

***** Demostración
Supongamos $p_\ast[h] = 0$; tengo una homotopía $H : p \circ h \simeq f_x$ que puedo levantar tomando
$\check{H}(0,0)=y$ y sabiendo $p \circ \check{H} = H$. Tengo:

\[ p \circ \check{H} (t,0) = H(t,0) = (p\circ h)(t)\]
\[ p \circ \check{H} (t,1) = H(t,1) = f_x(t) = x\]

Luego $\check{H}(t,0) = h(t)$ porque son levantamientos de lo mismo y $\check{H}(t,1) = y$ para poder
ser levantamiento. Esto me da $\check{H} : h \simeq f_y$.

**** Conjugación y recubridores
Si $p : Y \longrightarrow X$ es recubridor y $x\in X$, entonces la familia de subgrupos:

\[ \left\{p_\ast(\Pi(Y,y)) \mid y\in p^{-1}(x)\right\}\]

forma exactamente una clase de conjugación de subgrupos de $\Pi(X,x)$.

***** Demostración
Sea $\gamma$ camino entre $y_1,y_2 \in p^{-1}(x)$ con el isomorfismo $F_\gamma([f]) = [\tilde\gamma \ast f \ast \gamma]$, construimos 
el diagrama:

\[ \begin{tikzcd}
\Pi(Y,y_1) \rar{F_\gamma} \dar{p_\ast} & \Pi(Y,y_2) \dar{p_\ast} \\
\Pi(X,x) \rar{F_{p \circ \gamma}}& \Pi(X,x)
\end{tikzcd} \]

Que es conmutativo:

\[ \begin{tikzcd}
\math{[f]} \rar{F_\gamma} \dar{p_\ast} & 
\math{[\tilde\gamma \ast f \ast\gamma]} \dar{p_\ast} \\
\math{[p\circ f]} \rar{F_{p \circ \gamma}} & 
\math{[p(\tilde\gamma) \ast p(f) \ast p(\gamma)]}
\end{tikzcd} \]

Y que nos da por tanto:

\[ p_\ast(\Pi(Y,y_2)) = 
F_{p\circ\gamma}(\Pi(Y,y_1)) =
[p(\tilde\gamma)] \ast \Pi(Y,y_1) \ast [p(\gamma)]
\]

Ahora, sea una clase de conjugación de la proyección de un grupo fundamental
$H = [g]^{-1} \ast p_\ast(\Pi(Y,y)) \ast [g]$. El levantamiento $\check{g}$ da un camino entre dos puntos de 
$p^{-1}(x)$, y vemos que:

\[ \begin{aligned}
H &= \{[g]^{-1} \ast [p\circ h] \ast g \mid [h] \in \Pi(Y,y)\} \\
&= \{[p\circ \tilde{\check{g}} \ast p\circ h \ast p \circ \check{g}] \mid [h] \in \Pi(Y,y)\} \\
&= p_\ast(F_{\check{g}}(\Pi(Y,y))) = p_\ast(\Pi(Y,y'))
\end{aligned} \]

**** Levntamiento de aplicaciones
Sea $\Phi : Z \longrightarrow X$ continua con $x_0 = \Phi(z_0)$, $y_0 \in p^{-1}(x_0)$. Tenemos que
$\exists! \Psi : Z \longrightarrow Y$ continua cumpliendo:

- $\Psi(z_0) = y_0$
- $p \circ \Psi = \Phi$

ssi $\Phi_\ast(\Pi(Z,z_0)) \subset p_\ast(\Pi(Y,y_0))$.

***** Demostración
Definimos:

\[\check\Phi(z) = \widehat{\Phi \circ f}(1)\]

Siendo el levantamiento de la imagen de una curva que tenía $f(1)=z$, $f(0)=z_0$.
Esta es una función que lo cumple; debemos demostrar que está *bien definida* y
que es continua. Esto es, $\widehat{\phi \circ g}(1) = \widehat{\phi\circ f}(1)$, para otro $g$ cumpliendo lo mismo que $f$.

Como tenemos por la condición que $\phi_\ast[f \ast\tilde{g}] = p_\ast(\alpha)$, tenemos que ambas
$H : \phi(f \ast \tilde{g}) \simeq p\circ \alpha$. Tomamos $[f \circ \tilde{g}]$ para ver:

\[ p(\widehat{\phi\circ f} \ast \widehat{\phi\circ g}) = 
\phi\circ f \ast \widetilde{\phi\circ g}\]

Y usando eso, levantamos la homotopía, para tener 
$\check{H} : \widehat{\phi\circ f} \ast \widehat{\phi \circ \tilde{g}} 
\simeq \alpha$. Pero como $\alpha$ es lazo, tengo que es lazo lo primero.

Para ver que es *continua*, sea $\check{\Phi}(z) \in O$, abierto. Tenemos $p(O)$ abierto y tomamos
$W$ como la arcocomponente de la preimagen de $U^{\Phi(z)}$ en la que está $\check{\Phi}(z)$. Ahora
sea $\Phi(z) \in p(W \cap O)$, abierto por homeomorfismo; y sea

$\Phi(\hat{O}) \subset p(W \cap O)$, que existe por continuidad de $\Phi$ y es abierto arcoconexo. 

Veamos que $\Phi(\hat{O}) \subset \check{\Phi}^{-1}(O)$. Si $\hat{z} \in \hat{O}$, entonces hay un arco $g$ que une $z$ y $\hat{z}$; y tenemos
$\Phi(g) \subset p(W\cap O)$; como hay homeomorfismo, su levantamiento está en $W \cap O$, y
se tiene:

\[\check{\Phi}(z) = \widehat{\Phi(g)}(1) \in O\]

**** Estructura de grupo topológico
Sea $G$ un grupo topológico con neutro $e$. Sea $(\check{G},p)$ un recubridor y 
$\check{e} \in p^{-1}(e)$. Entonces $\check{G}$ admite una estructura de grupo topológico que tiene
a $\check{e}$ como elemento neutro y a $p$ como homomorfismo de grupos.

*** 3.1. Isomorfismos de recubridores
**** Homomorfismos
Sean $(Y_1,p_1)$, $(Y_2,p_2)$ recubridores. Un *homomorfismo de recubridores* es
$\Phi : (Y_1,p_1) \longrightarrow (Y_2,p_2)$ continua con $p_2 \circ \Phi = p_1$.

\[ \begin{tikzcd}
Y_1 \drar[swap]{p_1} \arrow{rr}{\Phi} & & Y_2 \dlar{p_2} \\
& X &
\end{tikzcd} \]

**** Propiedades de los homomorfismos de recubridores
Los homomorfismos de recubridores cumplen:

   1. La composición de homomorfismos es homomorfismo.
   2. La identidad $Id : Y \longrightarrow Y$ es homomorfismo.
   3. El inverso de isomorfismo es isomorfismo.

      Llamamos $Aut(Y,p)$ al *grupo de automorfismos* de un recubridor.
      # ¡Forman una categoría! Debe ser algo como una "slice category".

**** Puntos fijos de los automorfismos de recubridores
Sean dos homomorfismos de recubridores $\Phi,\Psi$ con $\Phi(y) = \Psi(y)$ en algún punto; 
entonces $\Phi = \Psi$. Por tanto, todo automorfismo distinto de la identidad actúa 
sin puntos fijos.

**** Existencia de homomorfismos
Existe un homomorfismo de recubridores $\Phi : (Y_1,p_1) \longrightarrow (Y_2,p_2)$ con $\Phi(y_1) = y_2$ ssi
$p_1_\ast(\Pi(Y_1,y_1)) \subseteq p_2_\ast(\Pi(Y_2,y_2))$. Es isomorfismo en el caso de igualdad.

**** Existencia de isomorfismos
Dos recubridores son isomorfos ssi las clases de conjugación asociadas a sus 
proyecciones al grupo fundamental son iguales:

\[\{ p_1_\ast(\Pi(Y_1,y)) \mid y\in p_1^{-1}(x)\}
= \{ p_2_\ast(\Pi(Y_2,y)) \mid y\in p_2^{-1}(x)\}\]

**** Ejemplos de recubridores
***** Espacios simplemente conexos
Sólo se admiten a sí mismos como recubridores.
***** Circunferencia unidad
Grupo fundamental isomorfo a $\mathbb{Z}$ y abeliano. Sus subgrupos son de la forma $m\mathbb{Z}$,
así que, salvo isomorfismos, sus recubridores son de la forma:

\[ p_0 : \mathbb{R} \longrightarrow \mathbb{S}^1,\ p(t) = e^{it} \]
\[p_m : \mathbb{S}^1 \longrightarrow \mathbb{S}^1,\ p_m(z) = z^m\]

***** Espacio proyectivo
Como $\mathbb{R}\mathbb{P}^n$ tiene grupo fundamental $\mathbb{Z}_2$, tiene sólo a $(\mathbb{RP}^n,Id)$ y a $(\mathbb{S}^n,p)$ 
como recubridores.

**** Recubridores de recubridores
Sean $(Y_1,p_1)$, $(Y_2,p_2)$ dos recubridores, y sea $\Phi : Y_1 \longrightarrow Y_2$ un homomorfismo de 
recubridores. Entonces $(Y_1,\Phi)$ es un recubridor de $Y_2$.

**** Recubridores universales
Un recubridor $(\check{X},p)$ de $X$ es *universal* si $\check{X}$ es simplemente conexo.

*** 3.2. Automorfismos de recubridores
**** Acción del grupo fundamental
Sea $(Y,p)$ un recubridor y $x$ punto de $X$ definimos la acción

\[ (\cdot) : p^{-1}(x) \times \Pi(X,x) \longrightarrow p^{-1}(x) \]

construyendo $y \cdot \alpha$ como sigue: sea $\alpha = [f]$, tomamos $\check{f}(0) = y$ y llamamos 
$y \cdot \alpha = \check{f}(1)$.

***** TODO Está bien definida
***** TODO Es una acción transitiva
***** TODO Espacios homogéneos.
**** Índice y hojas del recubridor
Sea $(Y,p)$ recubridor; su número de hojas es el índice del subgrupo  $p_\ast(\Pi(Y,y))$ 
en $\Pi(X,x)$; donde $y \in p^{-1}(x)$.

**** Automorfismos del espacio homogéneo
Un *automorfismo del espacio homogéneo* $p^{-1}(x)$ es una biyección
$\varphi : p^{-1}(x) \longrightarrow p^{-1}(x)$ tal que:

\[ \varphi(y \cdot \alpha) = \varphi(y) \cdot \alpha\]

**** Automorfismos del espacio homogéneo y automorfismos de recubridores
Como dado un automorfismo de recubridores $\Phi\in Aut(Y,p)$, tenemos que 
$\Phi(y\cdot \alpha) = \Phi(y)\cdot\alpha$, tenemos $\Phi|_{p^{-1}(x)}$ un automorfismo del espacio homogéneo.
De hecho, es isomorfismo de grupos:

\[ ( \bullet |_{p^{-1}(x)}) : Aut(Y,p) \longrightarrow Aut(p^{-1}(x))\]

***** Demostración
****** La restricción es automorfismo en el espacio homogéneo
Sea $y\in p^{-1}(x)$, tenemos $p(\Phi(y)) = p(y) = x$, luego $\Phi(y) \in p^{-1}(x)$.

****** Los automorfismos de recubridor respetan la acción de grupos
Sea $[f]=\alpha$, y sea $\check{f}$ su levantamiento en $y$; si considero $\Phi(\check{f})$ puedo 
comprobar que $\Phi(\check{f})(0) = \Phi(y)$ y que $p \circ \Phi (\check{f}) = f$, luego es el levantamiento
de $f$ en $\Phi(y)$. Así:

\[\Phi(y)\cdot\alpha = \Phi(\check{f})(1) = \Phi(y \cdot \alpha)\]

****** Es inyectiva
Llamamos $F = (\bullet |_{p^{-1}(x)})$. Sea $\Phi\in\ker(F)$, entonces $\Phi|_{p^{-1}(x)} = Id$; pero dos
homomorfismos de recubridores coindiciendo en un punto son iguales.

****** Es sobreyectiva
Sea $\varphi\in Aut(p^{-1}(x))$. Por el lema de existencia:
       
\[\exists \phi\in Aut(Y,p): \phi(y) = \varphi(y) \Leftrightarrow
p_\ast(\Pi(Y,y)) = p_\ast(\Pi(Y,\varphi(y))\]
       
Pero tenemos la igualdad de estos dos grupos de isotropía por:
       
\[\begin{aligned}
H_{\varphi(y)} 
&= \{\alpha\in\Pi(X,x) \mid \varphi(y)\cdot\alpha = \varphi(y)\} \\
&= \{\alpha\in\Pi(X,x) \mid \varphi(y\cdot\alpha) = \varphi(y)\} \\
&= \{\alpha\in\Pi(X,x) \mid y\cdot\alpha = y\} = H_y\\
\end{aligned}\]
       
**** Identificación de automorfismos de un espacio homogéneo
Dado $E$ espacio homogéneo sobre $G$, existe el isomorfismo:

\[ Aut(E) \cong N(H)/H \]

donde $H = \operatorname{Stab}(y)$ y $N(H)$ es su normalizador, el mayor 
grupo en el que es normal.

**** Identificación de automorfismos del recubridor
Aplicando las dos identificaciones anteriores:

\[Aut(Y,p) \cong N(p_\ast(\Pi(Y,y))) / p_\ast(\Pi(Y,y))\]

**** Recubridores regulares
Un recubridor es *regular* cuando $p_\ast(\Pi(Y,y))$ es subgrupo normal de 
$\Pi(X,x)$ siendo $y \in p^{-1}(x)$. Equivalen, por conjugación:

- $\exists x\in X, y \in Y: p_\ast(\Pi(Y,y))$ subgrupo normal en $\Pi(X,x)$.
- $\forall x\in X, y \in Y: p_\ast(\Pi(Y,y))$ subgrupo normal en $\Pi(X,x)$.

**** Propiedades de recubridores regulares
Sea $(Y,p)$ un recubridor regular:

1. $Aut(Y,p) \cong \Pi(X,x)/p_\ast(\Pi(Y,y))$
2. Si es el universal, $Aut(Y,p) \cong \Pi(X,x)$. Y el número de hojas es 
   el orden de $\Pi(X,x)$.

**** Ejemplos de recubridores
***** Circunferencia, recubridor universal
Tenemos el recubridor $(\mathbb{R},p)$ de $\mathbb{S}$, que tiene como automorfismos:

\[Aut(\mathbb{R},p) = \{\Phi_n(t) = t + 2\pi n \mid n \in \mathbb{N}\}\]

***** Espacio proyectivo
Siendo $(\mathbb{S}^n,p)$ un recubridor de dos hojas de $\mathbb{RP}^n$, sus automorfismos 
vienen dados por:

\[Aut(\mathbb{RP}^n,p) = \{Id, -Id\}\]

***** Circunferencia, otros recubridores
Sean los recubridores $(\mathbb{S}^1,p_n)$ de $\mathbb{S}^1$. Sus grupos de automorfismos son:

\[Aut(\mathbb{S}^1,p_n) = \left\{\Phi_k(z) = e^{\frac{2\pi k}{n}i}z 
\mid k = 0,1,\dots,n-1 \right\}\]

**** Teorema de Borsuk-Ulam
No existe una aplicación continua $F : \mathbb{S}^2 \longrightarrow \mathbb{S}^1$ respetando antípodas, es decir,
$F(-x) = -F(x)$.

*** 4. Recubridores regulares y espacios cocientes
**** Acción transitiva de automorfismos de recubridores regulares
$Aut(Y,p)$ actúa transitivamente sobre $p^{-1}(x)$ ssi $(Y,p)$ es regular.

**** Acciones discontinuas
Un $G \subset Homeo(Y)$ *actúa discontinuamente* si:
 
\[\forall y\in Y: \exists V_y\text{ entorno}: \forall \Phi \in G:\quad
\Phi \neq Id \Rightarrow \Phi(V) \cap V = \varnothing\]

**** Recubrimiento de cocientes
Sea $G \subset Homeo(Y)$ actuando propia y discontinuamente sobre $Y$. Si
$p : Y \longrightarrow Y/G$ es proyección al cociente, $(Y,p)$ es recubridor regular de $Y/G$, 
con $Aut(Y,p) = G$.

**** Espacios lente
Sean las aplicaciones $\Phi_k : \mathbb{S}^{2n+1} \longrightarrow \mathbb{S}^{2n+1}$ definidas por:

\[\Phi_k(z) = e^{\frac{2\pi ik}{p}}z\]

Entonces $G_p = \{\Phi_0,\Phi_1,\dots,\Phi_{p-1}\}$ actúa discontinuamente. Llamamos *espacio 
lente* al cociente:

\[ L_p^{2n+1} = \mathbb{S}^{2n+1}/G_p\]

*** 5. Existencia de espacios recubridores
**** Caso del recubridor universal
Si $X$ admite recubridor universal, toda clase de conjugación de subgrupos de
$\Pi(X,x)$ está asociada a un recubridor.

**** Espacios semilocalmente simplemente conexos
Todo $x$ posee un entorno abierto y arcoconexo $U_x$ tal que el homomorfismo 
inducido por la inclusión $\Pi(U_x,x) \longrightarrow \Pi(X,x)$ es trivial.

***** Contraejemplo
El espacio formado por infinitos círculos de radio cada vez menor y unidos 
en un punto:

\[ X = \bigcup_{n>0} \left\{(x,y) \in \mathbb{R}^2 \mid 
\left(x-\frac{1}{n}\right)^2 + y^2 = \frac{1}{n^2} \right\}\]

cumple que cualquier entorno del $(0,0)$ contiene un lazo no trivial.

**** Existencia del recubridor universal
Un espacio semilocalmente simplemente conexo tiene recubridor universal.

**** Teorema de existencia de recubridores
Sea $X$ semilocalmente simplemente conexo. Para toda clase de conjugación de 
subgrupos de $\Pi(X,x)$, existe un recubridor que la tiene asociada.

***** Demostración
Sea $C$ clase de conjugación de algún $H < \Pi(X,x)$. Sea $Y$ recubridor universal 
de $X$, que existe por ser semilocalmente simplemente conexo, cumpliendo 
$Aut(Y,p) \cong \Pi(X,x)$; puedo tomar $G < Aut(Y,p)$ cumpliendo $G \cong H$.

Los automorfismos del recubridor actúan discontinuamente, así que $Y/G$ es un 
espacio del que es $Y$ recubridor. Induciendo la siguiente aplicación:

\[ \begin{tikzcd}
Y \rar{p} \dar{q} & X \\
Y/G \urar{\hat{p}} &
\end{tikzcd} \]

Que está bien definida por respetar la relación y es continua. Veamos que es 
un recubridor. Sea $U^x$ entorno fundamental, las arcocomponentes de su 
preimagen cumplen:

\[ p^{-1}(U) = \bigcup_{\phi\in Aut(Y,p)} \phi(V) \]

Si tenemos en cuenta que $\forall \phi \in G: q(\phi(V)) = q(V)$, tendremos:

\[\hat{p}^{-1}(U)
= \bigcup_{\phi \in Aut(Y,p)/G} q(\phi(V)) \]

Donde hay $[G:H]$ componentes. Ahora intentamos ver que $\hat{p}_\ast(\Pi(Y/G,-))$ genera
la clase de conjugación de $C$. Recordando cómo actuaban los caminos en el 
espacio base sobre los puntos del recubridor, buscamos los $\alpha\in\Pi(X,x)$ que 
cumplan $q(y)\cdot\alpha = q(y)$, es decir $\exists \phi\in G: y\cdot\alpha = \phi(y)$; luego tenemos $\alpha\in H$. Si 
tengo otro $\beta \in H$, debe cumplir $\phi(y) = y\cdot\beta$ para algún $\phi\in G$, y entonces
$q(y)\cdot\alpha = q(y)$, siendo de los buscados. Tenemos:

\[\hat{p}(\Pi(Y/G, q(y)) = \operatorname{Stab}(q(y)) = H\]

**** Clasificación de recubridores del círculo
El círculo $\mathbb{S}^1$ tiene como grupo fundamental a $\mathbb{Z}$, que es abeliano y tiene como 
subgrupos a $n\mathbb{Z}$.

- $0$ tiene a la *recta* como recubridor universal $\mathbb{R}$ con $p(t) = e^{2\pi it}$.
- $n\mathbb{Z}$ tiene al *círculo* como recubridor $\mathbb{S}^1$ de $n$ hojas con $p(z) = z^n$.
- $\mathbb{Z}$ tiene al recubridor *identidad*.

**** Clasificación de recubridores del toro
El toro $\mathbb{S}^1\times\mathbb{S}^1$ tiene como grupo fundamental a $\mathbb{Z}\times\mathbb{Z}$, que tiene como subgrupos 
a los generados por un generador $<(a,b)>$ o a los generados por dos, de la 
forma $<(a,b),(0,d)>$.

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ con $p(x,y) = (e^{2\pi ix},e^{2\pi iy})$.
- $<(a,b)>$ tiene al *cilindro* como recubridor $\mathbb{S}\times\mathbb{R}$ con $p(z,y) = (az, bze^{2\pi iy})$.
- $<(a,b),(0,d)>$ tienen al *toro* como recubridor $\mathbb{S}\times\mathbb{S}$ con
  $p(z,w) = (z^a,z^bw^d)$.

**** Clasificación de recubridores del cilindro
El cilindro $\mathbb{S}\times\mathbb{R}$ tiene como grupo fundamental a $\mathbb{Z}$, que es abeliano y tiene 
como subgrupos a $n\mathbb{Z}$.

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ con $p(x,y) = (e^{2\pi ix},y)$.
- $n\mathbb{Z}$ tiene al *cilindro* como recubridor universal con $p(z,y) = (e^{2\pi inz},y)$.
- $\mathbb{Z}$ tiene al recubridor *identidad*.

**** Clasificación de recubridores de la cinta de Möbius.
La cinta de Möbius tiene tipo de homotopía del círculo y por tanto grupo 
fundamental $\mathbb{Z}$. 

- $0$ tiene al *plano* como recubridor universal $\mathbb{R}^2$ sobre el que actúa el grupo
  $\phi_n(x,y) = (x+n, (-1)^ny)$ discontinuamente.
- $n\mathbb{Z}$ con $n = 2m$ par tiene al *cilindro* recubriéndose a sí mismo $m$ veces y 
  un recubrimiento de dos hojas del *cilindro* a la banda de Möbius.
- $n\mathbb{Z}$ con $n$ impar tiene a la *banda de Möbius* como recubridor.
- $\mathbb{Z}$ tiene al recubridor *identidad*.
