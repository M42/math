#+TITLE: Inferencia estadística
#+AUTHOR: Mario Román
#+EMAIL: mromang08@gmail.com

#+SETUPFILE: config.setup
#+LANGUAGE: es

# ##
# Estos apuntes se han reescrito desde los apuntes de A. Hermoso Carazo y
# M.D. Ruiz Medina para la asignatura de Inferencia Estadística del grado
# de matemáticas de la Universidad de Granada.
# ##

* Prerrequisitos
** Distribuciones
*** Función generatriz de momentos
Se define para una variable aleatoria $X$ con función de distribución
$f$ como:

\[
M_X(t) = 
\mathbb{E}(e^{tX}) =
\int_\Omega e^{tx}f(x) \;dx
\]

**** Cálculo de momentos
Se cumple que:

\[
\mathbb{E}[X^n] = \frac{\partial^n}{\partial t^n} M_X(0)
\]

*** Función característica
Se define para una variable aleatoria $X$ con función de distribución $f$:

\[
\varphi_X(t) = \mathbb{E}[e^{itX}] = \int_\Omega e^{itx}f(x)\;dx
\]

**** Cálculo de momentos
Se cumple que:

\[
\varphi_X^{(n)}(0) = i^n\mathbb{E}[X^n]
\]

** Varianza
*** Varianza
La varianza se define equivalentemente como:

\[Var(X) = E\Big[(X-EX)^2\Big] = E[X^2] - E[X]^2\]

*** Covarianza
La covarianza se define equivalentemente como:

\[cov(X,Y) = E[(X-EX)(Y-EY)] = E[XY] - E[X]E[Y]\]

Nótese que $cov(X,X) = Var(X)$. Nótese además se comporta como el 
[[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][producto interno]] de un espacio prehilbertiano.

*** Varianza de la suma
La varianza de una suma cumple:

\[
Var(X+Y) = Var(X) + Var(Y) + 2cov(X,Y)
\]

En el caso general:

\[Var\left(\sum X_i\right) = \sum_i\sum_j cov(X_i,X_j)\]

*** Cauchy-Schwarz para la covarianza
Se tiene la desigualdad:

\[cov(X,Y)^2 \leq Var(X)Var(Y)
\]

**** Demostración
Sabiendo que la varianza es siempre no negativa:

\[
0 \leq Var\left(X - \frac{cov(X,Y)}{Var(Y)} Y\right) =
Var(X) - \frac{\left(cov(X,Y)\right)^2}{Var(Y)}
\]

**** Demostración por Cauchy-Schwarz
Se comprueba que la covarianza da un producto escalar que genera
un [[https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products][espacio cociente]] prehilbertiano. Aplicamos Cauchy-Schwarz.

** Esperanza condicional
*** Esperanza condicional en caso discreto
Definimos la esperanza condicional de dos variables discretas como:

\[\mathbb{E}[X|Y] = \sum_x xP(X=x\mid Y=y) = \sum_x x\frac{P(X=x, Y=y)}{P(Y=y)}\]

*** Esperanza condicional en el caso continuo
Más generalmente se define para el caso continuo:

\[
\mathbb{E}[X|Y] = \int_X x f_{X|Y}(x|y) dx = \int_X x \frac{f_{X,Y}(x,y)}{f_Y(y)} dx
\]

*** Ley de esperanza total
La esperanza condicional cumple:

\[\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]\]

**** Demostración en el caso discreto
Se tiene:

\[\begin{aligned}
E[E[X|Y]] &= \int_Y f(y)  \left(\int_X x \frac{f(x,y)}{f(y)} dx \right) dy \\ñ
&= \int_X x \int_Y f(x,y) dy dx \\&= \int_X x f(x) dx = E[X]
\end{aligned}\]

Nótese que asumimos una conmutatividad de las integrales discretas.

**** Demostración en el caso continuo
Puede consultarse la [[https://en.wikipedia.org/wiki/Law_of_total_expectation#Proof_in_the_general_case][Ley de la esperanza total]].

** Desigualdades
*** Desigualdad de Chebyshev
Para una variable aleatoria $X$ de segundo orden:

\[
P(|X-\mathbb{E}[X]| \geq a) \leq \frac{Var(X)}{a^2}
\]

** Convergencia
*** Convergencia casi segura
Una sucesión de variables aleatorias converge de forma casi segura a
otra $X_n \overset{c.s.}\longrightarrow X$ cuando el conjunto de sucesos que lo hacen tiene 
probabilidad 1.

\[
P\left(\lim_{n\to\infty} X_n = X\right) = 1
\]

*** Convergencia en probabilidad
Una sucesión de variables aleatorias converge en probabilidad a
otra $X_n \overset{P}\longrightarrow X$ cuando:

\[
\lim_{n\to\infty} P\left(|X_n-X| \geq \varepsilon\right) = 0
\]

para cualquier $\varepsilon$.

**** Equivalentemente
Si consideramos su complemento:

\[
\lim_{n\to\infty} P(|X_n-X| < \varepsilon) = 1
\]

*** Convergencia en distribución
Una sucesión de variables aleatorias converge en ley o en distribución 
a otra $X_n \overset{d}\longrightarrow X$, si se tiene que, dadas sus funciones de distribución,
convergen en los puntos en los que es continua:

\[
\forall x: F \mbox{ continua en } x:
\quad
\lim_{n\to\infty} F_n(x) = F(x)
\]

**** Equivalentemente
Se tiene $X_n\overset{d}\longrightarrow X$ si para cualquier $t$ real:

\[
\lim_{n\to\infty} E\left[ e^{tX_n} \right] = E\left[e^{tX}\right]
\]

*** Implicaciones
La convergencia casi segura implica la convergencia en probabilidad, que
implica a su vez la convergencia en distribución.

**** TODO Demostración

*** Ley débil de los grandes números
Si $X_1,X_2,\dots$ es una sucesión infinita de variables aleatorias 
independientes con la misma esperanza y varianza, entonces:

\[
\overline{X}_n = \frac{1}{n}(X_1+\dots+X_n)
\]

converge en probabilidad a $\mu$:

\[
\lim_{n\to\infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0
\]

*** Ley fuerte de los grandes números
Si $X_1,X_2,\dots$ es una sucesión infinita de variables aleatorias 
independientes e idénticamente distribuidas con $E\left[|X_i|\right] < \infty$ y
valor esperado $\mu$, entonces:

\[
P\left(
\lim_{n\to\infty} \overline{X}_n = \mu
\right) = 1
\]

* Distribuciones discretas
** 1. Distribución uniforme
*** Definición
Se define sobre un conjunto finito de valores $\{x_i\}$ con la misma 
probabilidad como:

\[f(x_i|n) = \frac{1}{n}\]
** 2. Distribución binomial
*** Definición
Determina la probabilidad de $x$ aciertos en $n$ experimentos de Bernoulli.
La función de distribución de $B(x|n,p)$ es:

\[
f(x|n,p) = {n \choose x}p^x (1-p)^{n-x}
\]

**** Esperanza

\[\mathbb{E}[X] = np\]

**** Varianza

\[Var[X] = np(1-p)\]

** 3. Distribución multinomial
*** Definición
Deriva de una binomial con $k$ salidas distintas de probabilidades $p_1,\dots,p_k$
como:

\[
f(X|n,p_1,\dots,p_k) = \frac{n!}{X_1!X_2!\dots X_n!}p^{X_1}p^{X_2}\dots p^{X_k}
\]

** 4. Distribución de Poisson
*** Definición
Definimos la distribución de Poisson $Poi(\lambda)$ como:

\[
f(n|\lambda) = \frac{e^{-\lambda}\lambda^n}{n!}
\]

**** Es una distribución
Comprobamos que suma la unidad:

\[
\sum_{n=1}^\infty f(n|\lambda) =
\sum_{n=1}^\infty e^{-\lambda}\frac{\lambda^n}{n!} = 1
\]

*** Función generatriz de momentos
La función generatriz se calcula como:

\[
M_X(t) = \sum_{n=1}^\infty e^{-\lambda}\frac{(e^t\lambda)^n}{n!}
= e^{\lambda(e^t-1)}
\]

**** Esperanza
Desde la función generadora:

\[
\mathbb{E}[X] = \frac{\partial M_X}{\partial t}(0) = \lambda
\]

**** Varianza
Desde la función generadora:

\[
Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \lambda
\]

*** Suma de Poisson
Para $X \leadsto Poi(\lambda_1)$, $Y \leadsto Poi(\lambda_2)$ independientes, su suma sigue la
distribución con el parámetro suma.

\[
X + Y \leadsto Poi(\lambda_1+\lambda_2)
\]

* Distribuciones continuas
** 1. Distribución normal
*** Definición
Definimos la distribución normal ${\cal N}(\mu,\sigma^2)$ como aquella con función de
densidad:

\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

**** Imagen de la distribución
#+BEGIN_SRC R :file images/normal.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dnorm, seq(-3, 3, 0.1),
                          mean = 0, sd = 1+0.1*i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "Distribución normal, variando σ²."))
#+END_SRC

#+RESULTS:
[[file:images/normal.png]]

**** Es una distribución
Tenemos que comprobar que integra la unidad sobre los reales, y
de hecho, tomando cambio de variable $y = (x-\mu)/\sqrt{2\sigma^2}$ queda:

\[\begin{aligned}
\int^{+\infty}_{-\infty} 
\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2} =
\frac{1}{\sqrt{\pi}}\int^{+\infty}_{-\infty} 
e^{-y^2} = 1
\end{aligned}\]

Que es la [[https://en.wikipedia.org/wiki/Gaussian_integral][integral de Gauss]].

*** Función característica
La función característica de ${\cal N}(\mu,\sigma^2)$ es:

\[\varphi_X(t) = e^{it\mu - t^2\sigma^2/2}\]

**** TODO Demostración
Usamos la definición de función característica y completamos
cuadrados para tener:

\[\begin{aligned}
\varphi_X(t) &= 
\mathbb{E}\left[e^{itX}\right] &= 
\int_{-\infty}^{+\infty}
e^{itx}\frac{1}{\sqrt{2\pi\sigma^2}} 
e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx \\&=
\frac{1}{\sqrt{2\pi\sigma^2}} 
\int_{-\infty}^{+\infty}
e^{it\mu}
\end{aligned}\]

*** Suma de normales
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ e $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. Entonces $X+Y\leadsto {\cal N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.

**** TODO Demostración

*** Producto por escalar
Si $X \leadsto {\cal N}(\mu,\sigma^2)$, entonces $kX \leadsto {\cal N}(k\mu,k\sigma^2)$.

**** TODO Demostración
*** Teorema de Cramer
Sean $X,Y$ independientes. Si $X+Y$ es normal, $X$ e $Y$ son normales.

**** TODO Demostración
** 2. Distribución χ² de Pearson
*** Distribución chi cuadrado
Es un caso particular de la distribución gamma, $X \leadsto \chi^2(k) = \Gamma(k/2,1/2)$.
Al parámetro $k$ se le llama *número de grados de libertad*.

**** Gráfica de la función de densidad
#+BEGIN_SRC R :file images/chi.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dchisq, seq(0, 6, 0.1),
                          df = i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "Distribución χ²(n), variando n."))
#+END_SRC

#+RESULTS:
[[file:images/chi.png]]

**** Función de densidad

 \[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]

***** TODO Demostración
*** Función generatriz de momentos

\[M_X(t) = \frac{1}{(1-2t)^{k/2}}\], para $t < 1/2$.

**** TODO Demostración
**** Esperanza y varianza

 - $E[X] = k$
 - $Var[X] = 2k$

***** Demostración
Se calculan desde la función generatriz.

*** Propiedad de reproductividad
Si tengo una serie de variables independientes distribuidas 
por $X_i \leadsto \chi^2(k_i)$, entonces:

\[\sum_{i=1}^n X_i \leadsto \chi^2 \left(\sum_{i=1}^n k_i \right)\]

*** Relación con la normal
Dadas variables independientes $X_i \leadsto {\cal N}(0,1)$,

 \[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]

**** TODO Demostración

*** Teorema central del límite de Lèvy
Para valores pequeños, pueden usarse tablas. Para valores grandes
de $n$, podemos aproximarla mediante el Teorema Central del Límite
como:

\[ \chi^2(n) \approx {\cal N}(n,2n)\]

**** TODO Demostración                                             :extra:
** 3. Distribución t de Student
*** Definición 
Dadas dos variables independientes $X \leadsto {\cal N}(0,1)$ e $Y \leadsto \chi^2(n)$,
tenemos:

\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]

**** Función de densidad

\[ f(t) 
= \frac
{\Gamma\left(\frac{n+1}{2}\right)}
{\Gamma\left(\frac{n}{2}\right) \sqrt{n\pi}} 
\left(
1 + \frac{t^2}{n}
\right)^{-\frac{n+1}{2}}
\], $t \in \mathbb{R}$

***** TODO Demostración

**** Gráfica de la función de densidad
#+BEGIN_SRC R :file images/tstudent.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dt, seq(0, 3, 0.1),
                          df=i,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "T de Student, variando n."))
#+END_SRC

#+RESULTS:
[[file:images/tstudent.png]]

*** Momentos
Tenemos que $\exists E[T^k] \iff k < n$. Cuando existen, se tiene

 - $E[T] = 0$
 - $Var[T] = \frac{n}{n-2}$

**** TODO Demostración

*** Aproximación por la normal
Tabulada para $n$ pequeños y aproximada por ${\cal N}(0,1)$ para valores
grandes.
** 4. Distribución F de Snedecor
*** Definición
*F de Snedecor*. Dadas dos variables independientes $X \leadsto \chi^2(m)$ e
$Y \leadsto \chi^2(n)$, su cociente nos da:

\[F = \frac{X/m}{Y/n} \leadsto F(m,n)\]

**** Gráfica de la función de densidad
#+BEGIN_SRC R :file images/fsnedecor.png :results graphics
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(df, seq(0, 2, 0.04),
                          df1=i, df2=i-1,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "F de Snedecor, variando n y m."))
#+END_SRC

#+RESULTS:
[[file:images/fsnedecor.png]]

**** Función de densidad

\[g(t)
= \frac
{\Gamma(\frac{m+n}{2})}
{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}
\left(\frac{m}{n}\right)^{\frac{m}{2}}
t^{m/2-1}
\left(1+\frac{m}{n}t\right)^{-\frac{m+n}{2}}\], para $t>0$.

***** TODO Demostración

*** Momentos
Tenemos que $\exists E[T^k] \iff k < n/2$.

 - $n > 2 \Rightarrow \exists E[F] = \frac{n}{n-2}$
 - $n > 4 \Rightarrow \exists Var[F] = \frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)}$

**** TODO Demostración

*** Propiedades
\[ F \leadsto F(m,n) \iff F^{-1} \leadsto F(n,m)\]
\[T \leadsto t(n) \iff T^2 \leadsto F(1,n)\]

*** Aproximación
La distribución está tabulada y las tablas incluyen aproximaciones 
para valores grandes de $n$ y $m$.

** 5. Distribución exponencial
*** Distribución exponencial
Dado un $\lambda>0$, definimos la distribución exponencial, $\operatorname{Exp}(\lambda)$, como aquella
con función de densidad:

\[f(x) = \lambda e^{-\lambda x}\qquad \forall x \in \mathbb{R}^+_0\]

**** Es una distribución
Trivialmente integrando:

\[
\int_0^\infty \lambda e^{-\lambda x} = 
-\left[ e^{-\lambda x} \right]^\infty_0 = 1
\]

*** Suma de exponenciales
La suma de variables exponenciales es una distribución Gamma:

*** Caso particular de la distribución Gamma
La exponencial es un caso particular de la distribución Gamma:

\[
Exp(\lambda) = \Gamma(1,\lambda)
\]
** 6. Distribución de Dirichlet
*** Distribución de Dirichlet
Dado un vector de reales $\alpha_1,\alpha_2,\dots,\alpha_n$, definimos la distribución $Dir(\alpha)$ 
como la que tiene función de densidad:

\[
f(x) = \frac{1}{B(\alpha)} \prod_{i=1}^K x_i^{\alpha_i-1}
\]

donde,

\[
B(\alpha) =
\frac
{\prod_{i=1}^K \Gamma(\alpha_i)}
{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}
\]

*** Momentos
**** Esperanza

\[
E[X_i] = \frac{\alpha_i}{\sum_k \alpha_k}
\]

**** Varianza

\[
Var[X_i] = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}
\]
** 7. Distribución Gamma
*** Función Gamma
Se define la función gamma $\Gamma : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\Gamma(\alpha) = \int^\infty_0 t^{\alpha-1}e^{-t} dt\]

**** La integral está definida
Por un lado, $t^{a-1}e^{-t} < t^{a-1}$, integrable en $[0,b]$. Por otro lado,

\[\lim_{t \to \infty} \frac{t^{\alpha-1}e^{-t}}{e^{-t/2}} = 0\]

Por lo que $t^{\alpha-1}e^{-t} < e^{-t/2}$ integrable, a partir de algún punto.
Partimos la integral como:

\[\int_0^b t^{\alpha-1}e^{-t}dt + \int^{\infty}_b t^{\alpha-1}e^{-t}dt
< \infty\]

*** Propiedades de la función Gamma
Sea $\alpha > 0$, se verifica:

  1. $\Gamma(1) = 1$.
  2. $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$.
  3. $\Gamma(n+1) = n!$ para $n \in \mathbb{N}$.
  4. $\Gamma(\alpha)\Gamma(1-\alpha) = \frac{\pi}{\sin(\alpha\pi)}$ para $0<\alpha<1$.
  5. $\Gamma(1/2) = \sqrt{\pi}$.
  6. $\Gamma(\alpha) = \beta^\alpha \int^\infty_0 t^{\alpha-1}e^{-\beta t} dt$ para $\beta > 0$.

**** Demostración
***** Punto 1
Trivial.
***** Punto 2
Integral por partes.
***** Punto 3
Inducción sobre los dos primeros apartados.
***** TODO Punto 4
***** Punto 5
Trivial desde el punto anterior.
***** Punto 6
Cambio de variable $\varphi(t) = \beta t$.

*** Distribución Gamma
Dados $\alpha,\beta > 0$, definimos la distribución Gamma $\Gamma(\alpha,\beta)$ como aquella con
función de densidad:

\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\]

para $x>0$. 

**** Imagen de la distribución
#+BEGIN_SRC R :results graphics :file images/gamma.png
  library(ggplot2)
  library(ggfortify)
  gg = NULL
  for (i in c(1,2,3,4,5)) {
      gg = ggdistribution(dgamma, seq(0, 4, 0.05),
                          shape = i, rate = 1,
                          colour=topo.colors(5)[i], p=gg)
  }
  print(gg + labs(title = "Distribución gamma, variando α."))
#+END_SRC

#+RESULTS:
[[file:images/gamma.png]]
*** Propiedades de la distribución Gamma
La función de densidad de una distribución $\Gamma(\alpha,\beta)$ verifica:

  1. Cuando $0<\alpha<1$, $f$ es decreciente y $\lim_{x\to 0} f(x) = \infty$.
  2. Cuando $\alpha = 1$, $f$ es decreciente y $f(0)=1$.
  3. Cuando $\alpha>1$, $f$ es creciente en $[0,(\alpha-1)/\beta]$ y decreciente 
     en $[(\alpha-1)/\beta,\infty]$.

Y sobre convexidad y concavidad se tiene:

  1. Si $0<\alpha\leq 1$, es convexa.
  2. Si $1 < \alpha \leq 2$, es cóncava en $[0,(\alpha-1+\sqrt{\alpha+1})/\beta]$ y convexa
     en $[(\alpha-1+\sqrt{\alpha+1})/\beta,\infty]$.
  3. Si $2 < \alpha$, es cóncava en $[(\alpha-1-\sqrt{\alpha+1})/\beta,(\alpha-1+\sqrt{\alpha+1}/\beta)]$ y
     convexa en todo el resto del dominio.

**** TODO Demostración
*** Suma de Gammas
Para $X \leadsto \Gamma(\alpha_1,\beta)$, $Y \leadsto \Gamma(\alpha_2,\beta)$, independientes:

\[
X+Y \leadsto \Gamma(\alpha_1+\alpha_2,\beta)
\]
** 8. Distribución Beta
*** Función Beta
Se define la función beta $\beta : (0,\infty) \longrightarrow (0,\infty)$ como:

\[\beta(x,y) = \int^1_0 t^{x-1}(1-t)^{y-1}dt\]

**** Está bien definida
Por la [[*Relación con la función Gamma][relación con la función Gamma]] sabemos que debe estar
bien definida.

*** Relación con la función Gamma
Para cada $x,y$ se tiene:

\[\frac{\Gamma(x)\Gamma(y)}{\Gamma(xy)} = \beta(x,y)\]

**** TODO Demostración

*** Distribución Beta
Dados $p,q>0$, definimos la distribución Beta $\beta(p,q)$ como aquella con 
función de densidad:

\[f(x) = \frac{1}{\beta(p,q)} x^{p-1}(1-x)^{q-1}\]
* 1. Introducción a la inferencia estadística. Estadísticos muestrales
** Planteamiento de un problema de inferencia
*** Modelo estadístico
Un modelo estadístico $(X,{\cal P})$ consta de:

  - $X : (\Omega, {\cal A},{\cal P}) \longrightarrow (\mathbb{R},{\cal B},P_X)$ variable aleatoria que describe el 
    objeto de estudio.
  - ${\cal P}$ familia de distribuciones que pueden ser la de $X$.

*** Modelo estadístico paramétrico
Cuando se conoce la forma funcional de $P_X$ y sólo desconocemos un 
parámetro tenemos una familia paramétrica de distribuciones $F(x,\theta)$ 
para $\theta$.

*** Modelo estadístico no paramétrico
Cuando la forma funcional de $P_X$ es desconocida.

*** Muestra aleatoria simple
Una muestra aleatoria simple es un vector $(X_1,\dots,X_n)$
de variables independientes idénticamente distribuidas. 

**** Realización muestral 
Una realización muestral a un valor concreto obtenido al
observar la muestra.

**** Espacio muestral
Conjunto de todas las posibles realizaciones.

** Función de distribución empírica
*** Función de distribución muestral
La *función de distribución empírica* es una función de 
distribución razonable que podemos obtener desde una 
realización muestral.

 \[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(X_i < x)} \]

*** Propiedades de la función de distribución empírica
Fijado un $x \in \mathbb{R}$, $F^\ast(x)$ es una variable aleatoria siguiendo por 
definición una binomial:

\[ nF^\ast(x) \leadsto {\cal B}(n, F(x))\]

Calculamos su *esperanza* y *varianza* desde Bernoulli como:

 - Esperanza: $E[F^\ast(x)] = F(x)$
 - Varianza: $Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}$

Aplicando entonces el Teorema Central del Límite:

\[ \frac{F^\ast(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \leadsto {\cal N}(0,1) \]

*** Teorema de Glivenko-Cantelli
Las funciones de distribución muestrales convergen 
casi seguramente uniformemente a la teórica.

\[ P\left\{ \lim_{n \rightarrow \infty} 
\sup_{x \in \mathbb{R}} |F^\ast_n(x) - F(x)| = 0\right\} = 1\]

**** Equivalentemente
Con probabilidad 1 se tiene que, al tomar sucesivas observaciones 
independientes y considerar las correspondientes funciones de 
distribución muestrales:

\[\forall x \in \mathbb{R}: \forall \epsilon>0: \exists n_\epsilon : \forall n \geq n_\epsilon:
\quad F^\ast_n(x) - \epsilon < F_X(x) < F^\ast_n(x) + \epsilon\]

**** Demostración
[[http://matematicas.unex.es/~nogales/estadisticamatematica/TGC.pdf][Teorema de Glivenko-Cantelli]].

** Estadísticos muestrales
*** Estadístico muestral
Dada una muestra aleatoria simple, un *estadístico muestral* es una 
función sobre ella $T : (\mathbb{R}^n,{\cal B}^n)\longrightarrow (\mathbb{R}^k,{\cal B}^k)$ medible e independiente 
de cualquier parámetro desconocido.

*** Momentos muestrales no centrados
Para cada $k \in \mathbb{N}$:

\[A_k = \frac{1}{n}\sum_{i=1}^n X_i^k\]

*** Momentos muestrales centrados
Para cada $k \in \mathbb{N}$:

\[B_k = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^k\]

*** Media muestral
Caso particular,

\[A_1 = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X}\]

*** Varianza muestral
Caso particular,

\[B_2 = \frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2\]

*** Cuasivarianza muestral

\[S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2\]

* 2. Muestreo de poblaciones normales
** Muestreo de la normal
*** Lema de Fisher
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple con $X \leadsto {\cal N}(\mu,\sigma^2)$.
Los estadísticos $\overline{X}$ y $S^2$ son independientes.

**** TODO Demostración

*** A1. Inferencia de la media con varianza conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$, y $\overline{X}$ su media muestral:

\[
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)
\]

**** Demostración
Usando las propiedades de la suma de normales y la linealidad de la
esperanza y cuadracidad de la varianza tenemos:

\[
\overline{X} \leadsto {\cal N}(\mu,\sigma^2/n)
\]

Desde donde simplemente normalizamos.

*** A2. Inferencia de la media con varianza desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$ con $\overline{X}$ su media muestral y $S^2$ su cuasivarianza muestral,
entonces:

\[
\frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]

**** Demostración
Por la definición de t de Student, sabiendo:

\[
\frac{\overline{X} - \mu}{S/\sqrt{n}}
=
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)S^2}{\sigma^2}/n-1}}
\leadsto
t(n-1)
\]

*** B1. Inferencia de la varianza con media conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$, entonces:

\[
\sum_{i=1}^n \left(\frac{X_i-\mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]

**** Demostración
Usando que la suma de cuadrados de normales estándar es una
distribución chi cuadrado.

*** B2. Inferencia de la varianza con media desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$ con $S^2$ su cuasivarianza muestral, entonces:

\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]

**** Demostración
Usamos la independencia entre $X_i-\overline{X}$ y $\overline{X}-\mu$ para escribir:

\[
\sum_{i=1}^n (X_i - \mu)^2
=
\sum_{i=1}^n (X_i - \overline{X})^2 +
\sum_{i=1}^n (\overline{X} - \mu)^2
\]

Ahora bien, sabemos que:

\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi^2(n)
\]

\[
n\left(\frac{\overline{X} - \mu}{\sigma}\right)^2}
\leadsto
\chi^2(1)
\]

Y desde aquí, por unicidad de las funciones generadoras de momentos
se tiene:

\[
\sum_{i=1}^n \frac{(X_i-\overline{X})^2}{\sigma^2}
\leadsto
\chi^2(n-1)
\]

** Muestreo de dos normales
*** Extensión del lema de Fisher
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes.
Los vectores $(\overline{X},\overline{Y})$ y $(S^2_1,S^2_2)$ son independientes.

**** TODO Demostración
*** Inferencia sobre diferencia de medias con varianzas conocidas
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes:

\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1}{\sigma_1^2} + \frac{(n_2-1)S^2_2}{\sigma_2^2}}
\sqrt{\frac{\sigma_1^2/n_1 + \sigma_2^2/n_2}{n_1+n_2-2}}
}
\leadsto
t(n_1+n_2-2)
\]

**** Demostración
El numerador sigue una distribución ${\cal N}(0,\sigma^2_1/n_1+\sigma^2_2/n_2)$, así que lo
dividimos para una normal estándar. Cada uno de los sumandos de
la otra raíz forma una chi cuadrada, que al sumarse da $\chi(n_1+n_2-2)$.

Usamos entonces la definición de t de Student.
*** Inferencia sobre diferencia de medias con varianzas iguales
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma^2)$ independientes:

\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
\sqrt{\frac{(n_1-1)S^2_1+ (n_2-1)S^2_2}{n_1+n_2-2}}
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]

**** Demostración
Desde el caso anterior, tomando las varianzas iguales.

**** Demostración alternativa
El numerador sigue una ${\cal N}(0,\sigma^2(1/n_1+1/n_2))$. Podemos dividirlo por
la raíz de la varianza e incluir otra varianza en la raíz de las
cuasivarianzas muestrales para tener una $\chi^2(n_1+n_2-2)$.

Aplicamos definición de t de Student.
*** Inferencia sobre cociente de varianzas con media conocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes con muestras de
tamaños $n_1$ y $n_2$:

\[
\frac
{\sum_{i=1}^{n_1}(X_i-\mu_1)^2 / n_1\sigma_1^2}
{\sum_{i=1}^{n_2}(X_i-\mu_2)^2 / n_2\sigma_2^2}
\leadsto
F(n_1,n_2)
\]

**** Demostración
Desde la definición de la F de Snedecor, sabiendo que cada factor
es una chi cuadrada.

*** Inferencia sobre cociente de varianzas con media desconocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$ y $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$ independientes:

\[
\frac
{S_1^2/\sigma_1^2}
{S_2^2/\sigma_2^2}
\leadsto
F(n-1,m-1)
\]

**** Demostración
Aplicando la definición de F de Snedecor y sabiendo que son dos
distribuciones chi cuadrado.

* 3. Suficiencia y completitud
** Estadísticos suficientes
*** Estadístico suficiente
Un estadístico $t$ es *suficiente* para un parámetro $\theta$ cuando una vez 
conocido no puede obtenerse más información de sobre $\theta$ de los datos;
esto es:

 \[\Pr(\theta| t,x) = \Pr(\theta|t)\]

**** Definición equivalente
De forma equivalente, es *suficiente* si la distribución condicionada 
al estadístico es independiente del parámetro $\theta$:

 \[\Pr(x|t,\theta) = \Pr(x|t)\]

*** Teorema de factorización de Fisher-Neyman
$T$ es suficiente para una familia $\theta \in \Theta$ ssi existen funciones no negativas
$g$,$h$ tales que la distribución $f_\theta$ es:

\[f_\theta(x) = h(x)g_\theta(T(x))\]

Donde $g_\theta$ sólo depende de $x$ a través de $T$ y $h$ no depende de $\theta$.

**** TODO Demostración
*** Propiedades de los estadísticos suficientes
Los estadísticos suficientes cumplen:

  1. Si $T$ es suficiente para $\{P_\theta \mid \theta \in \Theta\}$, lo es para $\{P_\theta \mid \theta \in \Theta' \subset \Theta\}$.
  2. Si $T$ es suficiente y $T = h'(U)$, $U$ es suficiente.
  3. Toda transformación biunívoca de suficiente es suficiente.

**** Demostración
***** Punto 1
Si cumple la factorización para un conjunto, lo cumple para también
un subconjunto.

***** Punto 2
Por el teorema de factorización:

\[f_\theta(x) = h(x)g_\theta(h'(U(x)))\]

***** Punto 3
Trivial desde lo anterior usando la inversa.

** Estadísticos completos
*** Familia de distribuciones completa
Una familia $\{P_\theta \mid \theta \in \Theta\}$ es completa si dada $X \leadsto P_\theta$ se tiene que
para cada $g$ medible:

\[
E_\theta[g(X)] = 0,\;\forall\theta\in\Theta
\implies
P_\theta(g(X) = 0) = 1,\;\forall\theta\in\Theta
\]

*** Estadístico completo
Un estadístico $T$ es *completo* cuando para cualquier función medible $g$,
se tiene:

\[ E_\theta [g(T)] = 0, \; \forall\theta\in\Theta
\implies
P_\theta(g(T) = 0) = 1,\; \forall\theta\in\Theta\]

** Suficiencia y completitud en familias exponenciales
*** Familia exponencial k-paramétrica
Una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramétrica si:

 1. $\Theta$ es intervalo de $\mathbb{R}^k$.
 2. Los valores de la variable no dependen de $\theta$, esto es:
    $\{{ x \mid f_{\theta}(x) > 0 \} = \{{ x \mid f_{\theta'}(x) > 0 \}$ para cualesquiera $\theta,\theta' \in \Theta$.
 3. La familia es de la forma:

    \[f_\theta(x) = exp\left\{\sum_{h=1}^k {Q_h(\theta) T_h(x) + S(x) + D(\theta)}\right\}\]

*** Teorema de suficiencia y complitud
Si una familia $\{P_\theta : \theta \in \Theta\}$ es exponencial k-paramétrica, cualquier muestra
aleatoria simple también lo es:

\[
f^n_\theta(x_1,\dots,x_n) = 
exp\left\{
\sum^k_{h=1} Q_h(\theta) \left(
\sum^n_{i=1} T_h(x_i)
\right) +
\sum^n_{i=1} S(x_i) + nD(\theta)
\right\}
\]

Teniéndose además:

 1. $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ estadístico *suficiente* para $\theta$.
 2. Si $k \leq n$, y $(Q_1(\Theta), \dots Q_k(\Theta))$ contiene un abierto;
    $(\sum_i T_1(X_i), \dots \sum_i T_k(X_i))$ es *completo*.

**** TODO Demostración

*** Ejemplo: la normal para la media
La familia $\{{\cal N}(\mu,\sigma^2) \mid \mu \in \mathbb{R}\}$ es uniparamétrica escribiendo la función
de distribución como:

\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\left( \frac{x^2}{2\sigma^2} - 2\frac{x\mu}{2\sigma^2} + \frac{\mu^2}{2\sigma^2} \right)
\right\}
\]

De aquí tenemos el $T(x) = x$ suficiente para $\mu$. Y con para una muestra 
tenemos $T(x_1,\dots,x_n) = x_1 + \dots + x_n$.

*** Ejemplo: la normal para la varianza
La familia $\{{\cal N}(\mu,\sigma^2) \mid \sigma\in\mathbb{R}\}$ es uniparamétrica escribiendo la función
de distribución como:

\[
f_\theta(x) = 
exp\left\{
log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -
\frac{1}{2\sigma^2}\left(x^2 - 2x\mu + \mu^2 \right)
\right\}
\]

Así $T(x) = \sum_{i=1}^n (x_i - \mu)^2$ es suficiente. Además es completo porque
tenemos que $Q(\sigma) = -\frac{1}{2\sigma^2}$ tiene en la imagen un intervalo abierto.

*** Ejemplo: distribución de Poisson
La familia de Poisson $\{Poi(\lambda) \mid \lambda \in \mathbb{R}\}$ tiene como estimador suficiente 
del parámetro a la suma de las muestras. Tenemos:

\[
f_\lambda(x) = \frac{1}{\prod x_i!} e^{-n\lambda} \lambda^{\sum x_i}
\]

Luego por Fisher-Neyman, sabemos que $\sum x_i$ es suficiente.

* 4. Estimación puntual
** Planteamiento del problema de estimación
*** Estimador puntual
Un estimador puntual de $\theta$ es un estadístico $T$ tomando valores en el 
dominio del parámetro, $\Theta$.

*** Función de pérdida y de riesgo
La *función de pérdida*, $L(\theta,t)$, nos dice la pérdida asociada a estimar 
un parámetro si su verdadero valor es otro.

\[
L : \Theta \times \Theta \longrightarrow \Theta
\]

*** Función de riesgo
La *función de riesgo* es la que asocia a cada valor del parámetro la 
pérdida media asociada al estimador.

\[ R^L_T(\theta) = E_\theta [L(\theta,T)] \]

*** Estimador óptimo
El *estimador óptimo*, $T$, dada una función de pérdida, es el que minimiza 
uniformemente la función de riesgo:

\[ R^L_T(\theta) \leq R^L_{T''}(\theta),\quad \forall \theta \in \Theta,\; \forall T''\]

*** TODO Ejemplo de estimador óptimo
** Estimación de menor error cuadrático
*** Función de pérdida cuadrática
La función de pérdida cuadrática, ${\cal L}(\theta, t) = (t - \theta)^2$, hace a la función de 
riesgo de un estimador su error cuadrático medio:

\[R^L_T(\theta) = E_\theta[(T - \theta)^2]\]

Nótese que en el caso de $E[T] = \theta$, se tiene $R^L_T(\theta) = Var_\theta[T]$.

** Estimación insesgada de mínima varianza
*** Estimador insesgado
Un estimador $T$ de $g(\theta)$, es *insesgado* o *centrado* si:

 $E_\theta[T] = g(\theta)$

*** UMVUE: Estimador insesgado uniformemente de mínima varianza
Un estimador $T$ insesgado y de segundo orden es *UMVUE* para $g(\theta)$ si para 
cualquier otro estimador insesgado $T'$ se tiene que:

\[ Var_\theta[T] \leq Var_\theta[T']\]

**** De segundo orden
Lo llamamos de segundo orden cuando existe el momento de segundo orden:

\[
\exists \mathbb{E}_\theta[T^2(X_1,\dots,X_n)]
\quad
\forall \theta \in \Theta
\]

*** Propiedades del UMVUE
El estimador UMVUE cumple:

 - Unicidad: El UMVUE de cualquier función paramétrica, si existe, es único.
 - Linealidad: Si $T,Q$ son UMVUE para $g,h$; $aT+bQ$ es UMVUE para $ag+bh$.

**** Unicidad
Si existieran dos UMVUE con $Var(T) = Var(T')$, tendríamos:

\[\begin{aligned}
Var\left(\frac{1}{2}(T+T')\right) &= 
\frac{1}{4}
\left(
Var(T) + Var(T') + 2cov(T,T')
\right) \\& \leq
\frac{1}{4}
\left(
Var(T) + Var(T') + 2\sqrt{Var(T)Var(T')}
\right) \\& = Var(T)
\end{aligned}\]

La igualdad se da por ser UMVUE, y entonces, $cov(T,T') = Var(T)$.
De aquí $cov(T-T',T-T') = 0$, haciendo constante la diferencia entre los
dos. La diferencia entre ellos debe ser constantemente $0$ por ser ambos 
insesgados.

**** TODO Linealidad
*** Teorema de Raó-Blackwell
Si $T$ es suficiente para $\theta$ y $S$ es un estimador insesgado de $g(\theta)$ de 
segundo orden:

  - $E[S \mid T]$ es estimador insesgado de $g(\theta)$ de segundo orden.
  - $Var_\theta[E[S \mid T]] \leq Var_\theta[S]$

Es decir, $E[S \mid T]$ será normalmente mejor estimador y nunca peor que $S$.

**** Demostración
Sabemos $E[S|T] = E[S] = \theta$ por la [[*Ley de esperanza total][ley de esperanza total]]. La desigualdad
entre varianzas la vemos como:

\[\begin{aligned}
E\Big[(E[S|T] - \theta)^2 \Big] &= 
E\Big[E[S-\theta | T]^2 \Big] \leq
E\Big[E[(S-\theta)^2|T] \Big] = E\Big[(S-\theta)^2\Big]
\end{aligned}\]

Donde volvemos a usar la ley de esperanza total. La desigualdad viene
de que la varianza es positiva, o de la desigualdad de Jensen para el
cuadrado.

*** Teorema de Lehmann-Scheffé
Para $T$ suficiente y completo para $\theta$; si $g(\theta)$ admite un estimador insesgado 
de segundo orden $S$, entonces existe el UMVUE de $g(\theta)$ y está dado por:

\[ \mathbb{E} [S \mid T]\]

De otra forma, un estimador insesgado que es función de estimador completo
y suficiente es el UMVUE.

**** Demostración
Por [[*Teorema de Raó-Blackwell][Raó-Blackwell]], sabemos que es un estimador insesgado; y que, dado
cualquier otro estimador insesgado $Q$, tenemos que:

\[ Var[E[Q|T]] \leq Var[Q]\]

Ahora bien, dado otro, tendríamos:

\[
E\Big[ E[S|T] - E[Q|T] \Big] = 0
\]

Y como $T$ es completo y ambos son dependientes de $T$, eso implica que:

\[P\Big(
E[S|T] - E[Q|T] = 0
\Big) = 1\]

Por lo tanto, ambos son el UMVUE.

*** Cálculo del UMVUE
Dado $T$ suficiente y completo. Para calcular el UMVUE de $g(\theta)$ podemos:

  1. Buscar un estimador insesgado y de segundo orden cualquiera de $g(\theta)$.
     Entonces $\mathbb{E}[S|T]$ será el UMVUE.
  2. Buscar $h(T)$ tal que $\mathbb{E}_\theta[h(T)] = g(\theta)$, un estimador insesgado que es
     sólo función de $T$. Se cumplirá $\mathbb{E}[h(T)|T] = h(T)$.

** Estimación eficiente
*** Condiciones de regularidad de Fréchet-Cramer-Rao
Una familia $\{P_\theta \mid \theta\in\Theta\}$ es *regular* en el sentido de Fréchet-Cramer-Rao
si cumple que:

  1. $\Theta$ es intervalo abierto de $\mathbb{R}$.
  2. $\forall \theta,\theta'\in\Theta : \{x \mid f_\theta(x) > 0\} = \{x \mid f_{\theta'}(x) > 0\} = \chi$
  3. Tenemos $f_\theta(x)$ derivable respecto a $\theta$ para todo $x \in \chi$ con:

     \[ \int_\chi \frac{d f_\theta(x)}{d\theta} dx = 
     \frac{d}{d\theta} \int_\chi f_\theta(x) dx = 
     0, \quad \forall \theta\in\Theta\]
   
     O, cuando la distribución es discreta:
   
     \[\sum_\chi \frac{d f_\theta(x)}{d\theta}
     = 
     \frac{d}{d\theta}\sum_\chi f_\theta(x)
     = 
     0\]

*** Función de información de Fisher
Si $\{P_\theta : \theta \in \Theta\}$ es regular, definimos la *función de información* asociada
a $X$ como:

\[I_X(\theta) = E_\theta\left[\left( \frac{d}{d\theta} \ln(f_\theta(X))
\right)^2\right]\]

Y la función de información asociada a una muestra como:

\[
\[I_{X_1,\dots,X_n}(\theta) = 
E_\theta\left[\left( \frac{d}{d\theta}\ln(f_\theta(X_1,\dots,X_n))
\right)^2\right]\]

*** Propiedades de la función de información
La función de información tiene como propiedades:

  1. $I_X(\theta) \geq 0$.

  2. En el caso $I_X(\theta) = 0$, $f_\theta(X)$ no depende de $\theta$.

  3. \[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = 0\].

  4. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X) \right] = I_X(\theta) \].

  5. \[E_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = 0\].

  6. \[ Var_\theta \left[\frac{d}{d\theta} \ln f_\theta(X_1,\dots,X_n) \right] = I_{X_1,\dots,X_n}(\theta) \].

  7. Aditividad, $I_{X_1,\dots,X_n}(\theta) = nI_X(\theta)$.
 
**** Demostración
***** Punto 3
Derivando y asumiendo las condiciones de regularidad:

\[
\mathbb{E}_\theta\left[\frac{\partial}{\partial\theta} \ln f_\theta(x)\right]
=
\int_\chi \left(\frac{\partial}{\partial\theta} \ln f_\theta(x)\right) f_\theta(x)\; dx
=
\int_\chi \frac{\partial}{\partial\theta} f_\theta(x)\;dx
= 0
\]

*** Función de información bajo Cramer-Raó
Bajo las hipótesis de regularidad fuertes de Fréchet-Cramer-Raó:

\[
I(\theta) = E_{\theta}\left[
-\frac{\partial^2}{\partial\theta^2} \log f_\theta(X)
\right]
\]

Es decir, debemos exigir que:

\[
\int_X \frac{\partial^2}{\partial\theta^2} f_\theta(x) dx =
\frac{\partial^2}{\partial\theta^2} \int_X  f_\theta(x) dx
\]

**** Demostración
Notando primero la siguiente igualdad:

\[
\frac{\partial^2}{\partial\theta^2}\log f_\theta(x) =
\frac{1}{f_\theta(x)}\frac{\partial^2}{\partial\theta^2}f_\theta(x) -
\left(\frac{\partial}{\partial\theta}\log f_\theta(x)\right)^2
\]

Y simplemente tomamos esperanzas, sabiendo que por las condiciones de
regularidad:

\[
\mathbb{E}\left[
\frac{1}{f_\theta(x)}
\frac{\partial^2}{\partial\theta^2} f_\theta(x)
\right]
=
\frac{\partial^2}{\partial\theta^2}
\int_X f_\theta(x)\;dx 
= 
\frac{\partial^2}{\partial\theta^2}
1
=
0
\]

*** Estadístico regular
Un estadístico $T$ es regular en el sentido de Fréchet-Cramer-Raó, si
siendo una distribución discreta:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\sum_{x \in \chi^n} T(x)f_\theta(x) \\&= 
\sum_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x)
\end{aligned}\]

O, siendo una distribución continua:

\[\begin{aligned}
\frac{d}{d\theta}
E_\theta[T] &=
\frac{d}{d\theta} 
\int_{x \in \chi^n} T(x)f_\theta(x) \;dx \\&= 
\int_{x \in \chi^n} T(x) \frac{d}{d\theta} f_\theta(x) \;dx
\end{aligned}\]

*** Cota de Fréchet-Cramer-Raó
Si $\{P_\theta \mid \theta \in \Theta\}$ es regular, la función de información se acota
$0 < I_X(\theta) < \infty$, y $T$ es un estadístico regular, de segundo orden e
insesgado en una función derivable $g(\theta)$, se tiene:

  1. \[Var_\theta[T] \geq \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]

  2. Para todo $\theta \in \Theta$ tal que $g'(\theta) \neq 0$:

     \[Var_\theta[T] = \frac{g'(\theta)^2}{I_{X_1,\dots,X_n}(\theta)}\]
     
     ssi existe $a(\theta) \neq 0$ tal que:

     \[P_\theta\left(
     \frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = 
     a(\theta)[T(X_1,\dots,X_n) - g(\theta)]
     \right) = 1\]

**** Demostración
***** Primer punto
Llamamos primero:

\[V_\theta = \frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\]

Tenemos que:

  - $Var_\theta(V_\theta) = I_{X_1,\dots,X_n}(\theta)$
  - $\mathbb{E}(V_\theta) = 0$
  - $Cov(T,V_\theta) = \mathbb{E}[TV_\theta] - \mathbb{E}[T]\mathbb{E}[V]$

Calculando:

\[\begin{aligned}
Cov(T,V_\theta) 
&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} \ln f_\theta(x_1,\dots,x_n)\right)
f_\theta(x_1,\dots,x_n)\;dx 
\\&=
\int_{\chi^n} 
T(x_1,\dots,x_n)
\left(\frac{\partial}{\partial\theta} f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} \int_{\chi^n} 
T(x_1,\dots,x_n)
\left(f_\theta(x_1,\dots,x_n)\right)\;dx 
\\&=
\frac{\partial}{\partial\theta} g(\theta)
\end{aligned}\]

Y finalmente aplicamos la desigualdad de Cauchy-Schwartz a la 
covarianza entre $T,V_\theta$ para tener:

\[
Cov(T,V_\theta) \leq Var[T]Var[V_\theta]
\]
    
*** Estimador eficiente
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con la función de información acotada 
$0 < I_X(\theta) < \infty$ y $g(\theta)$ función paramétrica derivable. Un estimador $T$ de $g(\theta)$ 
es *eficiente* si es insesgado, regular, y su varianza alcanza la
cota de Fréchet-Cramer-Raó en todo punto:

\[Var_\theta[T] = \frac{(g'(\theta))^2}{I_{X_1,\dots,X_n}(\theta)},
\qquad \forall\theta \in \Theta\]

*** Caracterización de estimadores eficientes
Sea $\{P_\theta \mid \theta \in \Theta\}$ regular, con $0 < I_X(\theta) < \infty$ y $g(\theta)$ función paramétrica
derivable y *no constante*. Un estimador $T$ es eficiente ssi existe un $a(\theta)$
cumpliendo:

  1. \[P_\theta\left(\frac{d}{d\theta} \ln f^n_\theta(X_1,\dots,X_n) = a(\theta)[T(X_1,\dots,X_n) - g(\theta)]\right) = 1\]

  2. \[I_{X_1,\dots,X_n}(\theta) = a(\theta)g'(\theta)\]

**** Demostración
***** Primera implicación
Si tenemos un $T$ eficiente, tomamos el $T$ de la cota y $g'(\theta)\neq 0$,
de ahí tenemos la primera igualdad.

*** Ejemplo: distribución binomial
Dada la familia de distribuciones $\{B(k_0,p) \mid p \in (0,1)\}$, veremos que es
regular.

**** Es regular
El intervalo $p \in (0,1)$ es abierto y $\chi = (0,\dots,k_0)$.

Si la expresamos exponencialmente es más fácil calcular su derivada
y relacionarla con una esperanza:

\[
f_p(x)
= 
\exp\left\{\log{k_0\choose x} + x \log p + (k_0-x)\log (1-p)\right\}
\]

Tenemos:

\[
\sum_\chi \frac{\partial f_p(x)}{\partial p}
=
\sum_\chi \frac{x-pk_0}{p(1-p)} f_p(x)
=
\mathbb{E}\left[\frac{X-pk_0}{p(1-p)} \right] = 0
\]

**** Función de información
Desde la derivada podemos calcular la función de información:

\[
I_{X_1,\dots,X_n}(p) = \frac{nk_0}{p(1-p)}
\]

**** Caracterización del estimador eficiente
Calcularemos para usar la caracterización:

\[
\frac{\partial}{\partial p} \ln f_p(x_1,\dots,x_n)
=
\sum \frac{\partial}{\partial p} f_p(x_i)
=
n\frac{\overline{x}-pk_0}{p(1-p)}
\]

Así, para cumplir la caracterización, tiene sentido tomar:

  - $g(p) = pk_0$
  - $T(X_1,\dots,X_n) = \overline{X}$
  - $a(p) = \frac{n}{p(1-p)}$

** Estimación de máxima verosimilitud
*** Función de verosimilitud
Para cada realización muestral se define la función de verosimilitud
de la realización, $L_{x_1,\dots,x_n} : \Theta \longrightarrow \mathbb{R}^+_0$, como:

\[L(\theta) = f_\theta(x_1,\dots,x_n)\]

*** Estimador de máxima verosimilitud
Tenemos $\hat\theta$ estimador de máxima verosimilitud de $\theta$ cuando la estimación
asociada a cada realización muestral maximiza la verosimilitud:

\[L_{x_1,\dots,x_n}\left(\hat\theta(x_1,\dots,x_n)\right)
= \max_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)\]

*** Ecuación de máxima verosimilitud
El procedimiento habitual para hallar el estimador de máxima 
verosimilitud es derivar e igualar:

\[
\frac{\partial}{\partial\theta_j} \ln L_{X_1,\dots,X_n}(\theta_1,\dots,\theta_k)
= 0
\]

Nótese que esto sólo nos da un punto crítico.

**** Demostración
Usando simplemente que el logaritmo es creciente y la caracterización
de los máximos.

*** Propiedades del estimador de máxima verosimilitud
Un estimador de máxima verosimilitud $\hat\theta$ de $\theta$ cumple:

  1. Consistencia:

     \[\lim_{n \to \infty}\hat\theta(x_1,\dots,x_n) = \theta\]

  2. Normalidad asintótica. Para $n$ suficientemente grande, sus errores
     pueden aproximarse por una normal:

     \[\sqrt{n}(\hat\theta(x_1,\dots,x_n) - \theta) \leadsto {\cal N}(0,1/I_X(\theta))\]

**** TODO Demostración

*** Relación con estadísticos suficientes
Si $\{P_\theta \mid \theta \in \Theta\}$ admite estadístico suficiente $T$, entonces $\hat\theta$ es función
de $T$.

**** Demostración
Por Teorema de factorización de Fisher-Neyman, tenemos que la
función de distribución se escribirá como:

\[
f_\theta(x) = h(x)g_\theta(T)
\]

Entonces para maximizarla habrá que maximizar $g_\theta(T)$.

*** Relación con estimadores eficientes
Si $T$ es estimador eficiente de $\theta$, entonces $T$ es el único estimador 
máximo verosímil de $\theta$.

**** TODO Demostración

*** Función de verosimilitud de una función paramétrica
Se define la función de verosimilitud de $g : \Theta \longrightarrow \Lambda$ asociada a
una realización, $M_{x_1,\dots,x_n} : \Lambda \longrightarrow \mathbb{R}^+_0$, como:

\[M_{x_1,\dots,x_n}(\lambda)
= \sup_{\theta \in g^{-1}(\lambda)} L_{x_1,\dots,x_n}(\theta) \]

*** Estimador de máxima verosimilitud de una función paramétrica
Será $\hat\lambda$ estimador máximo verosímil de $\lambda$ cuando:

\[M_{x_1,\dots,x_n}(\hat\lambda(x_1,\dots,x_n)) 
= \max_{\lambda \in \Lambda} M_{x_1,\dots,x_n}(\lambda) \]

*** Teorema de invarianza de Zenha
Si $\hat\theta$ es estimador máximo verosímil de $\theta$, entonces $g(\hat\theta)$ es estimador
máximo verosímil de $g(\theta)$.

**** TODO Demostración
Se cumple:

\[
M(\lambda') = sup_{\theta \in g^{-1}(\lambda')} L(\theta)
\leq
sup_{\theta \in \Theta} L(\theta)
=
L(\hat\theta)
=
M(g(\hat\theta))
\]

** Método de los momentos
*** Descripción
El estimador de una función dependiente en los momentos
poblacionales es el mismo dependiendo en los momentos muestrales.

\[g(\theta) = h(m_{\theta,1},\dots,m_{\theta,k}) 
\quad\Rightarrow\quad
\widehat{g(\theta)}(X_1,\dots,X_n) = h(A_1,\dots,A_k)\]

**** Momentos poblacionales
Definimos los momentos poblacionales como:

\[m_{\theta,j} = E_\theta[X^j]\]

**** Momentos muestrales
Definimos los momentos muestrales como:

\[A_j = \frac{1}{n}\sum_{i=1}^n X^j_i\]

** Método de mínimos cuadrados
*** Descripción
Si $X_i$ son las observaciones aleatorias de una magnitud $\varphi(t,\theta)$ con
errores $\varepsilon_i$; es decir:

\[X_i = \varphi(t_i,\theta) + \varepsilon_i\]

Entonces el estimador de mínimos cuadrados de $\theta$ es el que minimice
la suma de cuadrados de los errores:

\[\sum^n_{i=1}(X_i - \varphi(t_i,\theta))^2\]

* 5. Estimación por intervalos de confianza
** Definiciones y métodos de construcción
*** Intervalo de confianza
Para $X \leadsto P_\theta$, un intervalo de confianza $\alpha$ para $\theta$ es un intervalo 
aleatorio $(I_1,I_2)$ tal que para cualquier $\theta \in \Theta$:

\[P_\theta\left(
I_1(X_1,\dots,X_n) \leq \theta \leq I_2(X_1,\dots,X_n)
\right)
\geq 1 - \alpha\]

*** Intervalo de confianza de menor longitud esperada uniformemente
Un interavlo $(I_1,I_2)$ es el de menor longitud esperada uniformemente 
si para cualquier otro $(I_1',I_2')$ al mismo nivel, se tiene:

\[
E_\theta[I_2(X_1,\dots,X_n) - I_1(X_1,\dots,X_n)]
\leq
E_\theta[I_2'(X_1,\dots,X_n) - I_1'(X_1,\dots,X_n)]
\]

*** Intervalos mediante desigualdad de Chevychev
Si $T$ es estimador insesgado de $\theta$ con varianza uniformemente acotada:

  - $E_\theta[T(X_1,\dots,X_n)] = \theta$
  - $Var_\theta[T(X_1,\dots,X_n)] \leq c$

Por lo que por Chevychev tenemos, dado $k>0$, un intervalo de confianza
para $\theta$ al nivel de confianza $1 - c/k^2$:

\[P\left(T - k \leq \theta \leq  T + k \right) \geq 1 - c/k^2\]

**** TODO Demostración

*** Pivote para un parámetro
Un pivote es una función $T(X_1,\dots,X_n,\theta)$ tal que fijado cualquier $\theta$,
$T(X_1,\dots,X_n,\theta)$ es una variable con distribución independiente de $\theta$.

*** Intervalos obtenidos mediante el método pivotal
Dado un pivote $T$ estrictamente monótono respecto a $\theta$, y dos valores
$\lambda_1,\lambda_2$, tales que:

\[P_\theta(\lambda_1 < T < \lambda_2) \geq 1 - \alpha\]

Tomamos las soluciones $\hat\theta_1, \hat\theta_2$, cumpliendo $T(X_1,\dots,\hat\theta_1) = \lambda_1$ y
$T(X_1,\dots,\hat\theta_2) = \lambda_2$; y ellas forman un intervalo de confianza:

  - $P_\theta(\hat\theta_1 < \theta < \hat\theta_2) \geq 1 - \alpha$, para $T$ creciente.
  - $P_\theta(\hat\theta_2 < \theta < \hat\theta_1) \geq 1 - \alpha$, para $T$ decreciente.

**** TODO Demostración

*** Un pivote en distribuciones continuas
Si $X$ es continua con $F_\theta$ función de distribución, un pivote es:

\[ T(X_1,\dots,X_n,\theta) = -2 \sum_{i=1}^n \ln F_\theta(X_i) \leadsto \chi^2(2n)\]

**** TODO Demostración
*** Un pivote dado un estadístico
Sea $S$ un estadístico de distribución continua con $F^S_\theta$ función de 
distribución. Un pivote es:

\[T(X_1,\dots,X_n,\theta) = F^S_\theta(S(X_1,\dots,X_n)) \leadsto U(0,1)\]

**** TODO Demostración
** Ejemplos de intervalos de confianza
*** A1. Intervalo para la media de una normal con varianza conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2_0)$. El intervalo para $\mu$ de menor longitud
media uniforme a nivel de confianza $1-\alpha$ será:

\[\left(
\overline{X}-z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}},
\overline{X}+z_{\alpha/2}\frac{\sigma_0}{\sqrt{n}}
\right)\]

donde $z_{\alpha/2}$ cumple que \[P\left(Z > z_{\alpha/2}\right) = \alpha/2\] con $Z \leadsto {\cal N}(0,1)$.

**** Pivote
Usamos como pivote a la normalizada:

\[T(X_1,\dots,X_n,\mu) = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \leadsto {\cal N}(0,1)\]

**** Intervalos candidatos
Usando el pivote, tenemos el siguiente candidato a intervalo de
confianza:

\[
1 - \alpha > 
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{\sigma_0 / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}}
\right)
\]

Debiendo tenerse que, si $\Phi$ es la función de distribución de la
normal $\Phi(\lambda_2)-\Phi(\lambda_1) = 1 - \alpha$.

**** Longitud media
Buscamos el que minimice la longitud media:

\[E_\mu \left[
\left( \overline{X} - \lambda_1\frac{\sigma_0}{\sqrt{n}} \right) -
\left( \overline{X} - \lambda_2\frac{\sigma_0}{\sqrt{n}} \right)
\right] 
= (\lambda_2-\lambda_1)\frac{\sigma_0}{\sqrt{n}}\]

Por lo que tratamos de minimizar $(\lambda_2-\lambda_1)$.

**** Minimización
Usamos multiplicadores de Lagrange para definir:

\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]

Calculando las derivadas parciales tenemos:

\[\begin{aligned}
-1-\lambda\Phi'(\lambda_1) &= 0\\
1 + \lambda\Phi'(\lambda_2) &= 0
\end{aligned}
\]

Luego debe tenerse $\Phi'(\lambda_1) = \Phi'(\lambda_2)$. Sabiendo que $\Phi'$ es la función
de distribución de la normal, tenemos $\lambda_1 = \pm \lambda_2$. Como deben ser
distintos para cumplir la restricción, tenemos $\lambda_1 = -\lambda_2$.

**** Conclusión
La restricción nos fuerza a $\Phi(\lambda_2) - \Phi(-\lambda_2) = 1 - \alpha$, luego estamos
buscando el $z_{\alpha/2}$ que cumple, para una normalizada $Z$, $P(Z > z_{\alpha/2}) = \alpha/2$.

*** A2. Intervalo para la media de una normal con varianza desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\mu$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ será:

\[
\left(
\overline{X} - t_{n-1;\alpha/2}\frac{S}{\sqrt{n}},\,
\overline{X} + t_{n-1;\alpha/2}\frac{S}{\sqrt{n}}
\right)
\]

donde $t_{n-1;\alpha/2}$ cumple que $P(Z \leq t_{n-1;\alpha/2}) = \alpha/2$ con $Z \leadsto T(n-1)$.

**** Pivote
Usaremos como pivote la t de Student:

\[
T = \frac{\overline{X}-\mu}{S/\sqrt{n}} \leadsto t(n-1)
\]

**** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1 - \alpha =
P_\mu\left(
\lambda_1 < 
\frac{\overline{X} - \mu}{S / \sqrt{n}} <
\lambda_2
\right)
=
P_\mu\left(
\overline{X} - \lambda_2\frac{S}{\sqrt{n}} <
\mu <
\overline{X} - \lambda_1\frac{S}{\sqrt{n}}
\right)
\]

Donde, si $\Phi$ es la función de distribución de $t(n-1)$, tenemos
que $1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$.

**** Longitud media
Queremos minimizar la longitud esperada del intervalo:

\[
\mathbb{E}\left[
\left(\overline{X}-\lambda_1\frac{S}{\sqrt{n}}\right) - 
\left(\overline{X}-\lambda_2\frac{S}{\sqrt{n}}\right)
\right]
=
(\lambda_2-\lambda_1)\mathbb{E}\left[
\frac{S}{\sqrt{n}}
\right]
\]

Buscamos por tanto minimizar $\lambda_2-\lambda_1$.

**** Minimización
Usamos multiplicadores de Lagrange para definir:

\[F(\lambda_1,\lambda_2) = \lambda_2-\lambda_1 + \lambda(\Phi(\lambda_2) - \Phi(\lambda_1) - (1-\alpha))\]

De donde deducimos $\Phi'(\lambda_1) = \Phi'(\lambda_2)$. Como la t de Student es simétrica
y monótona en cada mitad, tenemos $\lambda_1 = -\lambda_2$.

**** Conclusión
Buscamos entonces el $t_{\alpha/2}$ que cumple $P(Z > t_{\alpha/2}) = \alpha/2$.

*** B1. Intervalo para la varianza de una normal con media conocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\sigma^2$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ es:

\[
\left(
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;\alpha/2}}
,\quad
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\chi^2_{n;1-\alpha/2}}
\right)
\]

Donde $\chi^2_{n;\alpha/2}$ cumple que $P(Z > \chi^2_{n;\alpha/2}) = \alpha/2$ para $Z \leadsto \chi^2(n)$.

**** Pivote
Tomamos como pivote a la función:

\[
\sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2
\leadsto
\chi(n)
\]

**** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1-\alpha = P\left(
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_2}
\leq
\sigma^2
\leq
\frac{\sum_{i=1}^n (X_i-\mu)^2}{\lambda_1}
\right)
\]

Donde, si $\Phi$ es la función de distribución de $\chi(n)$, tenemos que
$1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$. Buscamos minimizar $1/\lambda_1-1/\lambda_2$.

**** Minimización
Usamos minimizadores de Lagrange. Definimos:

\[
F(\lambda_1,\lambda_2) = 
\frac{1}{\lambda_2}-\frac{1}{\lambda_1} + 
\lambda(\Phi(\lambda_2)-\Phi(\lambda_1) - (1-\alpha))
\]

Calculando las derivadas parciales tenemos:

\[
\lambda\Phi'(\lambda_2) - \frac{1}{\lambda_2^2} = 0
\]
\[
\lambda\Phi'(\lambda_1) - \frac{1}{\lambda_1^2} = 0
\]

Y por tanto se minimiza cuando $\Phi(\lambda_1)/\Phi(\lambda_2) = \lambda_2^2/\lambda_1^2$. En la práctica
se usa el intervalo de colas iguales.

**** Conclusión
El intervalo de colas iguales nos da $\lambda_1 = \chi^2_{n;1-\alpha/2}$ y $\lambda_2 = \chi^2_{n;\alpha/2}$.

*** B2. Intervalo para la varianza de una normal con media desconocida
Sea $X \leadsto {\cal N}(\mu,\sigma^2)$. El intervalo para $\sigma^2$ de menor longitud media uniforme
a nivel de confianza $1-\alpha$ es:

\[
\left(
\frac{(n-1)S^2}{\chi^2_{n-1;\alpha/2}}
,\quad
\frac{(n-1)S^2}{\chi^2_{n-1;1-\alpha/2}}
\right)
\]

Donde $\chi^2_{n-1;\alpha/2}$ cumple que $P(Z > \chi^2_{n-1;\alpha/2}) = \alpha/2$ para $Z \leadsto \chi^2(n-1)$.

**** Pivote
Tomamos como pivote:

\[
\frac{(n-1)S^2}{\sigma^2} \leadsto \chi^2(n-1)
\]

*** C1. Intervalo para la diferencia de medias de normales de varianza dada
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\mu_1-\mu_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - 
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
,\quad
\overline{X}-\overline{Y} +
z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
\right)
\]

Donde $z_{\alpha/2}$ cumple que $P(Z>z_{\alpha/2}) = \alpha/2$ para $Z\leadsto {\cal N}(0,1)$.

**** Pivote
Usamos como pivote:

\[
\frac
{\overline{X}-\overline{Y}-(\mu_1-\mu_2)}
{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}
\leadsto
{\cal N}(0,1ñ)
\]

*** C2. Intervalo para la diferencia de medias de normales de varianza igual
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2)$, $Y \leadsto {\cal N}(\mu_2, \sigma^2)$. El intervalo para $\mu_1-\mu_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

donde,

\[
S_p = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S^2_2}{n_1+n_2-2}}
\]

y donde $t_{n_1+n_2-2;\alpha/2}$ cumple que $P(Z > t_{\alpha/2}) = \alpha/2$ con $Z \leadsto t(n_1+n_2-2)$.

**** Pivote
Tomamos como pivote a la función:


\[
\frac{(\overline{X}-\overline{Y}) - (\mu_1-\mu_2)}
{
S_p
\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
}
\leadsto
t(n_1+n_2-2)
\]

**** Intervalos candidatos
Usando el pivote tenemos el siguiente invervalo de confianza:

\[
1-\alpha = P\left(
\overline{X}-\overline{Y} - \lambda_1S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\leq
\mu_1-\mu_2
\leq
\overline{X}-\overline{Y} + \lambda_2S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

Donde, si $\Phi$ es la función de distribución de $t(n_1+n_2-2)$, tenemos
que $1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

**** Minimización
Usaremos minimizadores de Lagrange para deducir de nuevo que
$\Phi'(\lambda_1) = \Phi'(\lambda_2)$, por monotonía y simetricidad, $\lambda_1=\lambda_2$.

**** Conclusión
Buscamos entonces el $t_{\alpha/2}$ que cumple $P(Z > t_{\alpha/2}) = \alpha/2$.

*** D1. Intervalo para el cociente de varianzas de normales de media dada
Sean $X \leadsto {\cal N}(\mu_1,\sigma_1^2)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\sigma_1^2/\sigma_2^2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
F_{n_2,n_1;1-\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
,
F_{n_2,n_1;\alpha/2}
\frac{\sum_{i=1}^{n_1} (X_i-\mu_1)^2/n_1}{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2/n_2}
\right)
\]

donde, $F_{n_2,n_1;\alpha/2}$ cumple que $P(Z > F_{\alpha/2}) = \alpha/2$ con $Z \leadsto F(n_2,n_1)$.

**** Pivote
Tomamos como pivote a la función:

\[
\frac
{\sum_{i=1}^{n_2} (Y_i-\mu_2)^2 / n_2\sigma_2^2}
{\sum_{i=1}^{n_2} (X_i-\mu_1)^2 / n_1\sigma_1^2}
\leadsto
F(n_2,n_1)
\]

**** Intervalos candidatos
Usando el pivote tenemos el siguiente intervalo de confianza:

\[
1 - \alpha = P
\left(
\lambda_1\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\leq
\frac{\sigma^2_1}{\sigma^2_2}
\leq
\lambda_2\frac
{\frac{1}{n_1}\sum_{i=1}^{n_1} (X_i-\mu_1)^2}
{\frac{1}{n_2}\sum_{i=1}^{n_2} (Y_i-\mu_2)^2}
\right)
\]

Donde, si $\Phi$ es la función de distribución de $F(n_1,n_2)$, tenemos que
$1-\alpha = \Phi(\lambda_2)-\Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

**** Minimización
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a $\Phi'(\lambda_2) = \Phi'(\lambda_1)$. Nótese que en este caso la distribución no es
simétrica.

**** Conclusión
Buscamos entonces:

  - el $F_{n_2,n_1;1-\alpha/2}$ que cumple $P(Z > F) = 1-\alpha/2$.
  - el $F_{n_2,n_1;\alpha/2}$ que cumple $P(Z>F) = \alpha/2$.

*** D2. Intervalo para el cociente de varianzas de normales de media desconocida
Sean $X \leadsto {\cal N}(\mu_1,\sigma^2_1)$, $Y \leadsto {\cal N}(\mu_2,\sigma_2^2)$. El intervalo para $\sigma^2_1/\sigma^2_2$ de menor
longitud media uniforme a nivel de confianza $1-\alpha$ es:

\[
\left(
F_{n_2-1,n_1-1; 1-\alpha/2}\frac{S_1^2}{S_2^2}
,
F_{n_2-1,n_1-1; \alpha/2}\frac{S_1^2}{S_2^2}
\right)
\]

donde, $F_{n_2-1,n_1-1; 1-\alpha/2}$ cumple que $P(Z > F_{\alpha/2}) = \alpha/2$ con $Z \leadsto F(n_2-1,n_1-1)$.

**** Pivote
Tomamos como pivote a la función:

\[
\frac{S_2^2/\sigma_2^2}{S_1^2/\sigma_1^2}
\leadsto
F(n_2-1,n_1-1)
\]

**** Intervalos candidatos
Usando el pivote llegamos al siguiente intervalo de confianza:

\[
1-\alpha = P
\left(
\frac{\sigma_1^2}{\sigma_2^2}\lambda_1
\leq
\frac{S_2^2}{S_1^2}
\leq
\frac{\sigma_1^2}{\sigma_2^2}\lambda_2
\right)
\]

Donde, si $\Phi$ es la función de distribución de $F(n_2-1,n_1-1)$, tenemos
que $1-\alpha = \Phi(\lambda_2) - \Phi(\lambda_1)$. Buscamos minimizar $\lambda_2-\lambda_1$.

**** Minimización
Por el mismo razonamiento con multiplicadores de Lagrange, llegamos
a $\Phi'(\lambda_2) = \Phi'(\lambda_1)$. Nótese que en este caso la distribución no es
simétrica.

**** Conclusión
Buscamos entonces:

  - el $F_{n_2-1,n_1-1;1-\alpha/2}$ que cumple $P(Z > F) = 1-\alpha/2$.
  - el $F_{n_2-1,n_1-1;\alpha/2}$ que cumple $P(Z>F) = \alpha/2$.

** TODO Intervalos unilaterales
* 6. Contraste de hipótesis
** Planteamiento del problema
*** Problema de contraste de hipótesis
Dada $(X_1,\dots,X_n)$ una muestra aleatoria simple de $X \leadsto P_\theta$, para
$\theta \in \Theta_0 \cup \Theta_1$, llamamos:

 - *Hipótesis nula*: $H_0 : \theta \in \Theta_0$
 - *Hipótesis alternativa*: $H_1 : \theta \in \Theta_1$

a dos hipótesis posibles.

*** Test de hipótesis
El *test de hipótesis* es un estadístico $\varphi$ tomando valores en $[0,1]$, que 
da la posibilidad de rechazar $H_0$ dada una realización muestral. Se 
llama:

  - *Test no aleatorizado*, si toma valores $0,1$.
  - *Test aleatorizado*, si toma valor distinto de $0,1$.

*** Tipos de errores de un test de hipótesis
Hay dos tipos de erorres:

  - *Error de tipo 1*: Rechazar $H_0$ siendo cierta. Falso negativo.
  - *Error de tipo 2*: Aceptar $H_0$ siendo falsa. Falso positivo.

*** Función de potencia de un test
Dado un test $\varphi$, su función de potencia $\beta_\varphi : \Theta \longrightarrow [0,1]$ se define:

\[\beta_\varphi(\theta) = E_\theta[\varphi(X_1,\dots,X_n)]\]

Que es la probabilidad media de rechazar $H_0$ bajo $P_\theta$.

*** Tamaño del test
El tamaño del test es $\sup_{\theta \in \Theta_0} \beta_\varphi(\theta)$, la máxima probabilidad media de
cometer un error de tipo 1.

*** Nivel de significación de un test
Un test $\varphi$ tiene nivel de significación $\alpha$ si su tamaño es menor o igual
que $\alpha$. Es decir,

\[
\forall \theta \in \Theta_0, \quad
\beta_\varphi(\theta) =
E_\theta[\varphi(X_1,\dots,X_n)] \leq
\alpha
\]

*** Test uniformemente más potente
Un test con nivel de significación $\alpha$ es uniformemente más potente a 
dicho nivel si para cualquier otro test $\varphi'$ con nivel de significación
$\alpha$, se tiene:

\[\beta_{\varphi'}(\theta) \leq \beta_\varphi(\theta)
\quad \forall \theta \in \Theta_1\]

** Lema de Neyman-Pearson
*** El problema de contraste
Fijado un nivel de significación, encontrar el test uniformemente más
potente a dicho nivel.

*** Lema de Neyman-Pearson
Sea $X \longrightarrow \{P_{\theta_0}, P_{\theta_1}\}$ y $(X_1,\dots,X_n)$ una muestra aleatoria simple
con funciones de densidad $f_0,f_1$. Consideramos el problema de contraste 
con $H_0 : \theta = \theta_0$ y $H_1 : \theta = \theta_1$.

  1. Cualquier test de la forma:

     \[
     \varphi(\tilde X) = 
     \threepartdef
     {1}{f_1(\tilde X) > kf_0(\tilde X)}
     {\gamma(\tilde X)}{f_1(\tilde X) = kf_0(\tilde X)}
     {0}{f_1(\tilde X) < kf_0(\tilde X)}
     \]
     
     con $k \in \mathbb{R}^+_0$ y $\gamma(X_1,\dots,X_n) \in [0,1]$, es de máxima potencia entre todos
     los de nivel de significación $\alpha = E_{\theta_0}[\varphi]$, su tamaño.
     
  2. Para todo $\alpha \in (0,1]$ existe un test de la forma anterior con
     $\gamma(X_1,\dots,X_n) = \gamma$ constante y tamaño $\alpha$.

  3. Si $\varphi'$ es de máxima potencia al nivel de significación $\alpha = E_{\theta_0}[\varphi']$, 
     entonces $\varphi'$ es de la forma anterior con probabilidad 1 bajo $P_{\theta_0}$ y $P_{\theta_1}$.

  4. El test de máxima potencia entre todos los de nivel de significación
     0 es:
     
     \[
     \varphi_0(\tilde X) = \twopartdef
     {1}{f_0(\tilde X) = 0}
     {0}{f_0(\tilde X) > 0}
     \]

**** Demostración
***** Punto 1
Dado otro test $\varphi'$, como toma valores en $[0,1]$ tenemos que:

\[
(\varphi-\varphi')(f_1-kf_0) \geq 0
\]

Podemos entonces integrar para tener:

\[
\int (\varphi(x)-\varphi'(x))(f_1(x)-kf_0(x)) \;dx \geq 0
\]

Conociendo las funciones de potencia $\int \varphi f_i = \beta_\varphi(\theta_i)$ y $\int \varphi' f_i = \beta_{\varphi'}(\theta_i)$ , 
tenemos:

\[
\beta_{\varphi}(\theta_1) - \beta_{\varphi'}(\theta_1) +
k(\beta_{\varphi'}(\theta_0) - \beta_{\varphi}(\theta_0))
\geq 0\]

Usando ahora que $\beta_{\varphi'}(\theta_0) \leq \beta_\varphi(\theta_0) = \alpha$, tenemos que $\beta_\varphi(\theta_1) \geq \beta_{\varphi'}(\theta_1)$.
Así, nuestro test es el más potente uniformemente.

***** Punto 3
Cuando es de máxima potencia, se da el caso de igualdad en la última
ecuación, que lleva el caso de igualdad a la integral. Como es una
integral de términos positivos, debe ser distinta de cero sólo en
un conjunto de medida nula.

** TODO Descripción mediante p-valores
** Test de la razón de verosimilitudes
*** Test de la razón de verosimilitudes
Sea $(X_1,\dots,X_n) \in \chi^n$ una muestra aleatoria simple de $X \leadsto \{P_\theta \mid \theta \in \Theta_0 \cup \Theta_1\}$.
El test de razón de verosimilitudes para el problema de contraste con
$H_0 : \theta \in \Theta_0$ y $H_1 : \theta \in \Theta_1$; se define como:

\[
\varphi(X_1,\dots,X_n) = \twopartdef
{1}{\lambda(X_1,\dots,X_n) < c}
{0}{\lambda(X_1,\dots,X_n) \geq c}
\]

donde se define:

\[\lambda(x_1,\dots,x_n) = \frac
{\sup_{\theta \in \Theta_0} L_{x_1,\dots,x_n}(\theta)}
{\sup_{\theta \in \Theta} L_{x_1,\dots,x_n}(\theta)}
\]

siendo $L$ la función de verosimilitud y $c \in (0,1]$ una constante que se 
determina imponiendo el tamaño o nivel de significación requerido.

** Dualidad entre tests de hipótesis y regiones de confianza
*** Dualidad
Sea $X \leadsto \{P_\theta \mid \theta \in \Theta\}$. Para cada $\theta_0 \in \Theta$ consideramos un conjunto $A(\theta_0) \subseteq \chi^n$
y para cada relización se define:

\[
\varphi_{\theta_0}(x_1,\dots,x_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } (x_1,\dots,x_n) \notin A(\theta_0) \\
0 & \mbox{if } (x_1,\dots,x_n) \in A(\theta_0) \\
\end{array} 
\right.
\]

Y con esto se define:

\[
S(x_1,\dots,x_n) = \{\theta\in \Theta \mid (x_1,\dots,x_n) \in A(\theta)\}
\]

Cada uno de los tests $\varphi_{\theta_0}$ tiene nivel de significación $\alpha$ ssi $S$ es una
región de confianza para $\theta$ a nivel de confianza $1-\alpha$.

** Ejemplos
*** Contrastes sobre la media de una normal con varianza conocida
**** Hipótesis: valor de la media
Si tomamos $H_0:\mu=\mu_0$ y $H_1:\mu\neq\mu_0$, podemos crear un [[*Test de la razón de verosimilitudes][TRV]] como:

\[
\varphi(X_1,\dots,X_n) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| > z_{\alpha/2} \\
0 & \mbox{if } \left| \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \right| \leq z_{\alpha/2} \\
\end{array} 
\right.
\]

***** Cálculo
Sabiendo que la función de verosimilitud es:

\[
L_{x_1,\dots,x_n}(\mu) = 
\frac{1}{(\sigma_0^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma_0^2}
\]

Calculamos el test:

\[
\lambda = 
\frac{\sup_{\mu = \mu_0} L(\mu)}{\sup_{\mu\in\mathbb{R}} L(\mu)} = 
\frac{L(\mu_0)}{L(\overline{x})} =
\exp\left\{\frac{-n(\overline{x}-\mu_0)^2}{2\sigma^2_0}\right\}
\]

Y podemos tomar raíces para tener otro test equivalente que, al
seguir una distribución normal, podemos ajustar para tener el
parámetro $\alpha$ pedido.

**** Hipótesis: media menor que un valor
Si tomamos $H_0 : \mu \leq \mu_0$ y $H_1 : \mu > \mu_0$, podemos crear un TRV como:

\[
\varphi(X_1,\dots,X_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} > z_\alpha \\
0 & \mbox{if  } \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} \leq z_\alpha \\
\end{array} 
\right.
\]

***** Cálculo
Si en este caso calculamos la $\lambda$ tenemos que:

\[
\lambda(x_1,\dots,x_n) = 
\frac{\sup_{\mu\leq\mu_0} L(\mu)}{L(\overline{x})} =
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \leq 0 \\
\frac{L(\mu_0)}{L(\overline{x})} 
& \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq 0 \\
\end{array} 
\right.
\]

**** Hipótesis: media mayor que un valor
Si tomamos $H_0 : \mu \geq \mu_0$ y $H_1 : \mu < \mu_0$, podemos crear un TRV como:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < z_{1-\alpha} \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq z_{1-\alpha}
\end{array} 
\right.
\]

***** Cálculo
Si en este caso calculamos la $\lambda$ tenemos que:

\[
\lambda(x_1,\dots,x_n) 
=
\frac{\sup_{\mu\geq\mu_0} L(\mu)}{L(\overline{x})}
=
\left\{\begin{array}{ll} 
1& \mbox{if } \overline{x} \geq \mu_0 \\
\frac{L(\mu_0)}{L(\overline{x})}& \mbox{if } \overline{x} \leq \mu_0
\end{array} 
\right.
\]

Dándonos un test:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \lambda(x_1,\dots,x_n) < c \\
0 & \mbox{if } \lambda(x_1,\dots,x_n) \geq c
\end{array} 
\right.
=
\left\{\begin{array}{ll} 
0 & \mbox{if } \overline{x} \geq \mu_0 \\
1 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} < c \\
0 & \mbox{if } \frac{L(\mu_0)}{L(\overline x)} \geq c
\end{array} 
\right.
\]

Las dos primeras condiciones colapsan cuando tomamos la raíz y
comparamos con ella:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} < c \\
0 & \mbox{if } \frac{\overline{x}-\mu_0}{\sigma_0/\sqrt{n}} \geq \min(0,c)
\end{array} 
\right.
\]

Ahora ajustamos la $c$ para que nos dé la significancia $\alpha$:

\[
\alpha = 
\sup_{\mu \geq \mu_0} 
P\left( \frac{\overline{X}-\mu_0}{\sigma_0/\sqrt{n}} < c' \right)
=
\sup_{\mu \geq \mu_0}
P\left( \frac{\overline{X}-\mu}{\sigma_0/\sqrt{n}} < \frac{\mu_0-\mu}{\sigma_0/\sqrt{n}} + c' \right)
\]

Tomamos entonces como $c' = z_{1 - \alpha}$. Nótese que es negativo.

*** Contrastes sobre la varianza de una normal con media conocida
**** Hipótesis: valor de la varianza
Si tomamos $H_0 : \sigma^2 = \sigma_0^2$ y $H_1 : \sigma^2 \neq \sigma_0^2$, creamos un TRV como:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < \chi^2_{n;1-\alpha/2} 
   \mbox{ ó } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > \chi^2_{n;\alpha/2} \\
0& \mbox{if } \chi^2_{n;1-\alpha/2} \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq \chi^2_{n;\alpha/2}
\end{array} 
\right.
\]

***** Cálculo
Sabiendo que la función de verosimilitud es:

\[
L_{x_1,\dots,x_n}(\sigma) = 
\frac{1}{(\sigma^2)^{n/2}(2\pi)^{n/2}}
e^{-\sum_{i=1}^n (x_i-\mu)^2/2\sigma^2}
\]

Usamos que el estimador máximo verosímil de $\sigma^2$ es:

\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\mu_0)^2}{n}
\]

Calculamos el test:

\[
\lambda = 
\frac{\sup_{\sigma = \sigma_0} L(\sigma)}{\sup_{\sigma\in\mathbb{R}} L(\sigma)} = 
\frac{L(\sigma_0)}{L(\widehat\sigma)} =
\left(\frac{\widehat\sigma^2}{\sigma_0^2}\right)^{n/2}
\exp\left\{
\frac{-n\widehat\sigma_0^2}{2\sigma_0^2}+\frac{n}{2}
\right\}
\]

La función $xe^{-x}$ tiene un máximo antes de ir hacia $-\infty$ por ambos
lados. Así, podemos escribir:

\[
\varphi(x_1,\dots,x_n) = 
\left\{\begin{array}{ll} 
1& \mbox{if } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} < c_1 
   \mbox{ ó } \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} > c_2 \\
0& \mbox{if } c_1 \leq \frac{\sum^n_{i=1} (x_i-\mu_0)^2}{\sigma_0^2} \leq c_2
\end{array} 
\right.
\]

Nótese que sigue una distribución $\chi^2(n)$ como suma de $n$ normales.

*** Contrastes sobre la varianza de una normal con media desconocida
Se tendrá como estimador máximo verosímil a:

\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n}
\]

* 7. Teoría general de modelos lineales
** Modelo lineal general y modelo de Gauss Markov
*** Modelo lineal general
El modelo general lineal queda descrito por:

\[\mathbf{Y = X\beta + \varepsilon}\]

**** Vector observable
$Y = (Y_1,\dots,Y_n)$ es un vector aleatorio observable.

**** Matriz de diseño
Una matriz conocida $X$ de dimensión $n \times k$, cuyo rango determina el 
rango del modelo.

**** Vector de efectos
Un vector desconocido $\beta = (\beta_1,\dots,\beta_k)$.

**** Vector de errores
Un vector aleatorio no observable $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)$ representando el 
error entre $Y$ y $X\beta$.

*** Modelo de Gauss-Markov
Modelo lineal donde las componentes del vector de errores son variables
aleatorias de segundo orden, centradas, homocedásticas (igual varianza)
e incorreladas:

  - $E[\varepsilon_i] = 0$
  - $E[\varepsilon_i^2] = \sigma^2$
  - $E[\varepsilon_i\varepsilon_j] = 0$

**** Enunciado vectorial
Las condiciones sobre el vector de errores equivalen a exigir:

  - $E[\varepsilon] = 0$
  - $E[\varepsilon\varepsilon^T] = \sigma^2 I_{n \times n}$

**** Objetivo del modelo
Inferir $\beta$ y $\sigma^2$ a partir de observaciones del vector $Y$.

** Estimación de mínimos cuadrados del vector de efectos
*** Modelo
En el modelo de Gauss-Markov queremos minimizar la suma de
cuadrados de los errores:

\[S^2(\beta) = 
\sum^n_{i=1} \varepsilon^2_i =
\|Y - X\beta\|^2\]

**** Minimización
Para minimizarlo, calculamos la derivada:

\[\frac{\partial}{\partial \beta_h} S^2(\beta) = 
-2 x_{ih} \sum^n_{i=1} \left(
Y_i - \sum^k_{j=1} x_{ij}\beta_j
\right) = 0
\]

Y obtenemos las ecuaciones normales siguientes:

\[
\sum^n_{i=1} Y_i x_{ih} = \sum^n_{i=1}\sum^k_{j=1} x_{ij}x_{ih}\beta_j
\]

Que pueden expresarse matricialmente como:

\[X^TY = (X^TX)\beta\]

*** Estimador de mínimos cuadrados de beta
Llamamos $\widehat\beta$ al estimador de mínimos cuadrados de $\beta$.

  - Existencia: existe al menos un estimador de mínimos cuadrados de $\beta$.
  - Unicidad: garantizada cuando el modelo es de rango máximo por tenerse
    la solución \[\widehat\beta(Y) = (X^TX)^{-1}X^TY\].

** Funciones estimables
*** Función lineal estimable
Una $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable si admite un estimador insesgado,
lineal en las componentes de $Y$. Es decir:

\[\exists \widehat\psi(Y) = c_1Y_1 + \dots + c_nY_n\]

tal que $E[\widehat\psi(Y)] = \psi(\beta)$.

*** Teorema de Gauss-Markov
Si $\psi(\beta) = a_1\beta_1 + \dots + a_k\beta_k$ es estimable, admite un único UMVUE. 
Dicho estimador es:

\[\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y) \]

Donde $\widehat{\beta}(Y)$ es un estimador de mínimos cuadrados de $\beta$.

**** TODO Demostración

*** Propiedades del estimador de mínimos cuadrados en modelos de rango máximo
Sea $\widehat\beta(Y) = (X^TX)^{-1}X^TY$ el estimador de mínimos cuadrados en un modelo 
de rango máximo.

  1. $\widehat\beta_j(Y)$ es el estimador lineal insesgado de mínima varianza de $\beta_j$.
  2. Las varianzas y covarianzas vienen dadas: $Cov(\widehat\beta(Y)) = \sigma^2(X^TX)^{-1}$.
  3. Toda función lineal de las componentes de $\beta$ es estimable con 
     estimador lineal insesgado de mínima varianza
     $\widehat\psi(Y) = a_1\widehat\beta_1(Y) + \dots + a_k\widehat\beta_k(Y)$.

**** TODO Demostración

** Modelo estimado
*** Modelo estimado
Siendo $\widehat\beta$ el estimador de mínimos cuadrados, llamamos:

  - Modelo estimado: $\widehat Y = X\widehat\beta$
  - Residuos mínimo-cuadráticos: $R = Y - X\widehat\beta$

*** Propiedades del modelo estimado
El modelo estimado cumple:

  1. $\widehat Y_i$ es el estimador lineal insesgado de mínima varianza de $E[Y_i]$.
  2. Los residuos son centrados $E[R_i] = 0$.
  3. El vector de residuos es ortogonal al vector estimado:

     \[X^TR = 0,\quad \widehat{Y}^TR = 0\]

*** Varianza residual
Siendo $r$ el rango de $X$, la *varianza residual* es un estimador
insesgado de $\sigma^2$:

\[
S^2_R = \frac{1}{n-r}\sum_{i=1}^n R_i^2 = \frac{1}{n-r}\|Y-X\widehat\beta\|^2
\]

** Inferencia bajo hipótesis de normalidad
*** Hipótesis de normalidad
La hipótesis de normalidad asume que los errores se distribuyen
bajo una distribución normal:

\[
Y_i = \sum_{j=1}^k x_{ij}\beta_j + \varepsilon_i 
\leadsto 
{\cal N}\left(\sum_{j=1}^k x_{ij}\beta_j, \sigma^2 \right)
\]

Para $Y_1,\dots,Y_n$ independientes.

**** Equivalentemente
Podemos expresar los errores como:

\[\varepsilon \leadsto {\cal N}(0,\sigma^2)\]

*** Función de máxima verosimilitud
La función de máxima verosimilitud bajo la hipótesis de normalidad queda
como:

\[
L_y(\beta,\sigma^2) = 
\frac{1}{(2\pi)^{n/2}\sigma^n}
\exp\left\{
-\frac{\sum_{i=1}^n(y_i - \sum_{j=1}^k x_{ij}\beta_j)^2}{2\sigma^2}
\right\}
\]

*** Estimadores máximo verosímiles de efectos
Los estimadores máximo verosímiles de $\beta$ son $\widehat\beta$, estimadores de mínimos
cuadrados.

**** TODO Demostración

*** Estimador máximo verosímil de la varianza
El estimador máximo verosímil de la varianza es:

\[
\widehat\sigma^2 = \frac{1}{n}\sum_{i=1}^n R_i^2 = \frac{n-r}{n}S^2_R
\]

* 8. Inferencia Bayesiana
# ##
# Este capítulo se ha escrito siguiendo los apuntes de estadística
# de Andrés Herrera, Nuria Rodríguez, Javier Poyatos, María del Mar Ruiz y 
# Juan Luis Suárez.
#
# Pueden consultarse los apuntes originales en:
#   https://github.com/andreshp/math-notes/tree/master/StatisticalInference
# ##

** 8.1. Introducción
*** Ley de la probabilidad total
La ley de la probabilidad total establece, para $A_i$ una partición del
espacio de sucesos:

\[
P(B) = \sum_{i=1}^n P(B|A_i)P(A_i)
\]

*** Teorema de Bayes
Para un espacio de probabilidad $(\Omega,{\cal A},P)$, si tenemos una partición dada
por $A_1,\dots,A_n$ con probabilidad no nula:

\[P(A_i|B) 
=
\frac{P(A_i \cap B)}{P(B)} 
= 
\frac{P(B|A_i)P(A_i)}{P(B)}
\]

*** Distribución a priori
Sea $X$ variable aleatoria con distribución $f(x|\theta)$, con $\theta \in \Theta$. A una 
distribución $\pi(\theta)$ establecida con información previa sobre $\theta$ se le
llama *distribución a priori*.

*** Distribución condicionada
Dada una muestra $\tilde{X} = (X_1,\dots,X_n)$ y una distribución a priori $\pi(\theta)$,
tenemos una distribución conjunta con función de densidad:

\[
f(\tilde x,\theta) = f(\tilde x|\theta)\pi(\theta)
\]

*** Distribución marginal
La distribución marginal de $\tilde X$ la definimos como:

\[
m(\tilde x) 
= 
\int_{\Omega} f(\tilde x,\theta) d\theta
=
\int_{\Omega} f(\tilde x|\theta) \pi(\theta) d\theta
\]

*** Distribución a posteriori
Dada una realización de la muestra y una distribución a priori, definimos
una distribución a posteriori como:

\[
\pi(\theta|\tilde x) = 
\frac{f(\tilde x|\theta)\pi(\theta)}{m(\tilde x)} =
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
\]

** 8.2. Estadística clásica
Se destacan las siguientes diferencias con la estadística clásica.
En la estadística clásica:

  1. La probabilidad se limita a sucesos con frecuencias relativas.
  2. El parámetro $\theta$ es fijo, completamente desconocido.
  3. Se usan estimadores de máxima verosimilitud o insesgados.
  4. Los tests de hipótesis se construyen fijando un tamaño $\alpha$.

Mientras que en la estadística bayesiana:
  
  1. La probabilidad se puede establecer previa a cualquier suceso.
  2. El parámetro $\theta$ es una variable aleatoria.
  3. El método de muestreo es irrelevante.
  4. Podemos calcular la probabilidad de que una hipótesis sea cierta.

** 8.3. Familias conjugadas
*** Familia conjugada
Sea ${\cal F} = \{\pi_i(\theta) \mid i \in I\}$ una familia de distribuciones a priori. Se llama
*conjugada* respecto a una familia de densidades $P = \{f(x|\theta) \mid \theta \in \Theta\}$ si
para cada $\pi(\theta) \in {\cal F}$ y $f(x\mid \theta) \in P$, se tiene que $\pi(\theta\mid \tilde x) \in {\cal F}$.

*** Lema de caracterización de familias conjugadas
Para $\pi(\theta),\Pi(\theta) \in {\cal F}$, equivalen:

  1. $f(\tilde x|\theta)\pi(\theta) \propto \Pi(\theta)$
  2. $\pi(\theta|\tilde x) = \Pi(\theta)$

**** Demostración
***** Primera implicación
Por definición:

\[
\pi(\theta|\tilde x) 
=
\frac{f(\tilde x|\theta)\pi(\theta)}{\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta}
=
\frac{M\Pi(\theta)}{M \int_\Omega \Pi(\theta) d\theta}
=
\Pi(\theta)
\]

Usando que $\int_\Omega \Pi = 1$ por ser distribución.

***** Segunda implicación
Por definición:

\[
f(\tilde x|\theta)\pi(\theta) = \Pi(\theta)\int_\Omega f(\tilde x|\theta)\pi(\theta) d\theta
\]

*** Caracterización de familias conjugadas
Una familia de distribuciones a priori ${\cal F}$ es conjugada respecto a ${\cal P}$ ssi
el producto de cualesquiera dos distribuciones de ambas familias vuelve
a ser una distribución de la familia de distribuciones a priori, salvo
alguna constante.

\[\forall f \in {\cal P}, \pi \in{\cal F}: \exists k:\quad 
kf(x|\theta)\pi(\theta) \in {\cal F}\]

*** Ejemplos de familias conjugadas
**** Beta para Bernoulli
La familia de distribuciones Beta es una familia conjugada para:

  - distribuciones de Bernoulli.
  - distribuciones binomiales.
  - distribuciones binomiales negativas.

Se tiene:

  - $X \leadsto B(n,\theta)$
  - $\theta \leadsto \beta(p,q)$
  - $\theta|x \leadsto \beta(x+p,n-x+q)$

**** Gamma para Poisson
La familia de distribuciones Gamma es una familia conjugada para
distribuciones de Poisson.

Se tiene:

  - $X\leadsto Poi(\theta)$
  - $\theta \leadsto \Gamma(\alpha,\beta)$
  - $\theta|\tilde x \leadsto \Gamma(\sum x_i + \alpha, n + \beta)$

**** Normales para normales con varianza conocida
La familia de distribuciones normales es conjugada para las 
distribuciones normales de varianza conocida.

Se tiene:

  - $X \leadsto {\cal N}(\mu,\sigma^2)$
  - $\mu \leadsto {\cal N}(\eta,\tau)$
  - $\mu|\tilde x \leadsto {\cal N}\left(\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}, \frac{\sigma^2\tau^2}{n\tau^2+\sigma^2}\right)$

**** Dirichlet para multinomiales
La familia de distribuciones de Dirichlet es conjugada para la
familia de distribuciones multinomiales.

Se tiene:

  - $X_1,\dots,X_n \leadsto Multi(\theta_1,\dots,\theta_k)$
  - $\theta_1,\dots,\theta_k \leadsto Dir(\alpha_1,\dots,\alpha_k)$
  - $\theta_1,\dots,\theta_n|x_1,\dots,x_n \leadsto Dir(x_1+\alpha_1,\dots,x_k+\alpha_k)$

** 8.4. Distribuciones objetivas
*** Distribución de Jeffreys
Para una familia $\{f(x|\theta) \mid \theta\in\Theta\}$, la distribución de Jeffreys se define 
como:

\[\pi^J(\theta) \propto \sqrt{{\cal I}_X(\theta)}\]

para la información de Fisher.

** 8.5. Convergencia de distribuciones a posteriori
*** Convergencia en un espacio paramétrico discreto
Sea $\Theta = \{\theta_1,\dots,\theta_k\}$, cuando el tamaño de la muestra diverge, la distribución
a posteriori degenera en $\theta_0$, el valor verdadero del parámetro.

\[
\pi(\theta\mid X_1,\dots,X_n) 
\overset{P_{\theta_0}}{\underset{n \to \infty}\longrightarrow} \theta_0
\]

Nótese que los $\theta_i$ deben generar distribuciones distintas para aplicar
este resultado.

**** Demostración
Dada una distribución a priori $\pi$, llamamos $\pi(\theta_j) = p_j \in [0,1]$. Llamamos
$\theta_t$ al verdadero parámetro, y tomamos $X_1,\dots,X_n$ con la distribución
dada por $f(x|\theta_t)$.

\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n f(X_j|\theta_i)}
{\displaystyle \sum_{r=1}^k\left(\prod_{j=1}^n f(X_j|\theta_r) \right) p_r}
\]

Si multiplicamos por $\prod_{j=1}^n f(X_j|\theta_t)$ tenemos:

\[
\pi(\theta_i|X_1,\dots,X_n)
=
\frac
{\displaystyle p_i\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}}
{\displaystyle \sum_{r=1}^k\left(
\prod_{j=1}^n \frac{f(X_j|\theta_r)}{f(X_j|\theta_t)}
\right) p_r}
\]

Tomando logaritmos estudiamos las variables aleatorias $Z_j = \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}$,
que son i.i.d. y por la Ley fuerte de los grandes números, tenemos que
converge casi seguramente respecto a la probabilidad que define $\theta_t$:

\[
\frac{1}{n}\sum_{j=1}^n \log\frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
\mathbb{E}\left[\log \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)} \right]
\]

Que además sabemos (no trivialmente) que es negativo. Tenemos entonces
por continuidad del logaritmo que:

\[
\prod_{j=1}^n \frac{f(X_j|\theta_i)}{f(X_j|\theta_t)}
\longrightarrow
0
\]

Aplicando esto a la probabilidad a posteriori tenemos que sólo converge
a uno en el valor $\theta_t$ y converge a cero en todos los demás.

*** Nota: Influencia de la distribución a priori
Nótese que la distribución a priori ha sido independiente de la 
convergencia a la distribución a posteriori degenerada.

*** Nota: Estimadores bayesianos
Los modelos bayesianos asignan probabilidad 1 a la hipótesis correcta
cuando el tamaño de la muestra diverge.

** 8.6. Test de hipóesis bayesianos
*** Probabilidad a posteriori de un modelo
Dados dos modelos $M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}$ y $M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}$,
la probabilidad de que se cumpla el primero condicionada a una muestra es:

\[
\pi(M_1|x) = \frac{\pi(M_1)m(x|M_1)}{\pi(M_1)m(x|M_1) + \pi(M_2)m(x|M_2)}
\]

**** Modelos
Cada modelo $M : \{f(x|\theta,M), \pi(\theta|M), \pi(M)\}$ viene dado por:

  1. Una función de distribución condicionada a cada parámetro $\theta$.
  2. Una probabilidad para cada parámetro, condicionada al modelo.
  3. Probabilidad de que se cumpla el modelo.

Necesitamos $\pi(M_1)+\pi(M_2) = 1$ en el caso de comparar esos dos modelos.

*** Factor de Bayes
Dados dos modelos $M_1 : \{f_{\theta_1}(x), \pi(\theta_1|M_1), \pi(M_1)\}$ y $M_2 : \{f_{\theta_2}(x), \pi(\theta_2|M_2), \pi(M_2)\}$,
Definimos el factor de Bayes como:

\[
B_{21}(x) = \frac{m(x|M_2)}{m(x|M_1)}
\]

cociente entre distribuciones marginales.

Cuanto más alto es, más baja es la probabilidad a posteriori del modelo $M_1$.

*** Método de Leamer: motivación
Si usamos distribuciones impropias para realizar tests de hipótesis y
las multiplicamos por coeficientes para que sean integrables, el factor
de Bayes se vería afectado arbitrariamente por estos coeficientes.

*** Método de Leamer: muestras de entrenamiento
Una *muestra de entrenamiento* $\tilde x_1 \subset \tilde x$ es una sublista de la muestra original.
Se llama *propia* si $0 < m(\tilde x_1 | M) < \infty$. Se llama *minimal* si es propia y
ninguna sublista suya lo es.

*** Método de Leamer
Dada una muestra de entrenamiento, sabemos que $\pi(\theta|M)f(\tilde x_1|\theta,M)$
integrará y podremos usarlo como distribución a priori.

Interesa utilizar una muestra minimal para que se pierdan el menor
número de elementos en la muestra para el test de hipótesis.

** TODO 8.7. Probabilidades subjetivas
* Ejercicios
** Tema 1. Introducción a la inferencia estadística
*** Ejercicio 1
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X$. 
Dar el espacio muestral y calcular la función masa de probabilidad 
de $(X_1,\dots,X_n)$ en cada uno de los siguientes casos:

  1. $X \longrightarrow \{{\cal B}(k_0,p); p \in (0,1)\}$ binomial
  2. $X \longrightarrow \{{\cal P}(\lambda); \lambda\in\mathbb{R}^+\}$ Poisson
#+end_statement

**** Punto 1
El espacio muestral es $\{0,1,\dots,k_0\}^n$, una palabra $k_0\text{-aria}$ de $n$ letras. 
Usando independencia:

\[P(x_1,\dots,x_n) = \prod P(x_i) 
= \prod_{i=1}^n \left({k_0 \choose x_i} p^{x_i}(1-p)^{k_0-x_i} \right)\]
**** Punto 2
El espacio muestral es $\mathbb{N}^n$, palabras en los naturales.
Usando independencia:

\[P(x_1,\dots,x_n) = \prod P(x_i) = \prod_{i=0}^n e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\]

*** Ejercicio 2
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X$. Dar el espacio
muestral y calcular la función masa de probabilidad de $(X_1,\dots,X_n)$ en cada uno de
los siguientes casos:

  1. $X \longrightarrow \{U(a,b); a,b\in\mathbb{R}; a < b\}$ uniforme
  2. $X\longrightarrow \{{\cal N}(\mu,\sigma^2)\}$ normal
#+end_statement
**** Punto 1
El espacio muestral aquí es $[a,b]^n$, donde por independencia tengo como función
de densidad:

\[f(x_1,\dots,x_n) = \prod f(x_i) = \left(\frac{1}{b-a}\right)^n\]

**** Punto 2
El espacio muestral es $\mathbb{R}^n$, siendo la función de densidad:

\[f(x_1,\dots,x_n) = 
\prod_{i=0}^n \frac{1}{\sigma\sqrt{2\pi}} 
e^{-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2}\]

*** Ejercicio 4
#+begin_statement
Se dispone de una muestra aleatoria simple de tamaño 40 de una
distribución exponencial de media 3, ¿cuál es la probabilidad de que
los valores de la función de distribución muestral y la teórica, en
$x=1$, difieran menos de 0.01?  Aproximadamente, ¿cuál debe ser el
tamaño muestral para que dicha probabilidad sea como mínimo 0.98?
#+end_statement

**** Probabilidad de que difieran
Tenemos que $nF^\ast_X(1) \leadsto {\cal B}(n,F(1))$, luego podemos calcular la probabilidad
como:

\[\begin{aligned}
P\Big( F(1) - 0.01 < F^\ast(1) < F(1) + 0.01 \Big) = \\
P\Big( 10.93 < 40F^\ast(1) < 11.73 \Big) = \\
P\Big(10 < 40F^\ast(1) < 12) =\\
P\Big(11 = 40F^\ast(1)) =\\
{40 \choose 11} F(1)^{11}(1-F(1))^{40-11} \approx\\
0.1318
\end{aligned}\]

Sabiendo que $F(1) = 1 - e^{-1/3} \approx 0.283$ y que $F^\ast$ es variable discreta.

***** Cálculos
#+BEGIN_SRC R :results output
f1 = 1-exp(-1/3)
f1
n = 40
n*(f1 + 0.01)
n*(f1 - 0.01)
dbinom(11,n,0.3)
#+END_SRC

#+RESULTS:
: [1] 0.2834687
: [1] 11.73875
: [1] 10.93875
: [1] 0.1318644

**** TODO Tamaño muestral
Llamamos $\sqrt{\frac{1}{n}F(1)(1-F(1))} = \sigma_n$, y esta vez aplicamos el Teorema 
Central del Límite para tener que:

\[\frac{F^\ast(1) - F(1)}{\sigma_n} \leadsto {\cal N}(0,1)\]

Lo que buscamos es que:

\[\begin{aligned}
P\Big( -0.01 < F^\ast(1)-F(1) < 0.01 \Big) > 0.98 \\
P\Big( \frac{-0.01}{\sigma_n} < \frac{F^\ast(1)-F(1)}{\sigma_n} < \frac{0.01}{\sigma_n} \Big) > 0.98 \\
\end{aligned}\]

Dada $\Phi$ función de distribución de la normal tipificada, 
tenemos que:

\[\begin{aligned}
\Phi\left(\frac{0.01}{\sigma_n}\right) -
\Phi\left(\frac{-0.01}{\sigma_n}\right) > 0.98 \\
2\Phi\left(\frac{0.01}{\sigma_n}\right) > 0.98 +1 \\
1 -\Phi\left(\frac{0.01}{\sigma_n}\right) < 0.01
\end{aligned}\]

Usando la tabla de la normal, tenemos:

\[\frac{0.01}{\sigma_n} \geq 2.33\]

Desde donde calculamos:

\[n = 10978\]

# Esto debe estar mal

***** Cálculos
#+BEGIN_SRC R :results output
# Lookup on the normal distribution table
qnorm(1-0.01)
#+END_SRC

#+RESULTS:
: [1] 2.326348

*** Ejercicio 7
#+begin_statement
Dada una muestra aleatoria simple $(X_1,\dots,X_n)$ de una variable $X$, obtener
la distribución en el muestreo de $\overline{X}$ en los casos:

  1. $X \leadsto B(1,p)$
  2. $X\leadsto P(\lambda)$
  3. $X \leadsto exp(\lambda)$
#+end_statement

**** Punto 1
Por independencia y suma de binomiales:

\[
\overline{X} \leadsto \frac{1}{n}B(n,p)
\]

Nótese que deja de ser una binomial.

**** Punto 2
Por independencia y suma de Poisson:

\[
\overline{X} \leadsto \frac{1}{n}Poi(n\lambda)
\]

**** Punto 3
Por independencia y suma de Gammas:

\[
\overline{X} = \frac{1}{n}\Gamma(n,\lambda)
\]

*** Ejercicio 10
Desde la distribución de la estimación de la normal.

** Tema 2. Distribuciones en el muestreo de poblaciones normales
*** Ejercicio 1
#+begin_statement
Se toma una muestra aleatoria simple de tamaño $5$ de una variable aleatoria
con distribución ${\cal N}(2.5, 36)$. Calcular:

  1. Probabilidad de que la cuasivarianza muestral esté comprendida entre
     $1.863$ y $2.674$.
  2. Probabilidad de que la media muestral esté comprendida entre $1.3$ y $3.5$,
     supuesto que la cuasivarianza muestral está entre $30$ y $40$.
#+end_statement

**** Probabilidad de la Cuasivarianza
Buscamos:

\[\begin{aligned}
P\Big(1.863 \leq S^2 \leq 2.674 \Big) &\leq 
P\Big(\frac{n-1}{\sigma^2}1.863 \leq \frac{n-1}{\sigma^2}S^2 \leq \frac{n-1}{\sigma^2}2.674 \Big)
\end{aligned}\]

Y sabiendo que $\frac{n-1}{\sigma^2}S^2 \leadsto \chi^2(n-1)$, sea $\phi$ la función de distribución para
tener que la probabilidad será:

\[\phi\left(\frac{n-1}{\sigma^2}2.674\right) - 
\phi\left(\frac{n-1}{\sigma^2}1.863\right) =
0.010 - 0.005 = 0.005
\]

Consultando la tabla de Poisson.

***** Cálculos
#+BEGIN_SRC R
n = 5
s2 = 36
pchisq((n-1)/s2 * 2.674, df=n-1) - pchisq((n-1)/s2 * 1.863, df=n-1)
#+END_SRC

#+RESULTS:
: 0.00499959549303851

**** Probabilidad de la Media Muestral
La suposición de que la cuasivarianza muestral está entre 30 y 40 no
aporta nada porque la media y ella son estadísticos independientes por
el Lema de Fisher.

Buscamos:

\[P\Big(  
1.3 \leq \overline{X} \leq 1.5
\Big) = 
P\Big(  
\frac{1.3 - \mu}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq 
\frac{1.5 - \mu}{\sigma/\sqrt{n}}
\Big)
\]

Y como $\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)$, siendo $\phi$ la distribución de la normal, calculamos
la probabilidad usando las tablas de la normal.

\[
\phi(-0.3726) - \phi(-0.4472) = 0.3557 - 0.3300 = 0.0257
\]

***** Cálculos
#+BEGIN_SRC R
n = 5
m = 2.5
s2 = 36
s = sqrt(36)
pnorm((1.5-m)/(s/sqrt(n))) - pnorm((1.3-m)/(s/sqrt(n)))
#+END_SRC

#+RESULTS:
: 0.0273336344978247

*** Ejercicio 3
#+begin_statement 
¿De qué tamaño mínimo habría que seleccionar una muestra de una variable
con distribución normal ${\cal N}(\mu,4)$ para poder afirmar, con probabilidad mayor
que $0.9$, que la media muestral diferirá de la poblacional menos de $0.1$?
#+end_statement

Buscamos:

\[P\Big(
\frac{-0.1}{\sigma/\sqrt{n}} \leq 
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leq
\frac{0.1}{\sigma/\sqrt{n}}
\Big)\]

Sabiendo que $\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \leadsto {\cal N}(0,1)$, calculamos:

\[\begin{aligned}
1 - 2\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\geq 0.9 \\
\phi\left(\frac{-0.1}{\sigma/\sqrt{n}}\right) &\leq 0.05 \\
\end{aligned}\]

Usando la tabla de la distribución, tenemos que:

\[\frac{0.1}{2/\sqrt{n}} = 1.65\]

Desde donde: $n = 1089$.

**** Cálculos
#+BEGIN_SRC R
s2 = 4
s = sqrt(s2)
q = qnorm(1-0.05)
((s*q)/0.1)^2
#+END_SRC

#+RESULTS:
: 1082.21738163816

*** Ejercicio 7
Comprobando sumas se llega a que siguen una Poisson.

** Tema 3. Suficiencia y complitud
*** Ejercicio 1
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable $X \leadsto B(k,p)$
y sea $T(X_1,\dots,X_n) = \sum^n_{i=1} X_i$. Probar, usando la definición y aplicando el
teorema de factorización, que $T$ es suficiente para $p$.
#+end_statement

**** Usando la definición
Llamamos $S = \sum^n_{i=1} X_i$. Veremos que $P(x_1,\dots,x_n\mid S)$ no depende de $p$.
En el caso $x_1+\dots+x_n \neq S$, la probabilidad es $0$ y claramente 
independiente de $p$. En el otro caso, tenemos:

\[\begin{aligned}
P(x_1,\dots,x_n\mid S) = 
\frac{P(x_1,\dots,x_n)}{P(S)} =
\frac
{\prod {k \choose x_i} p^{x_i}(1-p)^{k-x_i}}
{{nk \choose S} p^{S}(1-p)^{nk-S}} =
\frac
{\prod {k \choose x_i}}
{{nk \choose S}}
\end{aligned}\]

Que no depende de $p$. Hemos usado que $S = \sum_{i=1}^n X_i \leadsto {\cal B}(nk,p)$ para 
calcular la probabilidad de que valga un valor concreto.

**** Usando el teorema de factorización
Factorizamos la función de densidad como:

\[
f(x_1,\dots,x_n) =
\prod_{i=1}^n {k \choose x_i} p^{x_i}(1-p)^{k-x_i} =
\left(p^{\sum x_i}(1-p)^{nk - \sum x_i}\right)
\left(\prod^{k}_{i=1} {k\choose x_i}\right)
\]

El primer factor sólo depende de los datos a través de la suma y el
segundo factor no depende de la probabilidad.

*** Ejercicio 3
#+begin_statement
Sea $(X_1,X_2,X_3)$ una muestra aleatoria simple de una variable $X \leadsto B(1,p)$.
Probar que el estadístico $X_1+2X_2+3X_3$ no es suficiente.
#+end_statement

Llamamos $S=X_1+2X_2+3X_3$. Vamos a calcular $P(1,1,0\mid S=3)$ y 
comprobaremos que depende de $p$. Nótese que puede llegarse a $S=3$ de dos
formas, como $1+2+0$ y como $0+0+3$.

\[
P(1,1,0\mid S=3) =
\frac{P(1,1,0)}{P(S=3)} =
\frac{p^2(1-p)}{p^2(1-p)+p(1-p)^2}=
p
\]

** Tema 4. Estimación puntual. Métodos de estimación
*** Ejercicio 2
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de $X \leadsto B(1,p)$ y sea 
$T = \sum_{i=1}^n X_i$.

  1. Probar que si $k \in \mathbb{N}$ y $k\leq n$ el estadístico

     \[\frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]

     es un estimador insesgado de $p^k$. ¿Es este estimador el UMVUE?
  2. Probar que si $k>n$, no existe ningún estimador insesgado para $p^k$.
  3. ¿Puede afirmarse que $\frac{T}{n}\left(1-\frac{T}{n}\right)^2$ es insesgado para $p(1-p)^2$?
#+end_statement

**** Punto 1. Es insesgado.
Llamamos $M = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}$. Comprobamos que es insesgado calculando 
la esperanza. Dividimos entre los casos $i\leq k$ que son nulos y los demás.
Usamos $P(T = i) = {n \choose i}p^i(1-p)^{n-i}$.

\[\begin{aligned}
\mathbb{E}[M] &=
\sum_{i=1}^n {n \choose i} p^i (1-p)^{n-i}
\frac{i(i-1)\dots(i-k+1)}{n(n-1)\dots(n-k+1)} \\&=
\sum_{i=1}^n p^i (1-p)^{n-i}
\frac{(n-k)!}{(i-k)!((n-k)-(i-k))!} \\&=
p^k \sum_{i=1}^n p^{i-k} (1-p)^{(n-k)-(i-k)}
{n-k \choose i-k} \\&= p^k
\end{aligned}\]

Usando binomio de Newton en el último paso.

**** Punto 1. Es el UMVUE.
Usaremos el teorema de Lehmann-Scheffé, sabiendo que $M$ es un estimador
insesgado de $p^k$; y que $T$ es suficiente y completo para $p$.

Para ver que $T$ es completo, calculamos:

\[\begin{aligned}
\mathbb{E}[g(T)] 
&= \sum_{t=0}^n g(t) {n \choose t} p^t(1-p)^{n-t} \\
&= (1-p)^n \sum_{t=0}^n g(t) {n \choose t} \left(\frac{p}{1-p}\right)^t \\
\end{aligned}\]

Para que se anule siempre, debe anularse el polinomio
$\sum g(t) {n \choose t}r^t$ para $r \in \mathbb{R}^+$, lo que implica $g(t) = 0$.

Pero ahora, por Lehmann-Scheffé, tenemos que el UMVUE será:

\[\mathbb{E}[M\mid T] = \frac{T(T-1)\dots(T-k+1)}{n(n-1)\dots(n-k+1)}\]

Que es un UMVUE.

**** Punto 2.
Supongamos un estimador $Q$ que fuera insesgado para $p^q$. Tendríamos:

\[\mathbb{E}[Q(X)] = p^q\]

Es decir, llamando $R(T) = \sum_{\sum x_i = T} Q(X)$,

\[
\sum^n {n \choose k} R(t) p^t(1-p)^{n-t} = p^q
\]

Pero esto nos daría un polinomio de grado $q$ sobre $p$, que no puede ser 
nulo.

**** TODO Punto 3.
No. Si calculamos la esperanza usando linealidad obtenemos algo distinto.

#+BEGIN_SRC sage
n,p = var('n p')
m1 = n*p
m2 = m1*(1-p+m1)
m3 = m1*(1-3*p+3*m1+2*p^2 - 3*n*p^2 + n^2*p^2)
(p - 2*m2/n^2 + m3/n^3).normalize()
#+END_SRC

#+RESULTS:
: (n^2*p^3 - 2*n^2*p^2 - 3*n*p^3 + n^2*p + 5*n*p^2 + 2*p^3 - 2*n*p - 3*p^2 + p)/n^2

*** Ejercicio 3
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable
$X \leadsto \{{\cal P}(\lambda)\mid \lambda > 0\}$. Encontrar, si existe, el UMVUE para $\lambda^s$, siendo
$s \in \mathbb{N}$ arbitrario.
#+end_statement

Por familia uniparamétrica demostramos $T = \sum X_i$ suficiente y completo.
Probando llegamos a que $\frac{1}{n^s}T(T-1)\dots(T-s)$ es insesgado.

*** Ejercicio 7
**** Punto 1
Se comprueba que es familia exponencial uniparamétrica. Con
$T(X) = X$ y por tanto con $\sum_i T(X_i)$ como estadístico suficiente.
Además, es completo porque $Q(\Theta)$ tiene un abierto en su imagen.

**** DONE Punto 2

*** Ejercicio 8
**** DONE Punto 1

** Tema 5. Intervalos de confianza
*** Ejercicio 1
El mínimo es $n=44$.
*** Ejercicio 2
**** Primer punto
Intervalo de $(170.75, 179.75)$.

**** Segundo punto
Da un $n \geq 865$.
*** Ejercicio 5
Se llega a $t = 2.1788$.

*** Ejercicio 7
**** Primer punto
Calculamos:

  - $\overline{X} = 37.2$
  - $\overline{Y} = 16.88$
  - $S_X^2 = 482.137$
  - $S_Y^2 = 208.517$
  - $n_1 = 6$
  - $n_2 = 5$

Y tenemos un intervalo de confianza para el cociente de varianzas.
Calculando primero desde las tablas (?), con $\alpha = 0.95$:

  - $F_{1-\alpha/2} =$
  - $F_{\alpha/2} =$
*** Ejercicio 9
Tomamos el estimador:

\[
T = \frac{1}{n}\sum_{i=1}^n X_i
\]

Si partimos de la desigualdad de Chebyshev con la cota $c = 1/n$ a la
varianza, llegamos a $k = 1/\sqrt{n\alpha}$, que nos da el intervalo:

\[
\left(
\frac{1}{n} \sum X_i + \frac{1}{\sqrt{n\alpha}}
,\quad
\frac{1}{n} \sum X_i - \frac{1}{\sqrt{n\alpha}}
\right)
\]

Con la confianza $\alpha$.
** Tema 6. Contraste de hipotésis
*** Ejercicio 1
#+begin_statement
Se toma una observación de una variable con distribución de Poisson para
contrastar que la media vale 1 frente a que vale 2.

  1. Construir un test no aleatorizado con nivel de significación 0.05 
     para el contraste planteado. Calcular las probabilidades de cometer
     error de tipo 1 y de tipo 2, el tamaño y la potencia del test frente
     a la hipótesis alternativa.
  2. ¿Cómo debe aleatorizarse el test para alcanzar el tamaño 0.05?¿Cuál
     es la potencia de este test?
#+end_statement

Usaremos el lema de Neyman-Pearson para crear un test donde el problema
de contraste es: $H_0 : \lambda = 2$, $H_1 : \lambda = 1$, para una distribución $Poi(\lambda)$.

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) \geq kf_0(X) \\
0 & \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]

Sabemos que este test tendrá tamaño $\mathbb{E}_{\lambda=2}[\varphi]$, calculamos:

\[f_1(x) = \frac{1}{ex!}\]

\[
f_0(x) = \frac{2^x}{e^2x!}
\]

La condición $f_1(X) \geq kf_0(X)$ equivale a:

\[
x \leq \log_2 \frac{e}{k}
\]

Siendo $\Phi$ la función de distribución de una Poisson $Poi(2)$, que es la 
distribución que sigue aquí $x$, tenemos, consultando las tablas:

\[
\mathbb{E}_{\lambda=2}[\varphi] = 
\Phi\left(\log_2\frac{e}{k}\right) = 
0.05
\]

# Tenemos que resolver esto y no parece que en las tablas se dé nada
# sensato. Repasar el ejercicio.
*** Ejercicio 2
#+begin_statement
Una urna contiene 10 bolas, blancas y negras. Para contrastar que el 
número de bolas blancas es 5 frente a que dicho número es 6 ó 7, se
extraen tres bolas con reemplazamiento y se rechaza $H_0$ sólo si se 
obtienen 2 ó 3 bolas blancas. Calcular el tamaño de este test y la
potencia frente a alternativas.
#+end_statement

**** Tamaño del test
Llamamos $X$ a la variable dada por cada extracción, siendo 1 si es blanca
y 0 si es negra. Vemos que $X \leadsto B(1,\theta/10)$. Tenemos como hipótesis la
hipótesis nula $H_0 : \theta = 5$ y la alternativa $H_1 : \theta \in \{6,7\}$. Nuestro test está
definido por:

\[
\varphi(X_1,X_2,X_3) = 
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum X_i = 2,3 \\
0 & \mbox{if } \sum X_i = 0,1
\end{array} 
\right.
\]

Calculamos el tamaño sabiendo que $Z = \sum X_i \leadsto B(3,\theta/10)$:

\[
\sup_{\theta = 5} \beta_\varphi(\theta) = E_{\theta=5}[\varphi]
=
P(Z = 2) + P(Z=3) = \frac{1}{2}
\]

**** Potencia frente alternativas
Calculamos:

\[\beta_\varphi(6)\]
\[\beta_\varphi(7)\]

*** Ejercicio 3
#+begin_statement
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una variable aleatoria
con distribución de Poisson de parámetro $\lambda$. Encontrar el test más potente
de tamaño $\alpha$ para resolver el problema de contraste:

\[
H_0 : \lambda = \lambda_0
\]
\[
H_1 : \lambda = \lambda_1
\]

Aplicación: En una centralita telegónica el número de llamadas por minuto
sigue una distribución de Poisson. Si en cinco minutos se han recibido 12
llamadas, ¿puede aceptarse que el número medio de llamadas por minuto es
1.5, frente a que dicho número es 2, al nivel de significación 0.05?
Calcular la potencia del test obtenido.
#+end_statement

**** Desarrollo teórico
Por Neyman-Pearson, el test más potente de tamaño $\alpha$ será de la forma:

\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } f_1(x_1,\dots,x_n) > kf_0(x_1,\dots,x_n) \\
\gamma & \mbox{if } f_1(x_1,\dots,x_n) = kf_0(x_1,\dots,x_n) \\
0 & \mbox{if } f_1(x_1,\dots,x_n) < kf_0(x_1,\dots,x_n)
\end{array} 
\right.
\]

La desigualdad es equivalente a:

\[
\frac{e^{-n\lambda_1}\lambda_1^{\sum x_i}}{\prod x_i!} 
> 
k \frac{e^{-n\lambda_0}\lambda_0^{\sum x_i}}{\prod x_i!} 
\]

\[
k = \frac
{n(\lambda_0-\lambda_1)-c}
{\log\left(\lambda_0/\lambda_1\right)}
> \sum x_i
\leadsto
Poi(n\lambda_0)
\]

Por tanto podemos tomar una distribución de Poisson siendo $\rho_\alpha$ el valor
que da $\alpha$ en la función de distribución y tener:

\[
c = n(\lambda_0-\lambda_1) + \rho_\alpha\log(\lambda_0/\lambda_1)
\]

Tenemos por tanto un test de la forma:

\[
\varphi(x_1,\dots,x_n) = \left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i < \rho_\alpha \\
0 & \mbox{if } \sum x_i \geq \rho_\alpha
\end{array} 
\right.
\]

El tamaño del test entonces será $\alpha$.

**** Aplicación
En este caso tenemos $\sum x_i = 12$, para $n=5$. Para nivel de significación
$\alpha = 0.05$, tenemos $P(Poi(10) > \rho_{0.05}) = 0.05$.

Tenemos:

\[
P(Poi(10) > 14) = 0.05
\]

Y como $12 < 14$, se el test da la hipótesis alternativa.
# Esto hay que comprobarlo, que lo he hecho muy rápido y creo que le
# dado la vuelta.

*** Ejercicio 4
#+begin_statement
Sea $(X_1,\dots,X_n)$ muestra aleatoria simple de una variable con distribución
${\cal N}(\mu,\sigma^2_0)$. Deducir el test más potente de tamaño arbitrario para contrastar
hipótesis simples sobre $\mu$.
#+end_statement

Aplicaremos Neyman-Pearson para crear un test con dos hipótesis,
$H_0 : \mu =\mu_0$, $H_1:\mu =\mu_1$, de la forma:

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } f_1(X) > kf_0(X) \\
\gamma(X) & \mbox{if } f_1(X) = kf_0(X)\\
0& \mbox{if } f_1(X) < kf_0(X)
\end{array} 
\right.
\]

Que será el de mayor potencia con nivel de significación $E_{\mu_0}[\varphi]$.
Calculamos la significación sabiendo que la condición $f_1 > kf_0$ nos
da:

\[
f_1(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_1)^2}{2\sigma^2}}
\]

\[
f_0(X) = \frac{1}{(2\pi\sigma)^{n/2}} e^{-\sum \frac{(x_i-\mu_0)^2}{2\sigma^2}}
\]

Simplificando la condición:

\[
\sum x_i > \frac{\log k - (\mu_0^2-\mu_1^2)}{2(\mu_1-\mu_0)} = k'
\]

Y usamos la normal con $P(Z \geq z_\alpha) = \alpha$ para obtener el valor necesario para
$k'$ si queremos un test de tamaño $\alpha$:

\[
k' = z_\alpha\sigma/\sqtr{n} + \mu_0
\]

El test más potente de tamaño $\alpha$ es entonces:

\[
\varphi(X) =
\left\{\begin{array}{ll} 
1 & \mbox{if } \sum x_i > k' \\
0& \mbox{if } \sum x_i \leq k'
\end{array} 
\right.
\]

*** DONE Ejercicio 5
#+begin_statement
Deducir el test más potente de tamaño $\alpha$ para contrastar $H_0:\theta=\theta_0$
frente a $H_1:\theta=\theta_1$ y calcular su potencia. ¿Cuál es el test óptimo fijado
un nivel de significación arbitrario?
#+end_statement

*** Ejercicio 6
#+begin_statement
Deducir el test más potente de tamaño arbitrario para contrastar $H_0 : \theta = \theta_0$
frente a $H_1 : \theta = \theta_1$, basándose en una muestra de tamaño $n$ de una variable
aleatoria con función de densidad


\[
f_\theta(x) = \frac{\theta}{x^2}, \quad x > \theta
\]

Deducir el test óptimo para un nivel de significación arbitrario.
#+end_statement


*** Ejercicio 8
Minimizando y calculando se llega a:

\[
\lambda(X) =
\frac{1}{\theta_0^n} e^{(1-1/\theta_0)\sum x_i}
\]

*** Ejercicio 9
#+begin_statement
En base a una observación $X \leadsto B(n,p)$, deducir el test de razón de 
verosimilitudes para contrastar la hipótesis de que el parámetro $p$ no
supera un determinado valor, $p_0$.
#+end_statement

*** Ejercicio 10
#+begin_statement
Sea $X$ variable con función de densidad

\[
f_\theta(x) = \theta x^{\theta-1},\quad 0<x<1
\]

Basándose en una observación de $X$, deducir el test de razón de 
verosimilitudes de tamaño arbitrario para contrastar:

\[
H_0 : \theta \leq \theta_0
\]
\[
H_1 : \theta > \theta_0
\]
#+end_statement

*** Ejercicio 13
#+begin_statement
Un profesor asegura que tiene un nuevo método de enseñanza mejor que el
usado tradicionalmente. Para comprobar si tiene razón se selecciona de
forma aleatoria e independiente dos grupos de alumnos, A y B, utilizándose
el nuevo método en el grupo A y el tradicional con el B. A final de curso
se hace un examen a los alumnos, obteniéndose las siguientes puntuaciones.

  - Grupo A: 6, 5, 4, 7, 3, 5.5, 6, 7, 6
  - Grupo B: 5, 4, 5, 6, 4, 6, 5, 3, 7

Supuesto que las puntuaciones de cada grupo siguen una distribución normal,
¿proporcionan estos datos evidencia para rechazar el nuevo método, con
un nivel de significación 0.05?
#+end_statement

Usaremos la dualidad entre intervalos de confianza y tests de hipótesis
para buscar si el 0 está en un intervalo de confianza 0.95 de la diferencia
de ambas medias. Las varianzas son desconocidas pero las suponemos iguales.

El intervalo para $\mu_1-\mu_2$ de menor longitud media uniforme a nivel de
confianza $1-\alpha$ es:

\[
\left(
\overline{X}-\overline{Y} - t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
,\quad
\overline{X}-\overline{Y} + t_{n_1+n_2-2;\alpha/2}S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}
\right)
\]

Y simplemente calculamos.
