#+TITLE: Inferencia estadística
#+SUBTITLE: Apuntes de teoría
#+AUTHOR: Mario Román
#+OPTIONS:
#+LANGUAGE: es

#+LaTeX: \setcounter{secnumdepth}{0}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amsthm}
#+latex_header: \usepackage{tikz-cd}
#+latex_header: \newtheorem{theorem}{Teorema}
#+latex_header: \newtheorem{fact}{Proposición}
#+latex_header: \newtheorem{definition}{Definición}
#+latex_header: \setlength{\parindent}{0pt}

** Introducción
*** Estadísticos
#+begin_definition
*Estadístico*. Función medible sobre variables aleatorias $f(X_1,X_2,\dots,X_n)$.
Se dice que es un *estimador consistente* de un parámetro cuando converge 
en probabilidad a él.
#+end_definition

** Distribuciones continuas
*** Distribución uniforme
#+begin_definition
*Distribución uniforme*. Sobre un intervalo $[a,b]$, definida por:

\[f(x|a,b) = \frac{1}{b-a}\]
#+end_definition

\[EX = \int^b_a \frac{x}{b-a}dx = \frac{b+a}{2}\]
\[Var X = \int_a^b \frac{(x-\frac{b+a}{2})^2}{b-a} dx = \frac{(b-a)^2}{12}\]

*** Distribución gamma
#+begin_definition
*Función gamma*. La siguiente integral converge para $\alpha > 0$:

\[\Gamma(\alpha) = \int_0^\infty t^{\alpha-1}e^{-t}dt\]
#+end_definition

Cumple que: $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$, de esta forma, generaliza al factorial
con $\Gamma(n) = (n-1)!$.

#+begin_definition
*Distribución gamma*. Sobre $[0,\infty)$, dada $\alpha$, se tiene la función de distribución:

\[f(x|\alpha,\beta) = 
\frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1}e^{-x/\beta}\]
#+end_definition

*** Distribución de Dirichlet
#+begin_definition
*Distribución de Dirichlet*. 

\[f(x_1\dots x_n| \alpha_1, \dots \alpha_k,\alpha_{k+1}) = 
\frac{\Gamma(\alpha_1+\dots+\alpha_{k+1})}
{\Gamma(\alpha_1)\dots \Gamma(\alpha_{k+1})}
x_1^{\alpha_1-1} \dots x_{k}^{\alpha_{k-1}-1}
\]
#+end_definition

# Esperanza
# Integral de dirichlet
# Subvector
# Dirichlet ordenada



** Máxima verosimilitud
#+begin_definition
*Máxima verosimilitud*. Sean $X_1,\dots,X_n$ v.as. independientes extraídas de una
función de probabilidad perteneciente a una familia 
$\{f(\bullet | \theta), \theta \in \Theta\}$ llamada *modelo*, pero con $\theta$ desconocida

Llamamos *estimador de máxima verosimilitud* de $\theta$ al $\hat\theta$ que maximiza 

\[{\cal L}(\hat\theta | x_1,\dots,x_n) = \prod_{i=1}^n f(x_i|\theta)\]

llamada *función de verosimilitud*.
#+end_definition

La idea del método es tomar la función de densidad conjunta asumiendo independencia:

\[f(x_1,\dots,x_n | \theta) = f(x_1|\theta) f(x_2|\theta) \dots f(x_n|\theta)\]

Y, suponiendo que los valores fueran fijos, estimar $\theta$ con la función de
la función de verosimilitud o de su logaritmo:

\[\hat l (\hat\theta | x_1,\dots,x_n) = \sum_{i=1}^n \ln f(x_i|\theta)\]

*** Estadístico suficiente
#+begin_definition
*Estadístico suficiente*. Un estadístico es suficiente para un parámetro $\theta$ 
cuando una vez conocido no puede obtenerse más información de sobre $\theta$ de
los datos; esto es:

\[\Pr(\theta| t,x) = \Pr(\theta|t)\]
#+end_definition

#+begin_theorem
*Caraterización de suficiencia de Fisher-Neyman*. $T$ es suficiente para $\theta$
ssi existen funciones no negativas $g$,$h$ tales que:

\[f_\theta(x) = h(x)g_\theta(T(x))\]

Donde $g_\theta$ sólo depende de $x$ a través de $T$ y $h$ no depende de $\theta$.
#+end_theorem

**** Ejemplo de una distribución binomial
