#+TITLE: Inferencia estadística
#+SUBTITLE: Apuntes de teoría
#+AUTHOR: Mario Román
#+OPTIONS:
#+LANGUAGE: es

#+LaTeX: \setcounter{secnumdepth}{0}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amsthm}
#+latex_header: \usepackage{tikz-cd}
#+latex_header: \newtheorem{theorem}{Teorema}
#+latex_header: \newtheorem{fact}{Proposición}
#+latex_header: \newtheorem{definition}{Definición}
#+latex_header: \setlength{\parindent}{0pt}

* Apuntes en clase
** Introducción
*** Estadísticos
#+begin_definition
*Estadístico*. Función medible sobre variables aleatorias $f(X_1,X_2,\dots,X_n)$.
Se dice que es un *estimador consistente* de un parámetro cuando converge 
en probabilidad a él.
#+end_definition

** Distribuciones continuas
*** Distribución uniforme
#+begin_definition
*Distribución uniforme*. Sobre un intervalo $[a,b]$, definida por:

\[f(x|a,b) = \frac{1}{b-a}\]
#+end_definition

\[EX = \int^b_a \frac{x}{b-a}dx = \frac{b+a}{2}\]
\[Var X = \int_a^b \frac{(x-\frac{b+a}{2})^2}{b-a} dx = \frac{(b-a)^2}{12}\]

*** Distribución gamma
#+begin_definition
*Función gamma*. La siguiente integral converge para $\alpha > 0$:

\[\Gamma(\alpha) = \int_0^\infty t^{\alpha-1}e^{-t}dt\]
#+end_definition

Cumple que: $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$, de esta forma, generaliza al factorial
con $\Gamma(n) = (n-1)!$.

#+begin_definition
*Distribución gamma*. Sobre $[0,\infty)$, dada $\alpha$, se tiene la función de distribución:

\[f(x|\alpha,\beta) = 
\frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1}e^{-x/\beta}\]
#+end_definition

*** Distribución de Dirichlet
#+begin_definition
*Distribución de Dirichlet*. 

\[f(x_1\dots x_n| \alpha_1, \dots \alpha_k,\alpha_{k+1}) = 
\frac{\Gamma(\alpha_1+\dots+\alpha_{k+1})}
{\Gamma(\alpha_1)\dots \Gamma(\alpha_{k+1})}
x_1^{\alpha_1-1} \dots x_{k}^{\alpha_{k-1}-1}
\]
#+end_definition

# Esperanza
# Integral de dirichlet
# Subvector
# Dirichlet ordenada


** Máxima verosimilitud
#+begin_definition
*Máxima verosimilitud*. Sean $X_1,\dots,X_n$ v.as. independientes extraídas de una
función de probabilidad perteneciente a una familia 
$\{f(\bullet | \theta), \theta \in \Theta\}$ llamada *modelo*, pero con $\theta$ desconocida

Llamamos *estimador de máxima verosimilitud* de $\theta$ al $\hat\theta$ que maximiza 

\[{\cal L}(\hat\theta | x_1,\dots,x_n) = \prod_{i=1}^n f(x_i|\theta)\]

llamada *función de verosimilitud*.
#+end_definition

La idea del método es tomar la función de densidad conjunta asumiendo independencia:

\[f(x_1,\dots,x_n | \theta) = f(x_1|\theta) f(x_2|\theta) \dots f(x_n|\theta)\]

Y, suponiendo que los valores fueran fijos, estimar $\theta$ con la función de
la función de verosimilitud o de su logaritmo:

\[\hat l (\hat\theta | x_1,\dots,x_n) = \sum_{i=1}^n \ln f(x_i|\theta)\]

*** Estadístico suficiente
#+begin_definition
*Estadístico suficiente*. Un estadístico es suficiente para un parámetro $\theta$ 
cuando una vez conocido no puede obtenerse más información de sobre $\theta$ de
los datos; esto es:

\[\Pr(\theta| t,x) = \Pr(\theta|t)\]
#+end_definition

#+begin_theorem
*Caraterización de suficiencia de Fisher-Neyman*. $T$ es suficiente para $\theta$
ssi existen funciones no negativas $g$,$h$ tales que:

\[f_\theta(x) = h(x)g_\theta(T(x))\]

Donde $g_\theta$ sólo depende de $x$ a través de $T$ y $h$ no depende de $\theta$.
#+end_theorem

**** Ejemplo de una distribución binomial

*** Conjuntos creíbles

* 1. Introducción a la inferencia estadística. Estadísticos muestrales.
** 1.1. Planteamiento de un problema de inferencia.
#+begin_definition
*Modelo*. Familia paramétrica de distribuciones $F(x,\theta)$ para un parámetro $\theta$.
#+end_definition

#+begin_definition
*Muestra aleatoria simple*. Vector $(X_1,\dots,X_n)$ de variables independientes idénticamente 
distribuidas.
#+end_definition

** 1.2. Función de distribución empírica
#+begin_definition
*Función de distribución empírica*. La función de distribución más razonable que podemos obtener
del muestreo.

\[F^\ast_{X_1,\dots,X_n}(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(x_i < x)} \]
#+end_definition

Fijado un $x \in \mathbb{R}$, $F^\ast(x)$ es una variable aleatoria cumpliendo por definición que:

\[ nF^\ast(x) \longrightarrow {\cal B}(n, F(x))\]

Calculamos su esperanza y varianza desde Bernoulli como:

- Esperanza: $E[F^\ast(x)] = F(x)$
- Varianza: $Var[F^\ast(x)] = \frac{F(x) (1-F(x))}{n}$

Aplicando entonces el Teorema Central del Límite:

\[ \frac{nF^\ast(x) - nF(x)}{\sqrt{nF(x)(1-F(x))}} \leadsto {\cal N}(0,1) \]

*** Teorema de Glivenko-Cantelli
#+begin_theorem
*Teorema de Glivenko-Cantelli*. Las funciones de distribución muestrales convergen 
casi seguramente y uniformemente a la teórica.

\[ P\left\{ \lim_{n \rightarrow \infty} \sup_{x \in \mathbb{R}} |F^\ast(x) - F(x)| = 0\right\} = 1\]
#+end_theorem

# TODO: Funciones características


* 2. Muestreo de poblaciones normales
** Distribución chi cuadrado de Pearson
*** Definición
#+begin_definition
*Distribución chi cuadrado*. Es un caso particular de la distribución gamma que se obtiene como la
distribución de la suma de cuadrados de variables normales, $X \leadsto \chi^2(k) = \Gamma(k/2,1/2)$.
#+end_definition

*** Función de densidad

\[f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{k/2}} x^{k/2-1}e^{-x/2}\]

*** Función generatriz de momentos

*** Esperanza y varianza

- $E[X] = k$
- $Var[X] = 2k$

*** Propiedad de reproductividad
Si tengo una serie de variables independientes distribuidas por $X_i \leadsto \chi^2(k_i)$, entonces:

\[\sum_{i=1}^n X_i = \chi^2 \left(\sum_{i=1}^n k_i \right)\]

*** Relación con la distribución normal
Dadas variables independientes $X_i \leadsto {\cal N}(0,1)$,

\[\sum_{i=1}^n X^2_i \leadsto \chi^2(n)\]


** Distribución t de Student
*** Definición
#+begin_definition
*T de Student*. Dadas dos variables independientes $X \leadsto {\cal N}(0,1)$ e $Y \leadsto \chi^2(n)$, tenemos

\[ T = \frac{X}{\sqrt{Y/n}} \leadsto t(n) \]
#+end_definition

*** Función de densidad
*** Momentos
Tenemos que $\exists E[T^k] \Leftrightarrow k < n$, cuando existen, se tiene

 - $E[T] = 0$
 - $Var[T] = \frac{n}{n-2}$

** Distribución F de Snedecor
